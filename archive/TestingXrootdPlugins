---+!! XrootD plug-ins testing

%TOC{depth="2"}%

---# About This Document

%ICON{hand}% This document is intended for OSG Software/Release teams to gather information on testing XrootD plug-ins

---# About XrootD

---# Requirements

   * There nodes with  root access

---# Hadoop name node installation:

Use the following script with option 1:

<pre class="file">
#!/bin/bash
set -e

select NODETYPE in namenode datanode gridftp; do
  [[ $NODETYPE ]] && break
done
case $NODETYPE in
  namenode ) NAMENODE=$HOSTNAME ;;
         * ) read -p 'hostname for NAMENODE? ' NAMENODE ;;
esac
echo NODETYPE=$NODETYPE
echo NAMENODE=$NAMENODE
read -p 'ok? [y/N] ' ok
case $ok in
  y*|Y*) ;;  # ok
      *) exit ;;
esac
#yum install --enablerepo=osg-minefield osg-se-hadoop-$NODETYPE
yum install osg-se-hadoop-$NODETYPE
case $NODETYPE in
  namenode|datanode )
    mkdir -p /data/{hadoop,scratch,checkpoint}
    chown -R hdfs:hdfs /data
    sed -i s/NAMENODE/$NAMENODE/ /etc/hadoop/conf.osg/{core,hdfs}-site.xml
    cp /etc/hadoop/conf.osg/{core,hdfs}-site.xml /etc/hadoop/conf/
    touch /etc/hosts_exclude
    ;;
  gridftp )
    ln -snf conf.osg /etc/hadoop/conf
    sed -i s/NAMENODE/$NAMENODE/ /etc/hadoop/conf.osg/{core,hdfs}-site.xml
    echo "hadoop-fuse-dfs# /mnt/hadoop fuse server=$NAMENODE,port=9000,rdbuffer=131072,allow_other 0 0" >> /etc/fstab
    mkdir /mnt/hadoop
    mount /mnt/hadoop
    cp -v /etc/redhat-release /mnt/hadoop/test-file
    sed -i '/globus_mapping/s/^# *//' /etc/grid-security/gsi-authz.conf
    sed -i s/yourgums.yourdomain/gums.fnal.gov/ /etc/lcmaps.db
    mkdir /mnt/hadoop/fnalgrid
    useradd fnalgrid -g fnalgrid
    chown fnalgrid:fnalgrid /mnt/hadoop/fnalgrid
    service globus-gridftp-server start
    if type -t globus-url-copy >/dev/null; then
      globus-url-copy file:////bin/bash  gsiftp://$HOSTNAME/mnt/hadoop/fnalgrid/first_test
    else
      echo globus-url-copy not installed
    fi
    ;;
esac
case $NODETYPE in
  namenode ) su - hdfs -c "hadoop namenode -format" ;;
esac
service hadoop-hdfs-$NODETYPE start

</pre>

---# Hadoop data node installation

   * Run same script as before but with option number 2.
   * Install xrootd-server: <pre class="rootscreen"> yum install xrootd-server </pre>
   * Install xrootd-plugins <pre class="rootscreen"> yum install xrootd-cmstfc xrootd-hdfs</pre>

---# Gridftp installation

Run same as script but with option number.

---# Testing instructions

---# On the name node:

<pre class="rootscreen">
[root@fermicloud092 ~]# hdfs dfs -ls /test-file
Found 1 items
-rw-r--r--   2 root root          0 2014-07-21 15:57 /test-file
</pre>

This means your hadoop is working.

Modify the file =/etc/xrootd/xrootd-clustered.cfg=
to look like this:
<pre class="file">
xrd.port 1094

# The roles this server will play.                                                                                            
all.role server
all.role manager if xrootd.unl.edu
# The known managers                                                                                                          
all.manager srm.unl.edu:1213
#all.manager xrootd.ultralight.org:1213                                                                                       

# Allow any path to be exported; this is further refined in the authfile.                                                     
all.export / nostage

# Hosts allowed to use this xrootd cluster                                                                                    
cms.allow host *

### Standard directives                                                                                                       
# Simple sites probably don't need to touch these.                                                                            
# Logging verbosity                                                                                                           
xrootd.trace all -debug
ofs.trace all -debug
xrd.trace all -debug
cms.trace all -debug

# Integrate with CMS TFC, placed in /etc/storage.xml                                                                          
oss.namelib /usr/lib64/libXrdCmsTfc.so file:/etc/xrootd/storage.xml?protocol=hadoop

xrootd.seclib /usr/lib64/libXrdSec.so
xrootd.fslib /usr/lib64/libXrdOfs.so
ofs.osslib /usr/lib64/libXrdHdfs.so
all.adminpath /var/run/xrootd
all.pidpath /var/run/xrootd

cms.delay startup 10
cms.fxhold 60s
cms.perf int 30s pgm /usr/bin/XrdOlbMonPerf 30

oss.space default_stage /opt/xrootd_cache
</pre>

Create file =/etc/xrootd/storage.xml= and place this:

<verbatim>
<storage-mapping>
<lfn-to-pfn protocol="hadoop" destination-match=".*" path-match=".*/+tmp2/test-file" result="/test-file"/>
</storage-mapping>
</verbatim>

For el7 the instrucctions are a little bit different. See:

https://jira.opensciencegrid.org/browse/SOFTWARE-2198?focusedCommentId=334667&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-334667

Now from a node do:

<pre class ="rootscreen">
xrdcp --debug 3 root://yourdatanode.yourdomain:1094//tmp2/test-file .
</pre>



If it is sucessful it would have tested both cmstfc and hdfs plugins
-- Main.EdgarFajardo - 25 Jul 2014
