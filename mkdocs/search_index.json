{
    "docs": [
        {
            "location": "/", 
            "text": "OSG Technology Area\n\n\nWelcome to the home page of the OSG Technology Team documentation area! This is currently a work in progress as we migrate our TWiki documentation to GitHub. If you are looking for our full documentation, please visit the \nTWiki\n.\n\n\nIf you are looking for site administrator documentation, please visit the \nOSG Documentation page\n.\n\n\nThe Team\n\n\n\n\n\n\n\n\nSoftware and Release\n\n\nTechnology\n\n\n\n\n\n\n\n\n\n\nBrian Lin (software manager) (100%)\n\n\nBrian Bockelman (manager) (50%)\n\n\n\n\n\n\nCarl Edquist\n\n\nDerek Weitzel (50%)\n\n\n\n\n\n\nEdgar Fajardo (50%)\n\n\nEdgar Fajardo (50%)\n\n\n\n\n\n\nMat Selmeci\n\n\nJeff Dost (50%)\n\n\n\n\n\n\nSuchandra Thapa (50%)\n\n\nMarian Zvada (25%)\n\n\n\n\n\n\nTim Cartwright (35%)\n\n\n\n\n\n\n\n\nTim Theisen (release manager) (50%)\n\n\n\n\n\n\n\n\n\n\nContact Us\n\n\n\n\nSlack channel\n\n\nosg-software@opensciencegrid.org\n - General discussion amongst team members\n\n\nosg-commits@cs.wisc.edu\n - Broadcast of all source code repo commits\n\n\n\n\nMeeting Notes\n\n\nWhen:\n Every Monday, 11:00 a.m. (U.S. Central)  \n\n\nWhere:\n +1 719-284-5267, PIN #57363; \nUber Conference\n\n\nRecent Notes\n\n\n\n\n9 October 2017\n\n\n2 October 2017\n\n\n25 September 2017\n\n\n18 September 2017\n\n\n11 September 2017\n\n\n5 September 2017\n\n\n28 August 2017\n\n\n21 August 2017 - Canceled\n\n\n14 August 2017\n\n\n7 August 2017\n\n\n31 July 2017\n\n\n24 July 2017\n\n\n17 July 2017\n\n\n10 July 2017\n\n\n3 July 2017\n\n\n26 June 2017\n\n\n19 June 2017\n\n\n12 June 2017\n\n\n5 June 2017\n\n\n30 May 2017\n - Memorial Day\n\n\n22 May 2017\n\n\n15 May 2017\n\n\n8 May 2017\n\n\n1 May 2017\n\n\n24 April 2017\n\n\n17 April 2017\n\n\n10 April 2017\n\n\n3 April 2017\n\n\n27 March 2017\n\n\n20 March 2017\n\n\n13 March 2017\n\n\n27 February 2017\n\n\n\n\nMeeting note archives older than February 27, 2017 can be found \nhere\n.", 
            "title": "Home"
        }, 
        {
            "location": "/#osg-technology-area", 
            "text": "Welcome to the home page of the OSG Technology Team documentation area! This is currently a work in progress as we migrate our TWiki documentation to GitHub. If you are looking for our full documentation, please visit the  TWiki .  If you are looking for site administrator documentation, please visit the  OSG Documentation page .", 
            "title": "OSG Technology Area"
        }, 
        {
            "location": "/#the-team", 
            "text": "Software and Release  Technology      Brian Lin (software manager) (100%)  Brian Bockelman (manager) (50%)    Carl Edquist  Derek Weitzel (50%)    Edgar Fajardo (50%)  Edgar Fajardo (50%)    Mat Selmeci  Jeff Dost (50%)    Suchandra Thapa (50%)  Marian Zvada (25%)    Tim Cartwright (35%)     Tim Theisen (release manager) (50%)", 
            "title": "The Team"
        }, 
        {
            "location": "/#contact-us", 
            "text": "Slack channel  osg-software@opensciencegrid.org  - General discussion amongst team members  osg-commits@cs.wisc.edu  - Broadcast of all source code repo commits", 
            "title": "Contact Us"
        }, 
        {
            "location": "/#meeting-notes", 
            "text": "When:  Every Monday, 11:00 a.m. (U.S. Central)    Where:  +1 719-284-5267, PIN #57363;  Uber Conference", 
            "title": "Meeting Notes"
        }, 
        {
            "location": "/#recent-notes", 
            "text": "9 October 2017  2 October 2017  25 September 2017  18 September 2017  11 September 2017  5 September 2017  28 August 2017  21 August 2017 - Canceled  14 August 2017  7 August 2017  31 July 2017  24 July 2017  17 July 2017  10 July 2017  3 July 2017  26 June 2017  19 June 2017  12 June 2017  5 June 2017  30 May 2017  - Memorial Day  22 May 2017  15 May 2017  8 May 2017  1 May 2017  24 April 2017  17 April 2017  10 April 2017  3 April 2017  27 March 2017  20 March 2017  13 March 2017  27 February 2017   Meeting note archives older than February 27, 2017 can be found  here .", 
            "title": "Recent Notes"
        }, 
        {
            "location": "/software/development-process/", 
            "text": "Software Development Process\n\n\nThis page is for the OSG Software team and other contributors to the OSG software stack. It is meant to be the central source for all development processes for the Software team. (But right now, it is just a starting point.)\n\n\nOverall Development Cycle\n\n\nFor a typical update to an existing package, the overall development cycle is roughly as follows:\n\n\n\n\nDownload the new upstream source (tarball, source RPM, checkout) into \nthe UW AFS upstream area\n\n\nIn \na checkout of our packaging code\n, update \nthe reference to the upstream file\n and, as needed, \nthe RPM spec file\n\n\nUse \nosg-build\n to perform a scratch build of the updated package\n\n\nVerify that the build succeeded; if not, redo previous steps until success\n\n\nOptionally, lightly test the new RPM(s); if there are problems, redo previous steps until success\n\n\nUse \nosg-build\n to perform an official build of the updated package (which will go into the development repos)\n\n\nPerform standard developer testing of the new RPM(s) \u2014 see below for details\n\n\nObtain permission from the Software manager to promote the package\n\n\nPromote the package to testing \u2014 see below for details\n\n\n\n\nVersioning Guidelines\n\n\nOSG-owned software should contain three digits, X.Y.Z, where X represents the major version, Y the minor version, and Z the maintenance version. New releases of software should increment one of the major, minor, or maintenance according to the following guidelines:\n\n\n\n\nMajor:\n Major new software, typically (but not limited to) full rewrites, new architectures, major new features; can certainly break backward compatibility (but should provide a smooth upgrade path). Worthy of introduction into Upcoming.\n\n\nMinor:\n Notable changes to the software, including significant feature changes, API changes, etc.; may break compatibility, but must provide an upgrade path from other versions within the same Major series.\n\n\nMaintenance:\n Bug fixes, minor feature tweaks, etc.; must not break compatibility with other versions within the same Major.Minor series.\n\n\n\n\nIf you are unsure about which version number to increment in a software update, consult the Software Manager.\n\n\nBuild Procedures\n\n\nBuilding packages for multiple OSG release series\n\n\nThe OSG Software team supports multiple release series, independent but in parallel to a large degree. In many cases, a single package is the same across release series, and therefore we want to build the package once and share it among the series. The procedure below suggests a way to accomplish this task.\n\n\nCurrent definitions:\n\n\n\n\nmaintenance: OSG 3.3 ( \nbranches/osg-3.3\n )\n\n\ncurrent: OSG 3.4 ( \ntrunk\n )\n\n\n\n\n\n\n\nProcedure:\n\n\n\n\nMake changes to \ntrunk\n\n\nOptionally, make and test a scratch build from \ntrunk\n\n\nCommit the changes\n\n\nMake an official build from \ntrunk\n (e.g.: \nosg-build koji \nPACKAGE\n)\n\n\nPerform the standard 4 tests for the \ncurrent\n series (see below)\n\n\nMerge the relevant commits from \ntrunk\n into the \nmaintenance\n branch (see below for tips)\n\n\nOptionally, make and test a scratch build from the \nmaintenance\n branch\n\n\nCommit the merge\n\n\nMake an official build from the \nmaintenance\n branch (e.g.: \nosg-build koji --repo=3.3 \nPACKAGE\n)\n\n\nPerform the standard 4 tests for the \nmaintenance\n series (see below)\n\n\nAs needed (or directed by the Software manager), perform the cross-series tests (see below)\n\n\n\n\nNote:\n Do not change the RPM Release number in the \nmaintenance\n branch before rebuilding; the %dist tag will differ automatically, and hence the \nmaintenance\n and \ncurrent\n NVRs will not conflict.\n\n\n\n\n\nMerging changes from one release series to another\n\n\nThese instructions assume that you are merging from \ntrunk\n to \nbranches/osg-3.3\n. They also assume that the current directory you are in is a checkout of \nbranches/osg-3.3\n. I will use \n$pkg\n to refer to the name of your package.\n\n\nFirst, you will need the commit numbers for your changes:\n\n\nsvn log \\^/native/redhat/trunk/$pkg | less\n\n\n\n\n\nWrite down the commits you want to merge.\n\n\nIf you only have one commit, merge that commit with -c as follows:\n\n\nsvn merge -c $commit_num \\^/native/redhat/trunk/$pkg $pkg\n\n\n\n\n\nWhere \n$commit_num\n is the SVN revision number of that commit (e.g. 17000). Merging an individual change like this is referred to as \"cherry-picking\".\n\n\nIf you have a range of commits and you wish to merge all commits within that range, then do the following:\n\n\nsvn merge -r $start_num:$end_num \\^/native/redhat/trunk/$pkg $pkg\n\n\n\n\n\nWhere \n$start_num\n is the SVN revision of the commit \nBEFORE\n your first commit, and \n$end_num\n is the SVN revision of your last commit in that range. \nNote:\n Be very careful when merging a range from trunk into the maintenance branch so that you do not introduce more changes to the maintenance branch than are necessary.\n\n\nIf you have multiple commits but they are not contiguous (i.e. there are commits made by you or someone else in that range that you do not want to merge), you will need to cherry-pick each individual commit.\n\n\nsvn merge -c $commit1 \\^/native/redhat/trunk/$pkg $pkg\nsvn merge -c $commit2 \\^/native/redhat/trunk/$pkg $pkg\n...\n\n\n\n\n\nWhere \n$commit1\n, \n$commit2\n are the commit numbers of the individual changes.\n\n\nNote that merge tracking in recent versions of SVN (1.5 or newer) should prevent commits from accidentally being merged multiple times. You should still look out for conflicts and examine the changes via \nsvn diff\n before committing the merge.\n\n\nTesting Procedures\n\n\nBefore promoting a package to a testing repository, each build must be tested lightly from the development repos to make sure that it is not completely broken, thereby wasting time during acceptance testing. Normally, the person who builds a package performs the development testing.\n\n\nIf you are not doing your own development testing for a package\n, contact the Software Manager and/or leave a comment in the associated ticket; otherwise, your package may never be promoted to testing and hence never released.\n\n\nThe \"Standard 4\" tests, defined\n\n\nIn most cases, the Software manager will ask a developer to perform the \u201cstandard 4\u201d tests on an updated package in a release series before promotion. This is a shorthand description for a standard set of 4 test runs:\n\n\n\n\nFresh install on el6\n\n\nFresh install on el7\n\n\nUpdate install on el6\n\n\nUpdate install on el7\n\n\n\n\nAn \u201cupdate install\u201d is a fresh install of the relevant package (or better yet, metapackage that includes it) \nfrom the production repository\n, followed by an update to the new build \nfrom the development repository\n.\n\n\nFor each test run, the amount of functional testing required will vary.\n\n\n\n\nFor very simple changes, it may be sufficient to verify that each installation succeeds and that the expected files are in place\n\n\nFor some changes, it may be sufficient to run osg-test on the resulting installation\n\n\nFor some changes, it will be necessary to perform careful functional tests of the affected component(s)\n\n\n\n\nIf you have questions, check with the Software Manager to determine the amount of testing that is required per test run.\n\n\nThe \"Cross-Series\" test, defined\n\n\nThe cross-series test may need to be run for packages that have been built for multiple release series of the OSG software stack (i.e. 3.3 and 3.4):\n\n\n\n\nOn el6, install from the 3.3 repositories, then update from the 3.4 repositories\n\n\nOn el7, install from the 3.3 repositories, then update from the 3.4 repositories\n\n\n\n\nViewed another way, this test is similar to the update installs, above, except from 3.3-release to 3.4-development.\n\n\nThe \"Long Tail\" tests, defined\n\n\nThese tests may need to be run when updating a package that's also in the old, unsupported (3.2) branch. They will consist of:\n\n\n\n\nInstall from 3.2-release and update to 3.4-development (on el6 only)\n\n\n\n\nThe \"full set of tests\", defined\n\n\nAll of the tests mentioned above.\n\n\nRunning the tests in VM Universe\n\n\nIn the case that the package you're testing is covered by osg-tested-internal, you can run the full set of tests in a manual VM universe test run. Make sure you meet the \npre-requisites\n required to submit VM Universe jobs on \nosghost.chtc.wisc.edu\n. After that's done, you can prepare the test suite by running:\n\n\nosg-run-tests \nTesting \nchange x\n\n\n\n\n\n\nAfter you \ncd\n into the directory specified in the output of the previous command, you will need to edit the \n*.yaml\n files in \nparameters.d\n to reflect the tests that you will want to run i.e. clean installs, upgrade installs and upgrade installs between OSG versions.\n\n\nOnce you're satisfied with your list of parameters, submit the dag:\n\n\ncondor_submit_dag master-run.dag\n\n\n\n\n\n\nPromoting a Package to Testing\n\n\nOnce development and development testing is complete, the final OSG Software step is to promote the package(s) to our testing repositories. After that, the Release team takes over with acceptance testing and ultimately release. Of course if they discover problems, the ticket(s) will be returned to OSG Software for further development, essentially restarting the development cycle.\n\n\nPreparing a Good Promotion Request\n\n\nDevelopers must obtain permission from the OSG Software manager to promote a package from development to testing. A promotion request goes into at least one affected JIRA ticket and will be answered there as well. Below are some tips for writing a good promotion request:\n\n\n\n\nMake sure that relevant information about goals, history, and resolution is in the associated ticket(s)\n\n\nInclude globs for the NVRs to be promoted (or a detailed list, if it is that complicated, which it almost never is)\n\n\nIf you ran automated tests:\n\n\nLink to the results page(s)\n\n\nVerify that relevant tests ran successfully (as opposed to being skipped or failing) \u2013\u00a0briefly summarize your findings\n\n\nNote whether the automated tests are just regression tests or actually test the current change(s)\n\n\nIf there are \nany\n failures, explain why they are not important to the promotion request\n\n\n\n\n\n\nIf you ran manual tests:\n\n\nSummarize your tests and findings\n\n\nIf there were failures, explain why they are not important to the promotion request\n\n\n\n\n\n\nIf there are critical build dependencies that we typically check, include reports from the \nbuilt-against-pkgs\n tool\n\n\nNote: This step is really just for known, specific cases, like the {HTCondor, HTCondor-CE, BLAHP} set and the {BeStMan, GUMS, VOMS Admin, etc.} Java set\n\n\nOccasionally, the OSG Software manager will request the tool to be run for other cases\n\n\n\n\n\n\nIf other packages depend on the to-be-promoted package, explain whether the dependent packages must be rebuilt or, if not, why not\n\n\n\n\nFor example (hypothetical promotion request for HTCondor-CE):\n\n\n\n\nMay I promote \nhtcondor-ce-2.3.4-2.osg3*.el*\n? I ran a complete set of automated tests \nLINK THE PRECEDING TEXT OR SEPARATELY HERE>\n; the HTCondor-CE tests ran and passed in all cases. There were some spurious failures of RSV in the All condition for RHEL 6, but this is a known failure case that is independent of HTCondor-CE. I also did a few spot checks manually (one VM each for SL 6 and SL 7), and in each case setting \nuse_frobnosticator = true\n in the configuration resulted in the expected behavior as defined in the description field above. The \nbuilt-against-pkgs\n tool shows that I built against all the latest HTCondor and BLAHP builds, see below. \nJIRA-formatted table comes after>\n\n\n\n\nPromoting\n\n\nFollow these steps to request promotion, promote a package, and note the promotion in JIRA:\n\n\n\n\nMake sure the package update has at least one associated JIRA ticket; if there is no ticket, at least create one for releasing the package(s)\n\n\nObtain permission to promote the package(s) from the Software manager (see above)\n\n\nUse \nosg-promote\n to promote the package(s) from development to testing\n\n\nComment on the associated JIRA ticket(s) with osg-promote\u2019s JIRA-formatted output (or at least the build NVRs) and, if you know, suggestions for acceptance testing\n\n\nMark each associated JIRA ticket as \u201cReady For Testing\u201d and (done automatically for you:) set the Assignee to \u201cUnassigned\u201d", 
            "title": "Development Process"
        }, 
        {
            "location": "/software/development-process/#software-development-process", 
            "text": "This page is for the OSG Software team and other contributors to the OSG software stack. It is meant to be the central source for all development processes for the Software team. (But right now, it is just a starting point.)", 
            "title": "Software Development Process"
        }, 
        {
            "location": "/software/development-process/#overall-development-cycle", 
            "text": "For a typical update to an existing package, the overall development cycle is roughly as follows:   Download the new upstream source (tarball, source RPM, checkout) into  the UW AFS upstream area  In  a checkout of our packaging code , update  the reference to the upstream file  and, as needed,  the RPM spec file  Use  osg-build  to perform a scratch build of the updated package  Verify that the build succeeded; if not, redo previous steps until success  Optionally, lightly test the new RPM(s); if there are problems, redo previous steps until success  Use  osg-build  to perform an official build of the updated package (which will go into the development repos)  Perform standard developer testing of the new RPM(s) \u2014 see below for details  Obtain permission from the Software manager to promote the package  Promote the package to testing \u2014 see below for details", 
            "title": "Overall Development Cycle"
        }, 
        {
            "location": "/software/development-process/#versioning-guidelines", 
            "text": "OSG-owned software should contain three digits, X.Y.Z, where X represents the major version, Y the minor version, and Z the maintenance version. New releases of software should increment one of the major, minor, or maintenance according to the following guidelines:   Major:  Major new software, typically (but not limited to) full rewrites, new architectures, major new features; can certainly break backward compatibility (but should provide a smooth upgrade path). Worthy of introduction into Upcoming.  Minor:  Notable changes to the software, including significant feature changes, API changes, etc.; may break compatibility, but must provide an upgrade path from other versions within the same Major series.  Maintenance:  Bug fixes, minor feature tweaks, etc.; must not break compatibility with other versions within the same Major.Minor series.   If you are unsure about which version number to increment in a software update, consult the Software Manager.", 
            "title": "Versioning Guidelines"
        }, 
        {
            "location": "/software/development-process/#build-procedures", 
            "text": "", 
            "title": "Build Procedures"
        }, 
        {
            "location": "/software/development-process/#building-packages-for-multiple-osg-release-series", 
            "text": "The OSG Software team supports multiple release series, independent but in parallel to a large degree. In many cases, a single package is the same across release series, and therefore we want to build the package once and share it among the series. The procedure below suggests a way to accomplish this task.  Current definitions:   maintenance: OSG 3.3 (  branches/osg-3.3  )  current: OSG 3.4 (  trunk  )    Procedure:   Make changes to  trunk  Optionally, make and test a scratch build from  trunk  Commit the changes  Make an official build from  trunk  (e.g.:  osg-build koji  PACKAGE )  Perform the standard 4 tests for the  current  series (see below)  Merge the relevant commits from  trunk  into the  maintenance  branch (see below for tips)  Optionally, make and test a scratch build from the  maintenance  branch  Commit the merge  Make an official build from the  maintenance  branch (e.g.:  osg-build koji --repo=3.3  PACKAGE )  Perform the standard 4 tests for the  maintenance  series (see below)  As needed (or directed by the Software manager), perform the cross-series tests (see below)   Note:  Do not change the RPM Release number in the  maintenance  branch before rebuilding; the %dist tag will differ automatically, and hence the  maintenance  and  current  NVRs will not conflict.", 
            "title": "Building packages for multiple OSG release series"
        }, 
        {
            "location": "/software/development-process/#merging-changes-from-one-release-series-to-another", 
            "text": "These instructions assume that you are merging from  trunk  to  branches/osg-3.3 . They also assume that the current directory you are in is a checkout of  branches/osg-3.3 . I will use  $pkg  to refer to the name of your package.  First, you will need the commit numbers for your changes:  svn log \\^/native/redhat/trunk/$pkg | less  Write down the commits you want to merge.  If you only have one commit, merge that commit with -c as follows:  svn merge -c $commit_num \\^/native/redhat/trunk/$pkg $pkg  Where  $commit_num  is the SVN revision number of that commit (e.g. 17000). Merging an individual change like this is referred to as \"cherry-picking\".  If you have a range of commits and you wish to merge all commits within that range, then do the following:  svn merge -r $start_num:$end_num \\^/native/redhat/trunk/$pkg $pkg  Where  $start_num  is the SVN revision of the commit  BEFORE  your first commit, and  $end_num  is the SVN revision of your last commit in that range.  Note:  Be very careful when merging a range from trunk into the maintenance branch so that you do not introduce more changes to the maintenance branch than are necessary.  If you have multiple commits but they are not contiguous (i.e. there are commits made by you or someone else in that range that you do not want to merge), you will need to cherry-pick each individual commit.  svn merge -c $commit1 \\^/native/redhat/trunk/$pkg $pkg\nsvn merge -c $commit2 \\^/native/redhat/trunk/$pkg $pkg\n...  Where  $commit1 ,  $commit2  are the commit numbers of the individual changes.  Note that merge tracking in recent versions of SVN (1.5 or newer) should prevent commits from accidentally being merged multiple times. You should still look out for conflicts and examine the changes via  svn diff  before committing the merge.", 
            "title": "Merging changes from one release series to another"
        }, 
        {
            "location": "/software/development-process/#testing-procedures", 
            "text": "Before promoting a package to a testing repository, each build must be tested lightly from the development repos to make sure that it is not completely broken, thereby wasting time during acceptance testing. Normally, the person who builds a package performs the development testing.  If you are not doing your own development testing for a package , contact the Software Manager and/or leave a comment in the associated ticket; otherwise, your package may never be promoted to testing and hence never released.", 
            "title": "Testing Procedures"
        }, 
        {
            "location": "/software/development-process/#the-standard-4-tests-defined", 
            "text": "In most cases, the Software manager will ask a developer to perform the \u201cstandard 4\u201d tests on an updated package in a release series before promotion. This is a shorthand description for a standard set of 4 test runs:   Fresh install on el6  Fresh install on el7  Update install on el6  Update install on el7   An \u201cupdate install\u201d is a fresh install of the relevant package (or better yet, metapackage that includes it)  from the production repository , followed by an update to the new build  from the development repository .  For each test run, the amount of functional testing required will vary.   For very simple changes, it may be sufficient to verify that each installation succeeds and that the expected files are in place  For some changes, it may be sufficient to run osg-test on the resulting installation  For some changes, it will be necessary to perform careful functional tests of the affected component(s)   If you have questions, check with the Software Manager to determine the amount of testing that is required per test run.", 
            "title": "The \"Standard 4\" tests, defined"
        }, 
        {
            "location": "/software/development-process/#the-cross-series-test-defined", 
            "text": "The cross-series test may need to be run for packages that have been built for multiple release series of the OSG software stack (i.e. 3.3 and 3.4):   On el6, install from the 3.3 repositories, then update from the 3.4 repositories  On el7, install from the 3.3 repositories, then update from the 3.4 repositories   Viewed another way, this test is similar to the update installs, above, except from 3.3-release to 3.4-development.", 
            "title": "The \"Cross-Series\" test, defined"
        }, 
        {
            "location": "/software/development-process/#the-long-tail-tests-defined", 
            "text": "These tests may need to be run when updating a package that's also in the old, unsupported (3.2) branch. They will consist of:   Install from 3.2-release and update to 3.4-development (on el6 only)", 
            "title": "The \"Long Tail\" tests, defined"
        }, 
        {
            "location": "/software/development-process/#the-full-set-of-tests-defined", 
            "text": "All of the tests mentioned above.", 
            "title": "The \"full set of tests\", defined"
        }, 
        {
            "location": "/software/development-process/#running-the-tests-in-vm-universe", 
            "text": "In the case that the package you're testing is covered by osg-tested-internal, you can run the full set of tests in a manual VM universe test run. Make sure you meet the  pre-requisites  required to submit VM Universe jobs on  osghost.chtc.wisc.edu . After that's done, you can prepare the test suite by running:  osg-run-tests  Testing  change x   After you  cd  into the directory specified in the output of the previous command, you will need to edit the  *.yaml  files in  parameters.d  to reflect the tests that you will want to run i.e. clean installs, upgrade installs and upgrade installs between OSG versions.  Once you're satisfied with your list of parameters, submit the dag:  condor_submit_dag master-run.dag", 
            "title": "Running the tests in VM Universe"
        }, 
        {
            "location": "/software/development-process/#promoting-a-package-to-testing", 
            "text": "Once development and development testing is complete, the final OSG Software step is to promote the package(s) to our testing repositories. After that, the Release team takes over with acceptance testing and ultimately release. Of course if they discover problems, the ticket(s) will be returned to OSG Software for further development, essentially restarting the development cycle.", 
            "title": "Promoting a Package to Testing"
        }, 
        {
            "location": "/software/development-process/#preparing-a-good-promotion-request", 
            "text": "Developers must obtain permission from the OSG Software manager to promote a package from development to testing. A promotion request goes into at least one affected JIRA ticket and will be answered there as well. Below are some tips for writing a good promotion request:   Make sure that relevant information about goals, history, and resolution is in the associated ticket(s)  Include globs for the NVRs to be promoted (or a detailed list, if it is that complicated, which it almost never is)  If you ran automated tests:  Link to the results page(s)  Verify that relevant tests ran successfully (as opposed to being skipped or failing) \u2013\u00a0briefly summarize your findings  Note whether the automated tests are just regression tests or actually test the current change(s)  If there are  any  failures, explain why they are not important to the promotion request    If you ran manual tests:  Summarize your tests and findings  If there were failures, explain why they are not important to the promotion request    If there are critical build dependencies that we typically check, include reports from the  built-against-pkgs  tool  Note: This step is really just for known, specific cases, like the {HTCondor, HTCondor-CE, BLAHP} set and the {BeStMan, GUMS, VOMS Admin, etc.} Java set  Occasionally, the OSG Software manager will request the tool to be run for other cases    If other packages depend on the to-be-promoted package, explain whether the dependent packages must be rebuilt or, if not, why not   For example (hypothetical promotion request for HTCondor-CE):   May I promote  htcondor-ce-2.3.4-2.osg3*.el* ? I ran a complete set of automated tests  LINK THE PRECEDING TEXT OR SEPARATELY HERE> ; the HTCondor-CE tests ran and passed in all cases. There were some spurious failures of RSV in the All condition for RHEL 6, but this is a known failure case that is independent of HTCondor-CE. I also did a few spot checks manually (one VM each for SL 6 and SL 7), and in each case setting  use_frobnosticator = true  in the configuration resulted in the expected behavior as defined in the description field above. The  built-against-pkgs  tool shows that I built against all the latest HTCondor and BLAHP builds, see below.  JIRA-formatted table comes after>", 
            "title": "Preparing a Good Promotion Request"
        }, 
        {
            "location": "/software/development-process/#promoting", 
            "text": "Follow these steps to request promotion, promote a package, and note the promotion in JIRA:   Make sure the package update has at least one associated JIRA ticket; if there is no ticket, at least create one for releasing the package(s)  Obtain permission to promote the package(s) from the Software manager (see above)  Use  osg-promote  to promote the package(s) from development to testing  Comment on the associated JIRA ticket(s) with osg-promote\u2019s JIRA-formatted output (or at least the build NVRs) and, if you know, suggestions for acceptance testing  Mark each associated JIRA ticket as \u201cReady For Testing\u201d and (done automatically for you:) set the Assignee to \u201cUnassigned\u201d", 
            "title": "Promoting"
        }, 
        {
            "location": "/software/markdown-migration/", 
            "text": "Migrating to Markdown\n\n\nAs part of the TWiki retirement (the read-only target date of Oct 1, 2017, with a shutdown date in 2018), we will need to convert the OSG Software and Release3 docs from TWiki syntax to \nMarkdown\n. The following document outlines the conversion process and conventions.\n\n\nChoosing the git repository\n\n\nFirst you will need to choose which git repoository you will be working with:\n\n\n\n\n\n\n\n\nIf you are converting a document from...\n\n\nUse this github repository...\n\n\n\n\n\n\n\n\n\n\nSoftwareTeam\n\n\ntechnology\n\n\n\n\n\n\nRelease3\n\n\ndocs\n\n\n\n\n\n\n\n\nOnce you've chosen the target repository for your document, move onto the next section and pick your conversion method.\n\n\nAutomatic TWiki conversion\n\n\n\n\nNote\n\n\nIf you are only archiving the documents, skip to this \nsection\n.\n\n\n\n\nChoose one of the following methods for converting TWiki documents:\n\n\n\n\nUsing our own \ndocker conversion image\n (recommended)\n\n\nDirectly using pandoc and mkdocs \non your own machine\n\n\n\n\nUsing docker\n\n\nThe twiki-converter docker image can be used to preview the document tree via a \nmkdocs\n development server, archive TWiki documents, and convert documents to Markdown via \npandoc\n. The image is available on \nosghost\n, otherwise, follow the instructions \nhere\n to build it locally. \n\n\nRequirements\n\n\nTo perform a document migration using docker, you will need the following tools and accounts:\n\n\n\n\nFork\n and \nclone\n the repository that you chose in the \nabove section\n\n\nA host with a running docker service\n\n\nsudo\n or membership in the \ndocker\n group\n\n\n\n\nIf you cannot install the above tools locally, they are available on \nosghost\n. Speak with Brian L for access.\n\n\nPreparing the git repository\n\n\n\n\ncd\n into your local git repository\n\n\n\n\nAdd \nopensciencegrid/technology\n as the upstream remote repository for merging upstream changes:\n\n\nuser@host $\n git remote add upstream https://www.github.com/opensciencegrid/\nREPOSITORY\n.git\n\n\n\n\n\n\n\n\n\nCreate a branch for the document you plan to convert:\n\n\nuser@host $\n git branch \nBRANCH NAME\n master\n\n\n\n\n\n\n\n\n\nChange to the branch you just created\n\n\nuser@host $\n git checkout \nBRANCH NAME\n\n\n\n\n\n\n\n\n\n\nPreviewing the document tree\n\n\nWhen starting a twiki-converter docker container, it expects your local github repository to be mounted in \n/source\n so that any changes made to the repository are reflected in the mkdocs development server. To start a docker container based off of the twiki-converter docker image:\n\n\n\n\n\n\nCreate a container from the image with the following command:\n\n\nuser@host $\n docker run -d -v \nPATH TO LOCAL GITHUB REPO\n:/source -p \n8000\n twiki\n\n\n\n\n\nThe above command should return the container ID, which will be used in subsequent commands. \n\n\n\n\nNote\n\n\nIf the docker container exits immediately, remove the \n-d\n option for details. If you see permission denied errors, you may need to disable SELinux or put it in permissive mode.\n\n\n\n\n\n\n\n\nTo find the port that your development server is lisetning on, use the container ID (you should only need the first few chars of the ID) returned from the previous command:\n\n\nuser@host $\n docker port \nCONTAINER ID\n\n\n\n\n\n\n\n\n\n\nAccess the development server in your browser via \nhttp://osghost.chtc.wisc.edu:\nPORT\n or \nlocalhost:\nPORT\n for containers run on \nosghost\n or locally, respectively. \nosghost\n has a restrictive firewall so if you have issues accessing your container from outside of the UW-Madison campus, use an SSH tunnel to map the \nosghost\n port to a local port.\n\n\n\n\n\n\nConverting documents\n\n\nThe docker image contains a convenience script, \nconvert-twiki\n for saving archives and converting them to Markdown. To run the script in a running container, run the following command:\n\n\nuser@host $\n docker \nexec\n \nCONTAINER ID\n convert-twiki \nTWIKI URL\n\n\n\n\n\n\nWhere \n is the docker container ID and \n is the link to the TWiki document that you want to convert, e.g. \nhttps://twiki.opensciencegrid.org/bin/view/SoftwareTeam/SoftwareDevelopmentProcess\n. This will result in an archive of the twiki doc, \narchive/SoftwareDevelopmentProcess\n, in your local repo and a converted copy, \nSoftwareDevelopmentProcess.md\n, placed into the root of your local github repository.  If the twiki url is for a specific revision of the document, a \n.rNN\n will be included in the output filenames.\n\n\n\n\nWarning\n\n\nIf the above command does not complete quickly, it means that Pandoc is having an issue with a specific section of the document. See \nTroubleshooting conversion\n for next steps.\n\n\n\n\nTo see the converted document in your browser:\n\n\n\n\nRename, move the converted document into a folder in \ndocs/\n.\n\n\nDocument file names should be lowercase, \n-\n delimited, and descriptive but concise, e.g. \nmarkdown-migration.md\n or \ncutting-release.md\n\n\nIt's not important to get the name/location correct on the first try as this can be discussed in the pull request\n\n\n\n\n\n\nsudo chown\n the archived and converted documents to be owned by you\n\n\nAdd the document to the \npages:\n section of \nmkdocs.yml\n in \ntitle case\n, e.g. \n- Migrating Documents to Markdown: 'software/markdown-migration.md'\n\n\nRefresh the document tree in your browser\n\n\n\n\nOnce you can view the converted document in your browser, move onto the \nnext section\n\n\nTroubleshooting conversion\n\n\nPandoc sometimes has issues converting documents and requires manual intervention by removing whichever section is causing issues in the conversion.\n\n\n\n\nCopy the archive of the document into the root of your git repository\n\n\n\n\nKill the process in the docker container:\n\n\nuser@host $\n docker \nexec\n \nCONTAINER ID\n pkill -9 pandoc\n\n\n\n\n\n\n\n\n\nRemove a section from the copy of the archive to find the problematic section (recommendation: use a binary search strategy)\n\n\n\n\n\n\nRun pandoc manually:\n\n\nuser@host $\n docker \nexec\n \nCONTAINER ID\n pandoc -f twiki -t markdown_github \nARCHIVE COPY\n \n \nMARKDOWN FILE\n\n\n\n\n\n\n\n\n\n\nRepeat steps 2-4 until you've narrowed down the problematic section\n\n\n\n\nManually convert the offending section\n\n\n\n\nConversion without Docker\n\n\nIf you've already used the \ndocker method\n, skip to the section about \ncompleting the conversion\n. \n\n\nRequirements\n\n\nThis method requires the following:\n\n\n\n\nFork\n and \nclone\n the repository that you chose in the \nabove section\n\n\npandoc (\n 1.16)\n\n\nmkdocs\n\n\nMarkdownHighlight\n\n\npygments\n\n\n\n\nPreparing the git repository\n\n\n\n\ncd\n into your local git repository\n\n\n\n\nAdd \nopensciencegrid/technology\n as the upstream remote repository for merging upstream changes:\n\n\nuser@host $\n git remote add upstream https://www.github.com/opensciencegrid/\nREPOSITORY\n.git\n\n\n\n\n\n\n\n\n\nCreate a branch for the document you plan to convert:\n\n\nuser@host $\n git branch \nBRANCH NAME\n master\n\n\n\n\n\n\n\n\n\nChange to the branch you just created\n\n\nuser@host $\n git checkout \nBRANCH NAME\n\n\n\n\n\n\n\n\n\n\nArchiving the TWiki document\n\n\nFollow the instructions for \narchival\n then continue to the next section to convert the document with pandoc.\n\n\nInitial conversion with Pandoc\n\n\nPandoc\n is a tool that's useful for automated conversion of markdown languages. \nOnce installed\n (alternatively, run pandoc \nvia docker\n), run the following command to convert TWiki to Markdown:\n\n\n$\n pandoc -f twiki -t markdown_github \nTWIKI FILE\n \n \nMARKDOWN FILE\n\n\n\n\n\n\nWhere \nTWIKI FILE\n is the path to initial document in raw TWiki and \nMARKDOWN FILE\n is the path to the resulting document in GitHub Markdown.\n\n\n\n\nNote\n\n\nIf you don't see output from the above command quickly, it means that Pandoc is having an issue with a specific section of the document. Stop the command (or docker container), find and temporarily remove the offending section, convert the remainder of the document with Pandoc, and manually convert the offending section.\n\n\n\n\nPreviewing your document(s) with Mkdocs\n\n\nMkdocs\n has a development mode that can be used to preview documents as you work on them and is available via package manager or \npip\n. \nOnce installed\n, add your document(s) to the \npages\n section of \nmkdocs.yml\n and launch the mkdocs server with the following command from the dir containing \nmkdocs.yml\n:\n\n\n$\n \nPYTHONPATH\n=\nsrc/ mkdocs serve\n\n\n\n\n\nAccess the server at \nhttp://127.0.0.1:8000\n and navigate to the document you're working on. It's useful to open the original TWiki doc in an adjacent tab or window to quickly compare the two.\n\n\nCompleting the conversion\n\n\nManual review of the automatically converted documents are required since the automatic conversion process isn't perfect. This section contains a list of problems commonly encountered in automatically converted documents.\n\n\nHeaders\n\n\nUse the following conventions for headers\n\n\n\n\nThere should only be one level 1 heading per document: the title\n\n\nLevel 1 headings should use the \n====\n format\n\n\nLevel 2 sub-headings should use the \n----\n format\n\n\nIf there is no other introduction to the document, remove the \"About this...\" sub-heading\n\n\n\n\nBroken links\n\n\nPandoc isn't aware of the entire TWiki structure so internal links using \nWikiWords\n result in broken links. If the broken link is for a document that has already been migrated to GitHub, link to it using relative paths to the markdown doc of interest. If the broken link is for a document that hasn't been migrated to GitHub, consult the documentation spreadsheet (contact Brian L for access) to see if it's targeted for archival.\n\n\nIf the broken link is:\n\n\n\n\nFor a document that has already been migrated to GitHub, update it to point at the new location.\n\n\nFor a document that not been migrated to GitHub, consult the documentation spreadsheet (contact Brian L for access):\n\n\nIf the link is targeted for archival, remove the link if it makes sense. If you're unsure, be sure to mention it in your final pull request\n\n\nIf the link is not targeted for archival, link directly to the TWiki page.\n\n\n\n\n\n\n\n\nFixing sub-section links\n\n\nTo link sections within a page, lowercase the entire section name and replace spaces with dashes. If there are multiple sections with the same name you can link the subsequent sections by appending \n_N\n where \nN\n is the section's ordinal number minus one, e.g. append \n_1\n for the second section. For example, if you have three sections named \"Optional Configuration\", link them like so:\n\n\n[1st section](#optional-configuration)\n[2nd section](#optional-configuration_1)\n[3rd section](#optional-configuration_2)\n\n\n\n\n\nBroken command blocks and file snippets\n\n\nPandoc doesn't do a good job of converting our \npre class=...\n blocks so manual intervention is required. Command blocks and file snippets should be wrapped in three backticks (```) followed by an optional code highlighting format:\n\n\n```python\n# stuff\n```\n\n\n\n\n\nMake sure to use the TWiki document as a reference when making fixes!\n\n\nWe use the \nPygments\n highlighting library for syntax; it knows about 100 different languages.  The Pygments website contains a live renderer if you want to see how your text will come out.  Please use the \nconsole\n language for shell sessions.\n\n\nFixing root and user prompts\n\n\n\n\n\n\n\n\nFind and replace...\n\n\nWith...\n\n\n\n\n\n\n\n\n\n\nspan class=\"twiki-macro UCL\\_PROMPT\\_ROOT\"\n/span\n\n\nroot@host #\n\n\n\n\n\n\nspan class=\"twiki-macro UCL\\_PROMPT\"\n/span\n\n\nuser@host $\n\n\n\n\n\n\n\n\nHighlighting user input\n\n\nWithin command blocks and file snippets, we've used \nlt;...\ngt;\n to highlight areas that users would have to insert text specific to their site. For now, use desciptive, all-caps text wrapped in angle brackets to indicate user input. You may also use TWiki-style color highlighting. \n\n\nroot@host #\n condor_ce_trace -d \nCE HOSTNAME\n\n\n\n\n\n\nOrdered Lists\n\n\nOrdered lists are often broken up into multiple lists if there are command blocks/file snippets and/or additional text within one of the list items. To make sure the contents of an item are indented properly, use the following formatting:\n\n\n\n\nFor code blocks or file snippets, add an empty line after any regular text, then insert \n(N+1)*4\n spaces at the beginning of each line, where N is the level of the item in the list. To apply code highlighting, start the code block with \n:::\nFORMAT\n; see \nthis page\n for details, including possible highlighting formats.  For an example of formatting a code section inside a list, see \nthe release series document\n.\n\n\nFor additional text (i.e. after a code block), insert \nN*4\n spaces at the beginning of each line, where N is the level of the item in the list.\n\n\n\n\nFor example:\n\n\n1. Foo\n    - Bar\n\n            :::console\n            COMMAND\n            BLOCK\n        text associated with Bar\n\n    text associated with Foo\n\n2. Baz\n\n        FILE\n        SNIPPET\n\n\n\n\n\nThere are 12 spaces and 8 spaces in front of the command block and text associated with \nBar\n, respectively; 4 spaces in front of the text associated with \nFoo\n; and 8 spaces in front of the file snippet associated with \nBaz\n.  The above block is rendered below:\n\n\n\n\n\n\nFoo\n\n\n\n\nBar\nCOMMAND\n\n\nBLOCK\n\n\n\n\n\n\ntext associated with Bar\n\n\n\n\n\n\ntext associated with Foo\n\n\n\n\n\n\nBaz\n\n\nFILE\nSNIPPET\n\n\n\n\n\n\n\n\n\nNotes\n\n\nTo catch the user's attention for important items or pitfalls, we used \n%NOTE%\n TWiki macros, these can be replaced with admonition-style notes and warnings:\n\n\n!!! note\n    things to note\n\n\n\n\n\nor\n\n\n!!! warning\n    if a user doesn\nt do this thing, bad stuff will happen\n\n\n\n\n\nThe above blocks are rendered below as an example.\n\n\n\n\nNote\n\n\nthings to note\n\n\n\n\nand\n\n\n\n\nWarning\n\n\nif a user doesn't do this thing, bad stuff will happen\n\n\n\n\nObvious errors\n\n\nIf you see any other obvious errors (e.g., links to gratia web), feel free to correct them while you're editing the doc \niff\n the changes take less than ~15 minutes. This isn't a renovation project!\n\n\nArchiving Documents\n\n\nIf the document is slated for archival (check if it says \"yes\" in the  \"archived\" column of the spreadsheet), just download the document to the \narchive\n folder of your local git repository:\n\n\nuser@host $\n \ncd\n technology/\n\nuser@host $\n curl \nTWIKI URL\n?raw=text\n \n|\n iconv -f windows-1252 \n archive/\nTWIKI TITLE\n\n\n\n\n\n\nFor example:\n\n\nuser@host $\n \ncd\n technology\n\nuser@host $\n curl \nhttps://twiki.opensciencegrid.org/bin/view/Documentation/Release3/SHA2Compliance?raw=text\n \n|\n iconv -f windows-1252 \n archive/SHA2Compliance\n\n\n\n\n\nAfter downloading the document, continue onto the next section to walk through pull request submission.\n\n\nSubmitting the pull request\n\n\n\n\n\n\nStage the archived raw TWiki (as well as the converted Markdown document(s) and \nmkdocs.yml\n if you are converting the document):\n\n\nuser@host $\n git add mkdocs.yml archive/\nTWIKI ARCHIVE\n \nPATH TO CONVERTED DOC\n\n\n\n\n\n\n\n\n\n\nCommit and push your changes to your GitHub repo:\n\n\nuser@host $\n git commit -m \nCOMMIT MSG\n\n\nuser@host $\n git push origin \nBRANCH NAME\n\n\n\n\n\n\n\n\n\n\nOpen your browser and navigate to your GitHub fork\n\n\n\n\n\n\nSubmit a pull request containing with the following body:\n\n\nLINK TO TWIKI DOCUMENT\n\n\n\n-\n \n[\n \n]\n \nEnter\n \ndate\n \ninto\n \nMigrated\n \ncolumn\n \nof\n \ngoogle\n \nsheet\n\n\n\n\n\n\n\n\n\n\nIf you are migrating a document, also add this task:\n\n\n- [ ] Add migration header to TWiki document\n\n\n\n\n\n\n\n\n\nIf you are archiving a document, add this task:\n\n\n- [ ] Move TWiki document to the trash\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee an example pull request \nhere\n.\n\n\nAfter the pull request\n\n\nAfter the pull request is merged, replace the contents of TWiki document with the div if you're migrating the document, linking to the location of the migrated document:\n\n\ndiv\n \nstyle=\nborder: 1px solid black; margin: 1em 0; padding: 1em; background-color: #FFDDDD; font-weight: 600;\n\nThis document has been migrated to !GitHub (\nLINK\n \nTO\n \nGITHUB\n \nDOCUMENT\n). If you wish to see the old TWiki document, use the TWiki history below.\n\nBackground:\n\nAt the end of year (2017), the TWiki will be retired in favor of !GitHub. You can find the various TWiki webs and their new !GitHub locations listed below:\n\n   * Release3: https://opensciencegrid.github.io/docs ([[https://github.com/opensciencegrid/docs/tree/master/archive][archive]])\n   * !SoftwareTeam: https://opensciencegrid.github.io/technology ([[https://github.com/opensciencegrid/technology/tree/master/archive][archive]])\n\n/div\n\n\n\n\n\n\nIf you are archiving a document, move it to the trash instead. Once the document has been updated or trashed, add the date to the spreadsheet and go back to your pull request and mark your tasks as complete. For example, if you completed the migration of a document:\n\n\n- [X] Enter date into \nMigrated\n column of google sheet\n- [X] Add migration div to TWiki document\n\n\n\n\n\nCurrently, we do not recommend changing backlinks (links on other twiki pages that refer to the Twiki page you are migrating) to point at the new GitHub URL.  This is to provide a simple reminder to users that the migration will occur, and also is likely low priority regardless as all pages will eventually migrate to GitHub.  This advice may change in the future as we gain experience with this transition.\n\n\nReviewing pull requests\n\n\nTo review pull requests, \ncd\n into the dir containing your git repository and check out the requester's branch, which the twiki-converter container should automatically notice. Here's an example checking out Brian's \ncut-sw-release\n branch of the technology repository:\n\n\n#\n Add the requester\ns repo as a remote if you haven\nt already\n\nuser@host $\n git remote add blin https://www.github.com/brianhlin/technology.git\n\nuser@host $\n git fetch --all\n\nuser@host $\n git checkout blin/cut-sw-release\n\n\n\n\n\nRefresh your browser and navigate to the document in the request.", 
            "title": "Migrating Documents to Markdown"
        }, 
        {
            "location": "/software/markdown-migration/#migrating-to-markdown", 
            "text": "As part of the TWiki retirement (the read-only target date of Oct 1, 2017, with a shutdown date in 2018), we will need to convert the OSG Software and Release3 docs from TWiki syntax to  Markdown . The following document outlines the conversion process and conventions.", 
            "title": "Migrating to Markdown"
        }, 
        {
            "location": "/software/markdown-migration/#choosing-the-git-repository", 
            "text": "First you will need to choose which git repoository you will be working with:     If you are converting a document from...  Use this github repository...      SoftwareTeam  technology    Release3  docs     Once you've chosen the target repository for your document, move onto the next section and pick your conversion method.", 
            "title": "Choosing the git repository"
        }, 
        {
            "location": "/software/markdown-migration/#automatic-twiki-conversion", 
            "text": "Note  If you are only archiving the documents, skip to this  section .   Choose one of the following methods for converting TWiki documents:   Using our own  docker conversion image  (recommended)  Directly using pandoc and mkdocs  on your own machine", 
            "title": "Automatic TWiki conversion"
        }, 
        {
            "location": "/software/markdown-migration/#using-docker", 
            "text": "The twiki-converter docker image can be used to preview the document tree via a  mkdocs  development server, archive TWiki documents, and convert documents to Markdown via  pandoc . The image is available on  osghost , otherwise, follow the instructions  here  to build it locally.", 
            "title": "Using docker"
        }, 
        {
            "location": "/software/markdown-migration/#requirements", 
            "text": "To perform a document migration using docker, you will need the following tools and accounts:   Fork  and  clone  the repository that you chose in the  above section  A host with a running docker service  sudo  or membership in the  docker  group   If you cannot install the above tools locally, they are available on  osghost . Speak with Brian L for access.", 
            "title": "Requirements"
        }, 
        {
            "location": "/software/markdown-migration/#preparing-the-git-repository", 
            "text": "cd  into your local git repository   Add  opensciencegrid/technology  as the upstream remote repository for merging upstream changes:  user@host $  git remote add upstream https://www.github.com/opensciencegrid/ REPOSITORY .git    Create a branch for the document you plan to convert:  user@host $  git branch  BRANCH NAME  master    Change to the branch you just created  user@host $  git checkout  BRANCH NAME", 
            "title": "Preparing the git repository"
        }, 
        {
            "location": "/software/markdown-migration/#previewing-the-document-tree", 
            "text": "When starting a twiki-converter docker container, it expects your local github repository to be mounted in  /source  so that any changes made to the repository are reflected in the mkdocs development server. To start a docker container based off of the twiki-converter docker image:    Create a container from the image with the following command:  user@host $  docker run -d -v  PATH TO LOCAL GITHUB REPO :/source -p  8000  twiki  The above command should return the container ID, which will be used in subsequent commands.    Note  If the docker container exits immediately, remove the  -d  option for details. If you see permission denied errors, you may need to disable SELinux or put it in permissive mode.     To find the port that your development server is lisetning on, use the container ID (you should only need the first few chars of the ID) returned from the previous command:  user@host $  docker port  CONTAINER ID     Access the development server in your browser via  http://osghost.chtc.wisc.edu: PORT  or  localhost: PORT  for containers run on  osghost  or locally, respectively.  osghost  has a restrictive firewall so if you have issues accessing your container from outside of the UW-Madison campus, use an SSH tunnel to map the  osghost  port to a local port.", 
            "title": "Previewing the document tree"
        }, 
        {
            "location": "/software/markdown-migration/#converting-documents", 
            "text": "The docker image contains a convenience script,  convert-twiki  for saving archives and converting them to Markdown. To run the script in a running container, run the following command:  user@host $  docker  exec   CONTAINER ID  convert-twiki  TWIKI URL   Where   is the docker container ID and   is the link to the TWiki document that you want to convert, e.g.  https://twiki.opensciencegrid.org/bin/view/SoftwareTeam/SoftwareDevelopmentProcess . This will result in an archive of the twiki doc,  archive/SoftwareDevelopmentProcess , in your local repo and a converted copy,  SoftwareDevelopmentProcess.md , placed into the root of your local github repository.  If the twiki url is for a specific revision of the document, a  .rNN  will be included in the output filenames.   Warning  If the above command does not complete quickly, it means that Pandoc is having an issue with a specific section of the document. See  Troubleshooting conversion  for next steps.   To see the converted document in your browser:   Rename, move the converted document into a folder in  docs/ .  Document file names should be lowercase,  -  delimited, and descriptive but concise, e.g.  markdown-migration.md  or  cutting-release.md  It's not important to get the name/location correct on the first try as this can be discussed in the pull request    sudo chown  the archived and converted documents to be owned by you  Add the document to the  pages:  section of  mkdocs.yml  in  title case , e.g.  - Migrating Documents to Markdown: 'software/markdown-migration.md'  Refresh the document tree in your browser   Once you can view the converted document in your browser, move onto the  next section", 
            "title": "Converting documents"
        }, 
        {
            "location": "/software/markdown-migration/#troubleshooting-conversion", 
            "text": "Pandoc sometimes has issues converting documents and requires manual intervention by removing whichever section is causing issues in the conversion.   Copy the archive of the document into the root of your git repository   Kill the process in the docker container:  user@host $  docker  exec   CONTAINER ID  pkill -9 pandoc    Remove a section from the copy of the archive to find the problematic section (recommendation: use a binary search strategy)    Run pandoc manually:  user@host $  docker  exec   CONTAINER ID  pandoc -f twiki -t markdown_github  ARCHIVE COPY     MARKDOWN FILE     Repeat steps 2-4 until you've narrowed down the problematic section   Manually convert the offending section", 
            "title": "Troubleshooting conversion"
        }, 
        {
            "location": "/software/markdown-migration/#conversion-without-docker", 
            "text": "If you've already used the  docker method , skip to the section about  completing the conversion .", 
            "title": "Conversion without Docker"
        }, 
        {
            "location": "/software/markdown-migration/#requirements_1", 
            "text": "This method requires the following:   Fork  and  clone  the repository that you chose in the  above section  pandoc (  1.16)  mkdocs  MarkdownHighlight  pygments", 
            "title": "Requirements"
        }, 
        {
            "location": "/software/markdown-migration/#preparing-the-git-repository_1", 
            "text": "cd  into your local git repository   Add  opensciencegrid/technology  as the upstream remote repository for merging upstream changes:  user@host $  git remote add upstream https://www.github.com/opensciencegrid/ REPOSITORY .git    Create a branch for the document you plan to convert:  user@host $  git branch  BRANCH NAME  master    Change to the branch you just created  user@host $  git checkout  BRANCH NAME", 
            "title": "Preparing the git repository"
        }, 
        {
            "location": "/software/markdown-migration/#archiving-the-twiki-document", 
            "text": "Follow the instructions for  archival  then continue to the next section to convert the document with pandoc.", 
            "title": "Archiving the TWiki document"
        }, 
        {
            "location": "/software/markdown-migration/#initial-conversion-with-pandoc", 
            "text": "Pandoc  is a tool that's useful for automated conversion of markdown languages.  Once installed  (alternatively, run pandoc  via docker ), run the following command to convert TWiki to Markdown:  $  pandoc -f twiki -t markdown_github  TWIKI FILE     MARKDOWN FILE   Where  TWIKI FILE  is the path to initial document in raw TWiki and  MARKDOWN FILE  is the path to the resulting document in GitHub Markdown.   Note  If you don't see output from the above command quickly, it means that Pandoc is having an issue with a specific section of the document. Stop the command (or docker container), find and temporarily remove the offending section, convert the remainder of the document with Pandoc, and manually convert the offending section.", 
            "title": "Initial conversion with Pandoc"
        }, 
        {
            "location": "/software/markdown-migration/#previewing-your-documents-with-mkdocs", 
            "text": "Mkdocs  has a development mode that can be used to preview documents as you work on them and is available via package manager or  pip .  Once installed , add your document(s) to the  pages  section of  mkdocs.yml  and launch the mkdocs server with the following command from the dir containing  mkdocs.yml :  $   PYTHONPATH = src/ mkdocs serve  Access the server at  http://127.0.0.1:8000  and navigate to the document you're working on. It's useful to open the original TWiki doc in an adjacent tab or window to quickly compare the two.", 
            "title": "Previewing your document(s) with Mkdocs"
        }, 
        {
            "location": "/software/markdown-migration/#completing-the-conversion", 
            "text": "Manual review of the automatically converted documents are required since the automatic conversion process isn't perfect. This section contains a list of problems commonly encountered in automatically converted documents.", 
            "title": "Completing the conversion"
        }, 
        {
            "location": "/software/markdown-migration/#headers", 
            "text": "Use the following conventions for headers   There should only be one level 1 heading per document: the title  Level 1 headings should use the  ====  format  Level 2 sub-headings should use the  ----  format  If there is no other introduction to the document, remove the \"About this...\" sub-heading", 
            "title": "Headers"
        }, 
        {
            "location": "/software/markdown-migration/#broken-links", 
            "text": "Pandoc isn't aware of the entire TWiki structure so internal links using  WikiWords  result in broken links. If the broken link is for a document that has already been migrated to GitHub, link to it using relative paths to the markdown doc of interest. If the broken link is for a document that hasn't been migrated to GitHub, consult the documentation spreadsheet (contact Brian L for access) to see if it's targeted for archival.  If the broken link is:   For a document that has already been migrated to GitHub, update it to point at the new location.  For a document that not been migrated to GitHub, consult the documentation spreadsheet (contact Brian L for access):  If the link is targeted for archival, remove the link if it makes sense. If you're unsure, be sure to mention it in your final pull request  If the link is not targeted for archival, link directly to the TWiki page.", 
            "title": "Broken links"
        }, 
        {
            "location": "/software/markdown-migration/#fixing-sub-section-links", 
            "text": "To link sections within a page, lowercase the entire section name and replace spaces with dashes. If there are multiple sections with the same name you can link the subsequent sections by appending  _N  where  N  is the section's ordinal number minus one, e.g. append  _1  for the second section. For example, if you have three sections named \"Optional Configuration\", link them like so:  [1st section](#optional-configuration)\n[2nd section](#optional-configuration_1)\n[3rd section](#optional-configuration_2)", 
            "title": "Fixing sub-section links"
        }, 
        {
            "location": "/software/markdown-migration/#broken-command-blocks-and-file-snippets", 
            "text": "Pandoc doesn't do a good job of converting our  pre class=...  blocks so manual intervention is required. Command blocks and file snippets should be wrapped in three backticks (```) followed by an optional code highlighting format:  ```python\n# stuff\n```  Make sure to use the TWiki document as a reference when making fixes!  We use the  Pygments  highlighting library for syntax; it knows about 100 different languages.  The Pygments website contains a live renderer if you want to see how your text will come out.  Please use the  console  language for shell sessions.", 
            "title": "Broken command blocks and file snippets"
        }, 
        {
            "location": "/software/markdown-migration/#fixing-root-and-user-prompts", 
            "text": "Find and replace...  With...      span class=\"twiki-macro UCL\\_PROMPT\\_ROOT\" /span  root@host #    span class=\"twiki-macro UCL\\_PROMPT\" /span  user@host $", 
            "title": "Fixing root and user prompts"
        }, 
        {
            "location": "/software/markdown-migration/#highlighting-user-input", 
            "text": "Within command blocks and file snippets, we've used  lt;... gt;  to highlight areas that users would have to insert text specific to their site. For now, use desciptive, all-caps text wrapped in angle brackets to indicate user input. You may also use TWiki-style color highlighting.   root@host #  condor_ce_trace -d  CE HOSTNAME", 
            "title": "Highlighting user input"
        }, 
        {
            "location": "/software/markdown-migration/#ordered-lists", 
            "text": "Ordered lists are often broken up into multiple lists if there are command blocks/file snippets and/or additional text within one of the list items. To make sure the contents of an item are indented properly, use the following formatting:   For code blocks or file snippets, add an empty line after any regular text, then insert  (N+1)*4  spaces at the beginning of each line, where N is the level of the item in the list. To apply code highlighting, start the code block with  ::: FORMAT ; see  this page  for details, including possible highlighting formats.  For an example of formatting a code section inside a list, see  the release series document .  For additional text (i.e. after a code block), insert  N*4  spaces at the beginning of each line, where N is the level of the item in the list.   For example:  1. Foo\n    - Bar\n\n            :::console\n            COMMAND\n            BLOCK\n        text associated with Bar\n\n    text associated with Foo\n\n2. Baz\n\n        FILE\n        SNIPPET  There are 12 spaces and 8 spaces in front of the command block and text associated with  Bar , respectively; 4 spaces in front of the text associated with  Foo ; and 8 spaces in front of the file snippet associated with  Baz .  The above block is rendered below:    Foo   Bar COMMAND  BLOCK   text associated with Bar    text associated with Foo    Baz  FILE\nSNIPPET", 
            "title": "Ordered Lists"
        }, 
        {
            "location": "/software/markdown-migration/#notes", 
            "text": "To catch the user's attention for important items or pitfalls, we used  %NOTE%  TWiki macros, these can be replaced with admonition-style notes and warnings:  !!! note\n    things to note  or  !!! warning\n    if a user doesn t do this thing, bad stuff will happen  The above blocks are rendered below as an example.   Note  things to note   and   Warning  if a user doesn't do this thing, bad stuff will happen", 
            "title": "Notes"
        }, 
        {
            "location": "/software/markdown-migration/#obvious-errors", 
            "text": "If you see any other obvious errors (e.g., links to gratia web), feel free to correct them while you're editing the doc  iff  the changes take less than ~15 minutes. This isn't a renovation project!", 
            "title": "Obvious errors"
        }, 
        {
            "location": "/software/markdown-migration/#archiving-documents", 
            "text": "If the document is slated for archival (check if it says \"yes\" in the  \"archived\" column of the spreadsheet), just download the document to the  archive  folder of your local git repository:  user@host $   cd  technology/ user@host $  curl  TWIKI URL ?raw=text   |  iconv -f windows-1252   archive/ TWIKI TITLE   For example:  user@host $   cd  technology user@host $  curl  https://twiki.opensciencegrid.org/bin/view/Documentation/Release3/SHA2Compliance?raw=text   |  iconv -f windows-1252   archive/SHA2Compliance  After downloading the document, continue onto the next section to walk through pull request submission.", 
            "title": "Archiving Documents"
        }, 
        {
            "location": "/software/markdown-migration/#submitting-the-pull-request", 
            "text": "Stage the archived raw TWiki (as well as the converted Markdown document(s) and  mkdocs.yml  if you are converting the document):  user@host $  git add mkdocs.yml archive/ TWIKI ARCHIVE   PATH TO CONVERTED DOC     Commit and push your changes to your GitHub repo:  user@host $  git commit -m  COMMIT MSG  user@host $  git push origin  BRANCH NAME     Open your browser and navigate to your GitHub fork    Submit a pull request containing with the following body:  LINK TO TWIKI DOCUMENT  -   [   ]   Enter   date   into   Migrated   column   of   google   sheet     If you are migrating a document, also add this task:  - [ ] Add migration header to TWiki document    If you are archiving a document, add this task:  - [ ] Move TWiki document to the trash      See an example pull request  here .", 
            "title": "Submitting the pull request"
        }, 
        {
            "location": "/software/markdown-migration/#after-the-pull-request", 
            "text": "After the pull request is merged, replace the contents of TWiki document with the div if you're migrating the document, linking to the location of the migrated document:  div   style= border: 1px solid black; margin: 1em 0; padding: 1em; background-color: #FFDDDD; font-weight: 600; \nThis document has been migrated to !GitHub ( LINK   TO   GITHUB   DOCUMENT ). If you wish to see the old TWiki document, use the TWiki history below.\n\nBackground:\n\nAt the end of year (2017), the TWiki will be retired in favor of !GitHub. You can find the various TWiki webs and their new !GitHub locations listed below:\n\n   * Release3: https://opensciencegrid.github.io/docs ([[https://github.com/opensciencegrid/docs/tree/master/archive][archive]])\n   * !SoftwareTeam: https://opensciencegrid.github.io/technology ([[https://github.com/opensciencegrid/technology/tree/master/archive][archive]]) /div   If you are archiving a document, move it to the trash instead. Once the document has been updated or trashed, add the date to the spreadsheet and go back to your pull request and mark your tasks as complete. For example, if you completed the migration of a document:  - [X] Enter date into  Migrated  column of google sheet\n- [X] Add migration div to TWiki document  Currently, we do not recommend changing backlinks (links on other twiki pages that refer to the Twiki page you are migrating) to point at the new GitHub URL.  This is to provide a simple reminder to users that the migration will occur, and also is likely low priority regardless as all pages will eventually migrate to GitHub.  This advice may change in the future as we gain experience with this transition.", 
            "title": "After the pull request"
        }, 
        {
            "location": "/software/markdown-migration/#reviewing-pull-requests", 
            "text": "To review pull requests,  cd  into the dir containing your git repository and check out the requester's branch, which the twiki-converter container should automatically notice. Here's an example checking out Brian's  cut-sw-release  branch of the technology repository:  #  Add the requester s repo as a remote if you haven t already user@host $  git remote add blin https://www.github.com/brianhlin/technology.git user@host $  git fetch --all user@host $  git checkout blin/cut-sw-release  Refresh your browser and navigate to the document in the request.", 
            "title": "Reviewing pull requests"
        }, 
        {
            "location": "/software/koji-workflow/", 
            "text": "Koji Workflow\n\n\nThis covers the basics of using and understanding the \nOSG Koji\n instance. It is meant primarily for OSG Software team members who need to interact with the service.\n\n\nTerminology\n\n\nUsing and understanding the following terminology correctly will help in the reading of this document:\n\n\nPackage\n\nThis refers to a named piece of software in the Koji database. An example would be \"lcmaps\".\n\n\nBuild\n\nA specific version and release of a package, and an associated state. A build state may be successful (and contain RPMs), failed, or in-progress. A given build may be in one or more tags. The build is associated with the output of the latest build task with the same version and release of the package.\n\n\nTag\n\nA named set of packages and builds, parent tags, and reference to external repositories. An example would be the \"osg-3.3-el6-development\" tag, which contains (among others) the \"lcmaps\" package and the \"lcmaps-1.6.6-1.1.osg33.el6\" build. There is an inheritance structure to tags: by default, all packages/builds in a parent tag are added to the tag. A tag may contain a reference to (possibly inherited) external repositories; the RPMs in these repositories are added to repositories created from this tag. Examples of referenced external repositories include CentOS base, EPEL, or JPackage.\n\n\n\n\nNote\n\n\nA tag is NOT a yum repository.\n\n\n\n\nTarget\n\nA target consists of a build tag and a destination tag. An example is \"osg-3.3-el6\", where the build tag is \"osg-3.3-el6-build\" and the destination tag is \"osg-3.3-el6\". A target is used by the build task to know what repository to build from and tag to build into.\n\n\nTask\n\nA unit of work for Koji. Several common tasks are:\n\n\n\n\n\n\nbuild\n\n    This task takes a SRPM and a target, and attempts to create a complete Build in the target's destination tag from the target's build repository. This task will launch one buildArch task for each architecture in the destination tag; if each subtask is successful, then it will launch a tagBuild subtask.\n\n\n\n\nNote\n\n\nIf the build task is marked as \"scratch\", then it won't result in a saved Build.\n\n\n\n\n\n\n\n\nbuildArch\n\n    This task takes a SRPM, architecture name, and a Koji repository as an input, and runs \nmock\n to create output RPMs for that arch. The build artifacts are added to the Build if all buildArch tasks are successful.\n\n\n\n\n\n\ntagBuild\n\n    This adds a successful build to a given tag.\n\n\n\n\n\n\nnewRepo\n\n    This creates a new repository from a given tag.\n\n\n\n\n\n\nBuild artifacts\n\nThe results of a buildArch task. Their metadata are recorded in the Koji database, and files are saved to disk. Metadata may include checksums, timestamps, and who initiated the task. Artifacts may include RPMs, SRPMs, and build logs.\n\n\nRepository\n\nA yum repository created from the contents of a tag at a specific point in time. By default, the yum repository will contain all successful, non-blocked builds in the tag, plus all RPMs in the external repositories for the tag.\n\n\nFurther reading\n\n\n\n\nOfficial Koji documentation: \nhttps://docs.pagure.org/koji/\n\n\nFedora's koji documentation: \nhttps://fedoraproject.org/wiki/Koji\n\n\nFedora's \"Using Koji\" page: \nhttps://fedoraproject.org/wiki/Using_the_Koji_build_system\n Note that some instructions there may not apply to OSG's Koji. For the most part though, they are useful.\n\n\n\n\nUsing Koji\n\n\nRequired Software\n\n\nUsing Koji requires:\n\n\n\n\nosg-build\n version 1.6.3 or later.\n\n\nkoji\n 1.6.0-2.osg or later. \nNote\n that you want a koji build from osg; the output of \nrpm -q koji\n should end in \".osg\".\n\n\n\n\nBoth pieces of software are available from the osg repositories. \nosg-build\n may also be obtained from GitHub by cloning out \nhttps://github.com/opensciencegrid/osg-build\n\n\nSpecial instructions for UW-Madison CSL machines:\n\n\n\n\n\n\nClone the osg-build GitHub repo:\n\n\n[you@host]$\n git clone https://github.com/opensciencegrid/osg-build\n\n\n\n\n\n\n\n\n\nAdd this directory to your \n$PATH\n\n\n\n\n\n\nRun\n\n\n[you@host]$\n osg-koji setup\n\n\n\n\n\nto set up the koji configuration and certificates in \n~/.osg-koji\n\n\n\n\n\n\nObtaining a login\n\n\nYou will be using your grid certificate to log in. Email a Koji admin the DN of your certificate, and we will set up a Koji account with the appropriate permissions.\n\n\nIf you are switching certificate providers, you will need to email a Koji admin with your new DN. You will also need to clear your browser cookies and cache for \nhttps://koji.chtc.wisc.edu\n before trying to use the Koji web interface again. If your CN has changed, you will not be able to use your old certificate.\n\n\nCurrent Koji admins are Mat Selmeci and Carl Edquist.\n\n\nConfiguring certificate authentication\n\n\nYou must also configure certificate authentication for the command-line tools on your build host:\n\n\n\n\n\n\nRun\n\n\n[you@host]$\n osg-koji setup\n\n\n\n\n\nto set up the appropriate configuration and certificates in \n~/.osg-koji\n\n\n\n\n\n\nAfter this, you will also be able to run koji commands manually by using the \nosg-koji\n wrapper script. You might need to rerun \nosg-koji setup\n if you renew or change your cert.\n\n\nCreating a new build\n\n\nWe create a new build in Koji from the package's directory in OSG Software subversion.\n\n\nIf a successful build already exists in Koji (regardless of whether it is in the tag you use), you cannot replace the build. Two builds are the same if they have the same NVR (Name-Version-Release). You \ncan\n do a \"scratch\" build, which recompiles, but the results are not added to the tag. This is useful for experimenting with koji.\n\n\nTo do a build, execute the following command from within the OSG Software subversion checkout:\n\n\n[you@host]$\n osg-build koji \nPACKAGE NAME\n\n\n\n\n\n\nTo do a scratch build, simply add the \n--scratch\n command line flag.\n\n\nEach invocation of osg-build will ask for the password once or twice; if you get asked more like 20 times, then you may not be running the OSG-patched version of Koji; try switching to the one from the osg-development repository.\n\n\nWhen you do a non-scratch build, it will build with the \nosg-el6\n and \nosg-el7\n targets. This will assign your build the \nosg-3.4-el6-development\n and \nosg-3.4-el7-development\n tags (and your package will be assigned the \nosg-el6\n and \nosg-el7\n tags). If successful, your build will end up in the Koji \nosg-minefield\n yum repos and will eventually show up in the \nosg-development\n yum repos. This is a high latency process.\n\n\nBuild task Results\n\n\nHow to find build results\n\n\nThe most recent build results are always shown on the home page of Koji:\n\n\nhttps://koji.chtc.wisc.edu/koji/index\n\n\nClicking on a build result brings you to the build information page. A successful build will result in the build page having build logs, RPMs, and a SRPM.\n\n\nIf your build isn't in the recent list, you can use the search box in the upper-right-hand corner. Type the exact package name (or use a wildcard), and it will bring up a list of all builds for that package. You can find your build from there. For example, the \"lcmaps\" package page is here:\n\n\nhttps://koji.chtc.wisc.edu/koji/packageinfo?packageID=56\n\n\nAnd the lcmaps-1.6.6-1.1.osg33.el6 build is here:\n\n\nhttps://koji.chtc.wisc.edu/koji/buildinfo?buildID=7427\n\n\nTrying our your build\n\n\nBecause it takes a while for your build to get into one of the regular repositories, it's simplest to download your RPM directly (see the previous section on How to find build results), and install it with:\n\n\n[root@host]#\n yum localinstall \nRPM\n\n\n\n\n\n\nHow to get the resulting RPM into a repository\n\n\nOnce a package has been built, it is added to a tag. We then must turn the tag into a yum repository. This is normally done automatically and you do not need to deal with it yourself. Three notes:\n\n\n\n\nThe kojira daemon creates a repository automatically post-build on the koji-hub host. Eventually, the development repository will be the one hosted by koji-hub.\n\n\n\n\nThe koji-hub repository can be created manually by running\n\n\n[you@host]$\n osg-koji regen-repo \nTAG NAME\n\n\n\n\n\n\nFor example, the tag name for osg-development in 3.4 on el6 is \"osg-3.4-el6-development\". Likely, you won't need to do this when kojira is working.\n-   Repositories are created on external hosts with the \nmash\n tool. These are usually triggered by cron jobs, but may be run by hand too. Documentation for running mash is on the TODO list.\n-   You can create your own personal repository using \nmash\n.\n\n\n\n\n\n\nDebugging build issues\n\n\n\n\n\n\nFailed build tasks can be seen from the Koji homepage. The logs from the tasks are included. Relevant logs include:\n\n\n\n\n\n\nroot.log\n\n    This is the log of mock trying to create an appropriate build root for your RPM. This will invoke yum twice: once to create a generic build root, once for all the dependencies in your BuildRequires. All RPMs in your build root will be logged here. If mock is unable to create the build root, the reason will show up here.\n\n\n\n\n\n\nbuild.log\n\n    The output of the rpmbuild executable. If your package fails to compile, the reason will show up here.\n\n\n\n\n\n\n\n\n\n\nOne input to the buildArch task is a repository, which is based on a Koji tag. If the repository hasn't been recreated for a dependency you need (for example, when kojira isn't working), you may not have the right RPMs available in your build root.\n\n\n\n\n\n\nOne common issue is building a chain of dependencies. For example, suppose build B depends on the results of build A. If you build A then build B immediately, B will likely fail. This is because A is not in the repository that B uses. The proper string of events building A, starting the regeneration of the destination and build repo (which should happen in a few minutes of the build A task completing), then submitting build task B.\n\n\n\n\nNote\n\n\nif you submit build task B while the build repository task is open, it will not start until the build task has finished.\n\n\n\n\n\n\n\n\n\n\n\n\nPromoting Builds from Development -\n Testing\n\n\nSoftware contributors can promote any package to testing. Members of the security team can promote ca-cert packages to testing.\n\n\nTo promote from development to testing:\n\n\nUsing \nosg-promote\n\n\nIf you want to promote the latest version:\n\n\n[you@host]$\n osg-promote -r \nOSGVER\n-testing \nPACKAGE NAME\n\n\n\n\n\n\nPACKAGE NAME\n is the bare package name without version, e.g. \ngratia-probe\n.\n\n\nIf you want to promote a specific version:\n\n\n[you@host]$\n osg-promote -r \nOSGVER\n-testing \nBUILD NAME\n\n\n\n\n\n\nBUILD NAME\n is a full \nname-version-revision.disttag\n such as \ngratia-probe-1.17.0-2.osg33.el6\n.\n\n\nOSGVER\n is the OSG major version that you are promoting for (e.g. \n3.4\n).\n\n\nosg-promote\n will promote both the el6 and el7 builds of a package. After promoting, copy and paste the JIRA code \nosg-promote\n produces into the JIRA ticket that you are working on.\n\n\nFor \nosg-promote\n, you may omit the \n.osg34.el6\n or \n.osg34.el7\n; the script will add the appropriate disttag on.\n\n\nSee \nOSG Building Tools\n for full details on \nosg-promote\n.", 
            "title": "Koji Workflow"
        }, 
        {
            "location": "/software/koji-workflow/#koji-workflow", 
            "text": "This covers the basics of using and understanding the  OSG Koji  instance. It is meant primarily for OSG Software team members who need to interact with the service.", 
            "title": "Koji Workflow"
        }, 
        {
            "location": "/software/koji-workflow/#terminology", 
            "text": "Using and understanding the following terminology correctly will help in the reading of this document:  Package \nThis refers to a named piece of software in the Koji database. An example would be \"lcmaps\".  Build \nA specific version and release of a package, and an associated state. A build state may be successful (and contain RPMs), failed, or in-progress. A given build may be in one or more tags. The build is associated with the output of the latest build task with the same version and release of the package.  Tag \nA named set of packages and builds, parent tags, and reference to external repositories. An example would be the \"osg-3.3-el6-development\" tag, which contains (among others) the \"lcmaps\" package and the \"lcmaps-1.6.6-1.1.osg33.el6\" build. There is an inheritance structure to tags: by default, all packages/builds in a parent tag are added to the tag. A tag may contain a reference to (possibly inherited) external repositories; the RPMs in these repositories are added to repositories created from this tag. Examples of referenced external repositories include CentOS base, EPEL, or JPackage.   Note  A tag is NOT a yum repository.   Target \nA target consists of a build tag and a destination tag. An example is \"osg-3.3-el6\", where the build tag is \"osg-3.3-el6-build\" and the destination tag is \"osg-3.3-el6\". A target is used by the build task to know what repository to build from and tag to build into.  Task \nA unit of work for Koji. Several common tasks are:    build \n    This task takes a SRPM and a target, and attempts to create a complete Build in the target's destination tag from the target's build repository. This task will launch one buildArch task for each architecture in the destination tag; if each subtask is successful, then it will launch a tagBuild subtask.   Note  If the build task is marked as \"scratch\", then it won't result in a saved Build.     buildArch \n    This task takes a SRPM, architecture name, and a Koji repository as an input, and runs  mock  to create output RPMs for that arch. The build artifacts are added to the Build if all buildArch tasks are successful.    tagBuild \n    This adds a successful build to a given tag.    newRepo \n    This creates a new repository from a given tag.    Build artifacts \nThe results of a buildArch task. Their metadata are recorded in the Koji database, and files are saved to disk. Metadata may include checksums, timestamps, and who initiated the task. Artifacts may include RPMs, SRPMs, and build logs.  Repository \nA yum repository created from the contents of a tag at a specific point in time. By default, the yum repository will contain all successful, non-blocked builds in the tag, plus all RPMs in the external repositories for the tag.", 
            "title": "Terminology"
        }, 
        {
            "location": "/software/koji-workflow/#further-reading", 
            "text": "Official Koji documentation:  https://docs.pagure.org/koji/  Fedora's koji documentation:  https://fedoraproject.org/wiki/Koji  Fedora's \"Using Koji\" page:  https://fedoraproject.org/wiki/Using_the_Koji_build_system  Note that some instructions there may not apply to OSG's Koji. For the most part though, they are useful.", 
            "title": "Further reading"
        }, 
        {
            "location": "/software/koji-workflow/#using-koji", 
            "text": "", 
            "title": "Using Koji"
        }, 
        {
            "location": "/software/koji-workflow/#required-software", 
            "text": "Using Koji requires:   osg-build  version 1.6.3 or later.  koji  1.6.0-2.osg or later.  Note  that you want a koji build from osg; the output of  rpm -q koji  should end in \".osg\".   Both pieces of software are available from the osg repositories.  osg-build  may also be obtained from GitHub by cloning out  https://github.com/opensciencegrid/osg-build", 
            "title": "Required Software"
        }, 
        {
            "location": "/software/koji-workflow/#special-instructions-for-uw-madison-csl-machines", 
            "text": "Clone the osg-build GitHub repo:  [you@host]$  git clone https://github.com/opensciencegrid/osg-build    Add this directory to your  $PATH    Run  [you@host]$  osg-koji setup  to set up the koji configuration and certificates in  ~/.osg-koji", 
            "title": "Special instructions for UW-Madison CSL machines:"
        }, 
        {
            "location": "/software/koji-workflow/#obtaining-a-login", 
            "text": "You will be using your grid certificate to log in. Email a Koji admin the DN of your certificate, and we will set up a Koji account with the appropriate permissions.  If you are switching certificate providers, you will need to email a Koji admin with your new DN. You will also need to clear your browser cookies and cache for  https://koji.chtc.wisc.edu  before trying to use the Koji web interface again. If your CN has changed, you will not be able to use your old certificate.  Current Koji admins are Mat Selmeci and Carl Edquist.", 
            "title": "Obtaining a login"
        }, 
        {
            "location": "/software/koji-workflow/#configuring-certificate-authentication", 
            "text": "You must also configure certificate authentication for the command-line tools on your build host:    Run  [you@host]$  osg-koji setup  to set up the appropriate configuration and certificates in  ~/.osg-koji    After this, you will also be able to run koji commands manually by using the  osg-koji  wrapper script. You might need to rerun  osg-koji setup  if you renew or change your cert.", 
            "title": "Configuring certificate authentication"
        }, 
        {
            "location": "/software/koji-workflow/#creating-a-new-build", 
            "text": "We create a new build in Koji from the package's directory in OSG Software subversion.  If a successful build already exists in Koji (regardless of whether it is in the tag you use), you cannot replace the build. Two builds are the same if they have the same NVR (Name-Version-Release). You  can  do a \"scratch\" build, which recompiles, but the results are not added to the tag. This is useful for experimenting with koji.  To do a build, execute the following command from within the OSG Software subversion checkout:  [you@host]$  osg-build koji  PACKAGE NAME   To do a scratch build, simply add the  --scratch  command line flag.  Each invocation of osg-build will ask for the password once or twice; if you get asked more like 20 times, then you may not be running the OSG-patched version of Koji; try switching to the one from the osg-development repository.  When you do a non-scratch build, it will build with the  osg-el6  and  osg-el7  targets. This will assign your build the  osg-3.4-el6-development  and  osg-3.4-el7-development  tags (and your package will be assigned the  osg-el6  and  osg-el7  tags). If successful, your build will end up in the Koji  osg-minefield  yum repos and will eventually show up in the  osg-development  yum repos. This is a high latency process.", 
            "title": "Creating a new build"
        }, 
        {
            "location": "/software/koji-workflow/#build-task-results", 
            "text": "", 
            "title": "Build task Results"
        }, 
        {
            "location": "/software/koji-workflow/#how-to-find-build-results", 
            "text": "The most recent build results are always shown on the home page of Koji:  https://koji.chtc.wisc.edu/koji/index  Clicking on a build result brings you to the build information page. A successful build will result in the build page having build logs, RPMs, and a SRPM.  If your build isn't in the recent list, you can use the search box in the upper-right-hand corner. Type the exact package name (or use a wildcard), and it will bring up a list of all builds for that package. You can find your build from there. For example, the \"lcmaps\" package page is here:  https://koji.chtc.wisc.edu/koji/packageinfo?packageID=56  And the lcmaps-1.6.6-1.1.osg33.el6 build is here:  https://koji.chtc.wisc.edu/koji/buildinfo?buildID=7427", 
            "title": "How to find build results"
        }, 
        {
            "location": "/software/koji-workflow/#trying-our-your-build", 
            "text": "Because it takes a while for your build to get into one of the regular repositories, it's simplest to download your RPM directly (see the previous section on How to find build results), and install it with:  [root@host]#  yum localinstall  RPM", 
            "title": "Trying our your build"
        }, 
        {
            "location": "/software/koji-workflow/#how-to-get-the-resulting-rpm-into-a-repository", 
            "text": "Once a package has been built, it is added to a tag. We then must turn the tag into a yum repository. This is normally done automatically and you do not need to deal with it yourself. Three notes:   The kojira daemon creates a repository automatically post-build on the koji-hub host. Eventually, the development repository will be the one hosted by koji-hub.   The koji-hub repository can be created manually by running  [you@host]$  osg-koji regen-repo  TAG NAME   For example, the tag name for osg-development in 3.4 on el6 is \"osg-3.4-el6-development\". Likely, you won't need to do this when kojira is working.\n-   Repositories are created on external hosts with the  mash  tool. These are usually triggered by cron jobs, but may be run by hand too. Documentation for running mash is on the TODO list.\n-   You can create your own personal repository using  mash .", 
            "title": "How to get the resulting RPM into a repository"
        }, 
        {
            "location": "/software/koji-workflow/#debugging-build-issues", 
            "text": "Failed build tasks can be seen from the Koji homepage. The logs from the tasks are included. Relevant logs include:    root.log \n    This is the log of mock trying to create an appropriate build root for your RPM. This will invoke yum twice: once to create a generic build root, once for all the dependencies in your BuildRequires. All RPMs in your build root will be logged here. If mock is unable to create the build root, the reason will show up here.    build.log \n    The output of the rpmbuild executable. If your package fails to compile, the reason will show up here.      One input to the buildArch task is a repository, which is based on a Koji tag. If the repository hasn't been recreated for a dependency you need (for example, when kojira isn't working), you may not have the right RPMs available in your build root.    One common issue is building a chain of dependencies. For example, suppose build B depends on the results of build A. If you build A then build B immediately, B will likely fail. This is because A is not in the repository that B uses. The proper string of events building A, starting the regeneration of the destination and build repo (which should happen in a few minutes of the build A task completing), then submitting build task B.   Note  if you submit build task B while the build repository task is open, it will not start until the build task has finished.", 
            "title": "Debugging build issues"
        }, 
        {
            "location": "/software/koji-workflow/#promoting-builds-from-development-testing", 
            "text": "Software contributors can promote any package to testing. Members of the security team can promote ca-cert packages to testing.  To promote from development to testing:", 
            "title": "Promoting Builds from Development -&gt; Testing"
        }, 
        {
            "location": "/software/koji-workflow/#using-osg-promote", 
            "text": "If you want to promote the latest version:  [you@host]$  osg-promote -r  OSGVER -testing  PACKAGE NAME   PACKAGE NAME  is the bare package name without version, e.g.  gratia-probe .  If you want to promote a specific version:  [you@host]$  osg-promote -r  OSGVER -testing  BUILD NAME   BUILD NAME  is a full  name-version-revision.disttag  such as  gratia-probe-1.17.0-2.osg33.el6 .  OSGVER  is the OSG major version that you are promoting for (e.g.  3.4 ).  osg-promote  will promote both the el6 and el7 builds of a package. After promoting, copy and paste the JIRA code  osg-promote  produces into the JIRA ticket that you are working on.  For  osg-promote , you may omit the  .osg34.el6  or  .osg34.el7 ; the script will add the appropriate disttag on.  See  OSG Building Tools  for full details on  osg-promote .", 
            "title": "Using osg-promote"
        }, 
        {
            "location": "/software/new-team-member/", 
            "text": "Setup Instructions for New Team Members\n\n\nAccounts, Group Memberships, Certificates, Etc.\n\n\n\n\nComputing account at FNAL\n\n\nTo get this, follow the instructions at \nhttps://fermi.service-now.com/kb_view.do?sysparm_article=KB0010797\n\n\n\n\n\n\nssh access to a UW CompSci account, including AFS access\n\n\nSend email to Tim C with top 3 requested usernames\n\n\n\n\n\n\nRead/write access to the UW Subversion repository;\n\n\nSend email to Mat or Tim C\n\n\n\n\n\n\nGrid certificate\n\n\nhttps://twiki.opensciencegrid.org/bin/view/Documentation/CertificateGetWeb\n \n\n\n\n\n\n\nAccess to FermiCloud\n\n\nhttp://fclweb.fnal.gov/\n\n\n\n\n\n\nJira ticket system\n\n\nSend email to \n and request access to JIRA\n\n\n\n\n\n\nAccess to Koji\n\n\nSend email to Mat or Carl with DN of grid certificate", 
            "title": "New Team Member"
        }, 
        {
            "location": "/software/new-team-member/#setup-instructions-for-new-team-members", 
            "text": "", 
            "title": "Setup Instructions for New Team Members"
        }, 
        {
            "location": "/software/new-team-member/#accounts-group-memberships-certificates-etc", 
            "text": "Computing account at FNAL  To get this, follow the instructions at  https://fermi.service-now.com/kb_view.do?sysparm_article=KB0010797    ssh access to a UW CompSci account, including AFS access  Send email to Tim C with top 3 requested usernames    Read/write access to the UW Subversion repository;  Send email to Mat or Tim C    Grid certificate  https://twiki.opensciencegrid.org/bin/view/Documentation/CertificateGetWeb      Access to FermiCloud  http://fclweb.fnal.gov/    Jira ticket system  Send email to   and request access to JIRA    Access to Koji  Send email to Mat or Carl with DN of grid certificate", 
            "title": "Accounts, Group Memberships, Certificates, Etc."
        }, 
        {
            "location": "/software/repository-management/", 
            "text": "Repository Management\n\n\nThis document attempts to record everything there is to know about repository management for the OSG.\n\n\nPublic repositories\n\n\nWe host four public-facing repositories at \nrepo.grid.iu.edu\n:\n\n\n\n\n\n\ndevelopment\n: This repository is the bleeding edge. Installing from this repository may cause the host to stop functioning, and we will not assist in undoing any damage.\n\n\n\n\n\n\ntesting\n: This repository contains software ready for testing. If you install packages from here, they may be buggy, but we will provide limited assistance in providing a migration path to a fixed verison.\n\n\n\n\n\n\nrelease\n: This repository contains software that we are willing to support and can be used by the general community.\n\n\n\n\n\n\ncontrib\n: RPMs contributed from outside the OSG.\n\n\n\n\n\n\nThese repos are updated by the \nmash\n script running on \nrepo1.grid.iu.edu\n and \nrepo2.grid.iu.edu\n.\n\n\nInternal repositories\n\n\nIn addition to the public repositories above, we host two repositories on \nkoji.chtc.wisc.edu\n. These are updated shortly after jobs are built into them or tagged into them. They are technically publicly accessible, but we discourage the public from using them.\n\n\n\n\n\n\nminefield\n: This repository is a copy of development above.\n\n\n\n\n\n\nprerelease\n: This repository is a staging area for software that is slated to be in the next release.\n\n\n\n\n\n\nThese repos are updated by the \nkojira\n daemon running on \nkoji.chtc.wisc.edu\n.\n\n\nBuild repositories\n\n\nThe \nkoji\n task in \nosg-build\n uses the \nosg-3.4-el6-build\n/\nosg-3.4-el7-build\n repo, which is the union of the following repositories:\n\n\n\n\nMinefield a.k.a. \nosg-3.4-el6-development\n / \nosg-3.4-el7-development\n\n\nThe \nosg-el6-internal\n / \nosg-el7-internal\n tag (containing build dependencies we do not want to make public)\n\n\nThe \ndist-el6-build\n / \ndist-el7-build\n tag (consisting of the appropriate macros for %dist)\n\n\nCentOS and EPEL\n\n\n\n\nKoji will work from its internal cache of the above repositories (downloading the packages from the source), and will not update until the build repository is regenerated. By default, Koji does a groupinstall of the build group, then resolves the BuildRequires dependencies.\n\n\nThe tarball creation scripts use the \nosg-3.4-el6-release-build\n / \nosg-3.4-el7-release-build\n repo, which is the union of the following repositories:\n\n\n\n\nRelease a.k.a. \nosg-3.4-el6-release\n / \nosg-3.4-el7-release\n\n\nThe \ndist-el6-build\n / \ndist-el7-build\n tag (consisting of the appropriate macros for \n%dist\n)\n\n\nCentOS and EPEL", 
            "title": "Repository Management"
        }, 
        {
            "location": "/software/repository-management/#repository-management", 
            "text": "This document attempts to record everything there is to know about repository management for the OSG.", 
            "title": "Repository Management"
        }, 
        {
            "location": "/software/repository-management/#public-repositories", 
            "text": "We host four public-facing repositories at  repo.grid.iu.edu :    development : This repository is the bleeding edge. Installing from this repository may cause the host to stop functioning, and we will not assist in undoing any damage.    testing : This repository contains software ready for testing. If you install packages from here, they may be buggy, but we will provide limited assistance in providing a migration path to a fixed verison.    release : This repository contains software that we are willing to support and can be used by the general community.    contrib : RPMs contributed from outside the OSG.    These repos are updated by the  mash  script running on  repo1.grid.iu.edu  and  repo2.grid.iu.edu .", 
            "title": "Public repositories"
        }, 
        {
            "location": "/software/repository-management/#internal-repositories", 
            "text": "In addition to the public repositories above, we host two repositories on  koji.chtc.wisc.edu . These are updated shortly after jobs are built into them or tagged into them. They are technically publicly accessible, but we discourage the public from using them.    minefield : This repository is a copy of development above.    prerelease : This repository is a staging area for software that is slated to be in the next release.    These repos are updated by the  kojira  daemon running on  koji.chtc.wisc.edu .", 
            "title": "Internal repositories"
        }, 
        {
            "location": "/software/repository-management/#build-repositories", 
            "text": "The  koji  task in  osg-build  uses the  osg-3.4-el6-build / osg-3.4-el7-build  repo, which is the union of the following repositories:   Minefield a.k.a.  osg-3.4-el6-development  /  osg-3.4-el7-development  The  osg-el6-internal  /  osg-el7-internal  tag (containing build dependencies we do not want to make public)  The  dist-el6-build  /  dist-el7-build  tag (consisting of the appropriate macros for %dist)  CentOS and EPEL   Koji will work from its internal cache of the above repositories (downloading the packages from the source), and will not update until the build repository is regenerated. By default, Koji does a groupinstall of the build group, then resolves the BuildRequires dependencies.  The tarball creation scripts use the  osg-3.4-el6-release-build  /  osg-3.4-el7-release-build  repo, which is the union of the following repositories:   Release a.k.a.  osg-3.4-el6-release  /  osg-3.4-el7-release  The  dist-el6-build  /  dist-el7-build  tag (consisting of the appropriate macros for  %dist )  CentOS and EPEL", 
            "title": "Build repositories"
        }, 
        {
            "location": "/software/ce-test-scaling/", 
            "text": "How to Run Scalability Tests on a CE\n\n\nIntroduction\n\n\nThis document is intended as a general overview of the process for scalability testing of an OSG CE (Compute Element). All examples are for testing an \nHTCondor-CE\n, but they should be applicable for other CE software.\n\n\nThe focus of testing a CE is on the number of concurrent running jobs the CE can sustain as well as the ramp-up rate when many jobs are queued.\n\n\nSleeper Pool\n\n\nWith the focus on the CE, actual job payloads can be minimal \u2013 simple long sleep jobs are fine. Thus, then can run on nearly any resources, and it is even possible to allow far more of these jobs to run on a single resource than would be sensible for real jobs.\n\n\nWhen large-scale testing a CE, one of the objectives is to see if the CE can fully utilize all resources (cores) available to it or if there are bottlenecks preventing that outcome. However to do this would normally require using up production slots, and it is hard to find a site willing to give up so many production slots for so long. Thus, running resourceless jobs in parallel with production jobs allows the testing to proceed without interfering with real work.\n\n\nSetting Up a Sleeper Pool\n\n\nA sleeper pool is created by \u201ctricking\u201d a worker node into thinking it has more cores than physically available. Then, the host is configured so that jobs marked for the sleeper pool are routed to the extra slots. In HTCondor, this is done by changing the START expression on each startd. For example, on a 32-core machine:\n\n\nSTART = ( \\\n          (SlotID \n= 1) \n \\\n          (SlotID \n 33) \n \\\n          (RequiresWholeMachine =!= TRUE ) \n \\\n          (SleepSlot =!= TRUE) \n \\\n          (distro =?= \nRHEL6\n ) \n \\\n          (CPU_Only == TRUE ) \\\n          ) || \\\n          ( (SlotID \n= 33) \n (distro =?= \nRHEL6\n ) \n (SleepSlot == TRUE) )\n\n\n\n\n\nUsual Topology of the Tests\n\n\nA brief introduction to the topology involved in the tests.\n\n\nBatch System and Sleeper Pool\n\n\nThis is normally the batch system of the resources which will be behind the CE to be tested. It is normally set up by a site administrator.\n\n\nCE\n\n\nThis is the physical hardware where the CE software runs, hopefully mimicking real production hardware specifications.\n\n\nSubmitter\n\n\nAn HTCondor submit host. It can be a virtual machine for most test submissions.\n\n\nMonitoring tools\n\n\nTo monitor tests, two software components are needed (which can be installed on the same node): ganglia-gmond and ganglia-gmetad. Once they are installed, then some ad-hoc metrics can be created to monitor the CE; for example:\n\n\ncondor_q -pool red.unl.edu:9619 -name sleeper@red.unl.edu -const \nJobStatus=?=2\n | wc -l\n\n\ngmetric --name RunningJobsCE \n\n\n\n\n\n\nGenerating Load\n\n\nLocation\n\n\nThe load_generators are found in the  \nOSgscal github repo\n. The binary of interest here is \nloadtest_condor\n\n\nUse\n\n\nJust untar it or check it out from mas on the HTCondor submit node (see above):\n\n\ngit checkout https://github.com/efajardo/osgscal\n\n\ncd load_generators/loadtest_condor/trunk/bin\n\n\n\n\n\n\nKeep in mind that you also need a valid proxy for grid submissions.\n\n\nFor example, if the goal is to keep 1,000 jobs in the queue and run 6-hour sleep jobs (on average), you can run this command:\n\n\n./loadtest_condor.sh -type grid condor sleeper@red.unl.edu red.unl.edu:9619 -jobs 40000 -cluster 10 -proxy /home/submituser/.globus/cmspilot01.proxy -end random 21600 -maxidle 1000 -in sandbox 50", 
            "title": "CE Scale Testing"
        }, 
        {
            "location": "/software/ce-test-scaling/#how-to-run-scalability-tests-on-a-ce", 
            "text": "", 
            "title": "How to Run Scalability Tests on a CE"
        }, 
        {
            "location": "/software/ce-test-scaling/#introduction", 
            "text": "This document is intended as a general overview of the process for scalability testing of an OSG CE (Compute Element). All examples are for testing an  HTCondor-CE , but they should be applicable for other CE software.  The focus of testing a CE is on the number of concurrent running jobs the CE can sustain as well as the ramp-up rate when many jobs are queued.", 
            "title": "Introduction"
        }, 
        {
            "location": "/software/ce-test-scaling/#sleeper-pool", 
            "text": "With the focus on the CE, actual job payloads can be minimal \u2013 simple long sleep jobs are fine. Thus, then can run on nearly any resources, and it is even possible to allow far more of these jobs to run on a single resource than would be sensible for real jobs.  When large-scale testing a CE, one of the objectives is to see if the CE can fully utilize all resources (cores) available to it or if there are bottlenecks preventing that outcome. However to do this would normally require using up production slots, and it is hard to find a site willing to give up so many production slots for so long. Thus, running resourceless jobs in parallel with production jobs allows the testing to proceed without interfering with real work.", 
            "title": "Sleeper Pool"
        }, 
        {
            "location": "/software/ce-test-scaling/#setting-up-a-sleeper-pool", 
            "text": "A sleeper pool is created by \u201ctricking\u201d a worker node into thinking it has more cores than physically available. Then, the host is configured so that jobs marked for the sleeper pool are routed to the extra slots. In HTCondor, this is done by changing the START expression on each startd. For example, on a 32-core machine:  START = ( \\\n          (SlotID  = 1)   \\\n          (SlotID   33)   \\\n          (RequiresWholeMachine =!= TRUE )   \\\n          (SleepSlot =!= TRUE)   \\\n          (distro =?=  RHEL6  )   \\\n          (CPU_Only == TRUE ) \\\n          ) || \\\n          ( (SlotID  = 33)   (distro =?=  RHEL6  )   (SleepSlot == TRUE) )", 
            "title": "Setting Up a Sleeper Pool"
        }, 
        {
            "location": "/software/ce-test-scaling/#usual-topology-of-the-tests", 
            "text": "A brief introduction to the topology involved in the tests.", 
            "title": "Usual Topology of the Tests"
        }, 
        {
            "location": "/software/ce-test-scaling/#batch-system-and-sleeper-pool", 
            "text": "This is normally the batch system of the resources which will be behind the CE to be tested. It is normally set up by a site administrator.", 
            "title": "Batch System and Sleeper Pool"
        }, 
        {
            "location": "/software/ce-test-scaling/#ce", 
            "text": "This is the physical hardware where the CE software runs, hopefully mimicking real production hardware specifications.", 
            "title": "CE"
        }, 
        {
            "location": "/software/ce-test-scaling/#submitter", 
            "text": "An HTCondor submit host. It can be a virtual machine for most test submissions.", 
            "title": "Submitter"
        }, 
        {
            "location": "/software/ce-test-scaling/#monitoring-tools", 
            "text": "To monitor tests, two software components are needed (which can be installed on the same node): ganglia-gmond and ganglia-gmetad. Once they are installed, then some ad-hoc metrics can be created to monitor the CE; for example:  condor_q -pool red.unl.edu:9619 -name sleeper@red.unl.edu -const  JobStatus=?=2  | wc -l  gmetric --name RunningJobsCE", 
            "title": "Monitoring tools"
        }, 
        {
            "location": "/software/ce-test-scaling/#generating-load", 
            "text": "", 
            "title": "Generating Load"
        }, 
        {
            "location": "/software/ce-test-scaling/#location", 
            "text": "The load_generators are found in the   OSgscal github repo . The binary of interest here is  loadtest_condor", 
            "title": "Location"
        }, 
        {
            "location": "/software/ce-test-scaling/#use", 
            "text": "Just untar it or check it out from mas on the HTCondor submit node (see above):  git checkout https://github.com/efajardo/osgscal  cd load_generators/loadtest_condor/trunk/bin   Keep in mind that you also need a valid proxy for grid submissions.  For example, if the goal is to keep 1,000 jobs in the queue and run 6-hour sleep jobs (on average), you can run this command:  ./loadtest_condor.sh -type grid condor sleeper@red.unl.edu red.unl.edu:9619 -jobs 40000 -cluster 10 -proxy /home/submituser/.globus/cmspilot01.proxy -end random 21600 -maxidle 1000 -in sandbox 50", 
            "title": "Use"
        }, 
        {
            "location": "/software/rpm-development-guide/", 
            "text": "RPM Development Guide\n\n\nThis page documents technical guidelines and details about RPM development for the OSG Software Stack. The procedures, conventions, and policies defined within are used by the OSG Software Team, and are recommended to all external developers who wish to contribute to the OSG Software Stack.\n\n\nPrinciples\n\n\nThe principles below guide the design and implementation of the technical details that follow.\n\n\n\n\nPackages should adhere to community standards (e.g., \nFedora Packaging Guidelines\n) when possible, and significant deviations must be documented\n\n\nEvery released package must be reproducible from data stored in our system\n\n\nSource code for software should be clearly separable from the packaging of that software\n\n\nUpstream source files (which should not be modified) should be clearly separated from files owned by the OSG Software Team\n\n\nBuilding source and binary packages from our system should be easy and efficient\n\n\nExternal developers should have a clear and effective system for developing and contributing packages\n\n\nWe should use standard tools from relevant packaging and development communities when appropriate\n\n\n\n\nContributing Packages\n\n\nWe encourage all interested parties to contribute to OSG Software, and all the infrastructure described on this page should be friendly to external contributors.\n\n\n\n\nTo participate in the packaging community: You must subscribe to the \n email list. Subscribing to an OSG email list is \ndescribed here\n.\n\n\nTo create and edit packages: \nObtain access to VDT SVN\n.\n\n\nTo upload new source tarballs: You must have a cs.wisc.edu account with write access to the VDT source tarball directory. Email the osg-software list and request permission.\n\n\nTo build using the OSG's Koji build system: You must have a valid grid certificate and a Koji account. Email the osg-software list with your cert's DN and request permission.\n\n\n\n\nDevelopment Infrastructure\n\n\nThis section documents most of what a developer needs to know about our RPM infrastructure:\n\n\n\n\nUpstream Source Cache \u2014 a filesystem scheme for caching upstream source files\n\n\nRevision Control System \u2014 where to get and store development files, and how they are organized\n\n\nBuild System \u2014 how to build packages from the revision control system\n\n\nYum Repository \u2014 the location and organization of our Yum repository, and how to promote packages through it\n\n\n\n\nUpstream Source Cache\n\n\nOne of our principles (every released package must be reproducible from data stored in our system) creates a potential issue: If we keep all historical source data, especially upstream files like source tarballs and source RPMs, in our revision control system, we may face large checkouts and consequently long checkout and update times.\n\n\nOur solution is to cache all upstream source files in a separate filesystem area, retaining historical files indefinitely. To avoid tainting upstream files, our policy is to leave them unmodified after download.\n\n\nLocating Files in the Cache\n\n\nUpstream source files are stored in the filesystem as follows:\n\n\n\n\n/p/vdt/public/html/upstream/\nPACKAGE\n/\nVERSION\n/\nFILE\n\n\n\n\nwhere:\n\n\n\n\n\n\n\n\nSymbol\n\n\nDefinition\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nPACKAGE\n\n\nUpstream name of the source package, or some widely accepted form thereof\n\n\nndt\n\n\n\n\n\n\nVERSION\n\n\nUpstream version string used to identify the release\n\n\n3.6.4\n\n\n\n\n\n\nFILE\n\n\nUpstream filename itself\n\n\nndt-3.6.4.tar.gz\n\n\n\n\n\n\n\n\nThe authoritative cache is the VDT webserver, which is fully backed up. The Koji build system uses this cache.\n\n\nUpstream source files are referenced from within the revision control system; see below for details.\n\n\nContributing Upstream Files\n\n\nYou must make sure that any new upstream source files are cached on the VDT webserver before building the package via Koji. You have two options:\n\n\n\n\nIf you have access to a UW\u2013Madison CSL machine, you can scp the source files directly into the AFS locations using that machine\n\n\nIf you do not have such access, write to the osg-software list to find someone who will post the files for you\n\n\n\n\nRevision Control System\n\n\nAll packages that the OSG Software Team releases are checked into our revision control system (currently Subversion but with the option to change at a later date).\n\n\nSubversion Access\n\n\nOur Subversion repository is located at:\n\n\n\n\nhttps://vdt.cs.wisc.edu/svn\n\n\n\n\n\n\n\nProcedure for offsite users obtaining access to Subversion\n\n\nOr, from a UW\u2013Madison Computer Sciences machine:\n\n\n\n\nfile:///p/condor/workspaces/vdt/svn\n\n\n\n\n\n\n\nThe current SVN directory housing our native package work is \n$SVN/native/redhat\n (where \n$SVN\n is one of the ways of accessing our SVN repository above). For example, to check out the current package repository via HTTPS, do:\n\n\n[you@host]$\n svn co https://vdt.cs.wisc.edu/svn/native/redhat\n\n\n\n\n\nOSG-Owned Software\n\n\nOSG-owned software goes into GitHub under the \nopensciencegrid\n organization. Files are organized as the developer sees fit.\n\n\nIt is strongly recommended that each software package include a top-level Makefile with at least the following targets:\n\n\n\n\n\n\n\n\nSymbol\n\n\nPurpose\n\n\n\n\n\n\n\n\n\n\ninstall\n\n\nInstall the software into final FHS locations rooted at \nDESTDIR\n\n\n\n\n\n\ndist\n\n\nCreate a distribution source tarball (in the current section directory) for a release\n\n\n\n\n\n\nupstream\n\n\nInstall the distribution source tarball into the upstream source cache\n\n\n\n\n\n\n\n\nPackaging Top-Level Directory Organization\n\n\nThe top levels of our Subversion directory hierarchy for packaging are as follows:\n\n\n\n\nnative/redhat/\nSECTION\n/\nPACKAGE\n\n\n\n\nwhere:\n\n\n\n\n\n\n\n\nSymbol\n\n\nDefinition\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nSECTION\n\n\nDevelopment section\n\n\nStandard Subversion sections like \ntrunk\n and \nbranches/*\n\n\n\n\n\n\nPACKAGE\n\n\nOur standardized name for a source package\n\n\nndt\n\n\n\n\n\n\n\n\nPackage Directory Organization\n\n\nWithin a source package directory, the following files (detailed in separate sections below) may exist:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nREADME\n\n\ntext file\n\n\npackage notes, by and for developers\n\n\n\n\n\n\nupstream/\n\n\ndirectory\n\n\nreferences to the upstream source cache and other kinds of upstream files\n\n\n\n\n\n\nosg/\n\n\ndirectory\n\n\noverrides and patches of upstream files, plus new files, which contribute to the final OSG source package\n\n\n\n\n\n\n\n\nREADME\n\n\nThis is a free-form text file for developers to leave notes about the package. Please document anything interesting about how you procured the upstream source, the reasons for the modifications you made, or anything else people might need to know in order to maintain the package in the future. Please document the \nwhy\n, not just the \nwhat\n.\n\n\nupstream\n\n\nWithin the per-package directories of the revision control system, there must be a way to refer to cached files. This is done with small text files that (a) are named consistently, and (b) contain the location of the referenced file as its contents.\n\n\nA reference file is named:\n\n\n\n\nDESCRIPTION\n.\nTYPE\n.source\n\n\n\n\nwhere:\n\n\n\n\n\n\n\n\nSymbol\n\n\nDefinition\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nDESCRIPTION\n\n\nDescriptive label of the source of the referenced file\n\n\ndeveloper\n, \nepel\n, \nemi\n\n\n\n\n\n\nTYPE\n\n\nType of referenced file\n\n\ntarball\n, \nsrpm\n\n\n\n\n\n\n\n\nThe contents of the file match the upstream source cache path defined above, without the prefix component:\n\n\n\n\nPACKAGE\n/\nVERSION\n/\nFILE\n\n\n\n\nIn addition, the \n.source\n files may contain comments, which start with \n#\n and continue until the end of the line. It is useful to add the source of the upstream file into a comment.\n\n\nFor example, the reference file for \nglobus-common\n's source tarball is named \nepel.srpm.source\n and might contain:\n\n\nglobus-common/16.4/globus-common-16.4-1.el6.src.rpm\n# Downloaded from \nhttp://dl.fedoraproject.org/pub/epel/6/SRPMS/globus-common-16.4-1.el6.src.rpm\n\n\n\n\n\n\nosg\n\n\nThe \nosg\n directory contains files that are owned by the OSG Software Team and that are used to create the final, released source package. It may contain a variety of development files:\n\n\n\n\nAn RPM \n.spec\n file, which overrides any spec file from a referenced source\n\n\nPatch (\n.patch\n) or replacement files, which override any same-named file from the top-level directory of a referenced source\n\n\nOther files, which must be explicitly placed into the package by the spec file\n\n\n\n\nGenerated directories\n\n\nThe following directories may be generated by our build tool, \nOSG-Build\n. They are not under revision control.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n_upstream_srpm_contents/\n\n\nexpanded contents of a cached upstream source package\n\n\n\n\n\n\n_upstream_tarball_contents/\n\n\nexpanded contents of all cached upstream source tarballs\n\n\n\n\n\n\n_final_srpm_contents/\n\n\nthe final contents of the OSG source package\n\n\n\n\n\n\n_build_results/\n\n\nOSG source and binary packages resulting from a build\n\n\n\n\n\n\n_quilt/\n\n\nexpanded, patched contents of the upstream sources, as generated by the \nquilt\n tool\n\n\n\n\n\n\n\n\n_upstream_srpm_contents\n\n\nThe \n_upstream_srpm_contents\n directory contains the files that are part of the upstream source package. It is a volatile record of the upstream source for developer use.\n\n\n_upstream_tarball_contents\n\n\nThe \n_upstream_tarball_contents\n directory contains the files that are part of the upstream source tarballs. It is generated by the package build tool if the \n--full-extract\n option is passed. It is not used for anything by the build tool, but meant as a convenience to allow the developer to look inside the upstream sources (for making patches, etc.).\n\n\n_final_srpm_contents\n\n\nThe \n_final_srpm_contents\n directory contains the final files that are part of the released source package. It is a volatile record of a build for developer use.\n\n\n_build_results\n\n\nThe \n_build_results\n directory contains the source and binary RPMs that are produced by a local build. It is a volatile record of a build for developer use.\n\n\n_quilt\n\n\nThe \n_quilt\n directory contains the unpacked sources after they have been patched using the \nquilt\n utility. This allows easier patch development.\n\n\nPackaging Organization Examples\n\n\nUse Case 1: Packaging an Upstream Source Tarball\n\n\nWhen the OSG Software Team packages an upstream source tarball, for which there is no existing package, the source tarball is referenced with a .source file and we provide a spec file and, if necessary, patches. For example, RSV is provided as a source tarball only. Its package directory contains:\n\n\n\n\nrsv/\n    osg/\n        rsv.spec\n    upstream/\n        developer.tarball.source\n\n\n\n\n\n\n\nUse Case 2: Passing Through a Source RPM\n\n\nWhen the OSG Software Team simply provides a copy of an existing source RPM, it is referenced with a .source file and that is it. For example, we do not modify the \nglobus-common\n source RPM from EPEL. Its package directory contains:\n\n\n\n\nglobus-common/\n    upstream/\n        epel.srpm.source\n\n\n\n\n\n\n\nUse Case 3: Modifying a Source RPM\n\n\nWhen the OSG Software Team modifies an existing source RPM, it is referenced with a .source file and then all changes to the upstream source are contained in the \nosg\n directory. For example, we use this mechanism for the \nglobus-ftp-client\n package, originally obtained from EPEL. Its package directory contains:\n\n\n\n\nglobus-ftp-client/\n    osg/\n        globus-ftp-client.spec\n        1853-ssh-bin.patch\n    upstream/\n        epel.srpm.source\n\n\n\n\n\n\n\nBuild Process\n\n\n\n\nAll necessary information to create the package will be committed to the VDT source code repository (see below)\n\n\nThe \nOSG build tools\n will take those files, create a source RPM, and submit it to our Koji build system\n\n\n\n\nDevelopers may use \nrpmbuild\n and \nmock\n for faster iterative development before submitting the package to Koji. \nosg-build\n may be used as a wrapper script around \nrpmbuild\n and \nmock\n.\n\n\nOSG Software Repository\n\n\nOSG Operations maintains the Yum repositories that contain our source and binary RPMs at \nhttps://repo.grid.iu.edu/osg/\n and are mirrored at other institutions as well.\n\n\nRelease Levels\n\n\nEvery package is classified into a release level based on the amount of testing it has undergone and our confidence in its stability. When a package is first built, it goes into the lowest level (\nosg-development\n). The members of the OSG Software and Release teams may promote packages through the release levels, as per our \nRelease Policy page\n.\n\n\nPackaging Conventions\n\n\nIn addition to adhering to the \nFedora Packaging Guidelines\n (FPG), we have a few rules and guidelines of our own:\n\n\n\n\nWhen we pass-through an RPM and make any changes to it (so it has an updated package number), we construct the version-release as follows:\n\n\nThe version of the original RPM remains unchanged\n\n\nThe release is composed of three parts: ORIGINALRELEASE.OSGRELEASE\n\n\nWe add a distro tag based on the OSG major version and OS major version, e.g. \"osg33.el6\". (Use \n%{?dist}\n in the Release field)\n\n\n\n\n\n\n\n\nExample: We copy package foobar-3.0.5-1 from somewhere. We need to patch it, so the full name-version-release (NVR) for OSG 3.3 on EL 6 becomes \nfoobar-3.0.5-1.1.osg33.el6\n Note that we added \".1.osg33.el6\" to the release number. If we update our packaging (but still base on foobar-3.0.5-1), we change to \".2.osg33.el6\". In the spec file, this would look like:\n\n\nRelease\n:\n 1.2\n%{?dist}\n\n\n\n\n\n\nPackaging for Multiple Distro Versions\n\n\nConditionalizing spec files\n\n\nSome packages may need different build behavior between major versions of the OS; RPM conditional statements will be used to handle this.\n\n\nThe following macros are defined:\n\n\n\n\n\n\n\n\nName\n\n\nValue (EL6)\n\n\nValue (EL7)\n\n\n\n\n\n\n\n\n\n\n%rhel\n\n\n6\n\n\n7\n\n\n\n\n\n\n%el6\n\n\n1\n\n\nundefined\n or \n0\n\n\n\n\n\n\n%el7\n\n\nundefined\n or \n0\n\n\n1\n\n\n\n\n\n\n\n\nHere's how to use them:\n\n\n%if\n 0%{?el6}\n\n# this code will be executed on EL 6 only\n\n\n%endif\n\n\n\n%if\n 0%{?el7}\n\n# this code will be executed on EL 7 only\n\n\n%endif\n\n\n\n%if\n 0%{?rhel} \n= 7\n\n# this code will be executed on EL 7 and newer\n\n\n%endif\n\n\n\n\n\n\n(There does not seem to be an \n%elseif\n).\n\n\nThe syntax \n%{?el6}\n expands to the value of the \n%el6\n macro if it is defined, and to the empty string if not; the \n0\n is there to keep the condition from being empty in the \n%if\n statement if the macro is not defined.", 
            "title": "RPM Development Guide"
        }, 
        {
            "location": "/software/rpm-development-guide/#rpm-development-guide", 
            "text": "This page documents technical guidelines and details about RPM development for the OSG Software Stack. The procedures, conventions, and policies defined within are used by the OSG Software Team, and are recommended to all external developers who wish to contribute to the OSG Software Stack.", 
            "title": "RPM Development Guide"
        }, 
        {
            "location": "/software/rpm-development-guide/#principles", 
            "text": "The principles below guide the design and implementation of the technical details that follow.   Packages should adhere to community standards (e.g.,  Fedora Packaging Guidelines ) when possible, and significant deviations must be documented  Every released package must be reproducible from data stored in our system  Source code for software should be clearly separable from the packaging of that software  Upstream source files (which should not be modified) should be clearly separated from files owned by the OSG Software Team  Building source and binary packages from our system should be easy and efficient  External developers should have a clear and effective system for developing and contributing packages  We should use standard tools from relevant packaging and development communities when appropriate", 
            "title": "Principles"
        }, 
        {
            "location": "/software/rpm-development-guide/#contributing-packages", 
            "text": "We encourage all interested parties to contribute to OSG Software, and all the infrastructure described on this page should be friendly to external contributors.   To participate in the packaging community: You must subscribe to the   email list. Subscribing to an OSG email list is  described here .  To create and edit packages:  Obtain access to VDT SVN .  To upload new source tarballs: You must have a cs.wisc.edu account with write access to the VDT source tarball directory. Email the osg-software list and request permission.  To build using the OSG's Koji build system: You must have a valid grid certificate and a Koji account. Email the osg-software list with your cert's DN and request permission.", 
            "title": "Contributing Packages"
        }, 
        {
            "location": "/software/rpm-development-guide/#development-infrastructure", 
            "text": "This section documents most of what a developer needs to know about our RPM infrastructure:   Upstream Source Cache \u2014 a filesystem scheme for caching upstream source files  Revision Control System \u2014 where to get and store development files, and how they are organized  Build System \u2014 how to build packages from the revision control system  Yum Repository \u2014 the location and organization of our Yum repository, and how to promote packages through it", 
            "title": "Development Infrastructure"
        }, 
        {
            "location": "/software/rpm-development-guide/#upstream-source-cache", 
            "text": "One of our principles (every released package must be reproducible from data stored in our system) creates a potential issue: If we keep all historical source data, especially upstream files like source tarballs and source RPMs, in our revision control system, we may face large checkouts and consequently long checkout and update times.  Our solution is to cache all upstream source files in a separate filesystem area, retaining historical files indefinitely. To avoid tainting upstream files, our policy is to leave them unmodified after download.", 
            "title": "Upstream Source Cache"
        }, 
        {
            "location": "/software/rpm-development-guide/#locating-files-in-the-cache", 
            "text": "Upstream source files are stored in the filesystem as follows:   /p/vdt/public/html/upstream/ PACKAGE / VERSION / FILE   where:     Symbol  Definition  Example      PACKAGE  Upstream name of the source package, or some widely accepted form thereof  ndt    VERSION  Upstream version string used to identify the release  3.6.4    FILE  Upstream filename itself  ndt-3.6.4.tar.gz     The authoritative cache is the VDT webserver, which is fully backed up. The Koji build system uses this cache.  Upstream source files are referenced from within the revision control system; see below for details.", 
            "title": "Locating Files in the Cache"
        }, 
        {
            "location": "/software/rpm-development-guide/#contributing-upstream-files", 
            "text": "You must make sure that any new upstream source files are cached on the VDT webserver before building the package via Koji. You have two options:   If you have access to a UW\u2013Madison CSL machine, you can scp the source files directly into the AFS locations using that machine  If you do not have such access, write to the osg-software list to find someone who will post the files for you", 
            "title": "Contributing Upstream Files"
        }, 
        {
            "location": "/software/rpm-development-guide/#revision-control-system", 
            "text": "All packages that the OSG Software Team releases are checked into our revision control system (currently Subversion but with the option to change at a later date).", 
            "title": "Revision Control System"
        }, 
        {
            "location": "/software/rpm-development-guide/#subversion-access", 
            "text": "Our Subversion repository is located at:   https://vdt.cs.wisc.edu/svn   Procedure for offsite users obtaining access to Subversion  Or, from a UW\u2013Madison Computer Sciences machine:   file:///p/condor/workspaces/vdt/svn   The current SVN directory housing our native package work is  $SVN/native/redhat  (where  $SVN  is one of the ways of accessing our SVN repository above). For example, to check out the current package repository via HTTPS, do:  [you@host]$  svn co https://vdt.cs.wisc.edu/svn/native/redhat", 
            "title": "Subversion Access"
        }, 
        {
            "location": "/software/rpm-development-guide/#osg-owned-software", 
            "text": "OSG-owned software goes into GitHub under the  opensciencegrid  organization. Files are organized as the developer sees fit.  It is strongly recommended that each software package include a top-level Makefile with at least the following targets:     Symbol  Purpose      install  Install the software into final FHS locations rooted at  DESTDIR    dist  Create a distribution source tarball (in the current section directory) for a release    upstream  Install the distribution source tarball into the upstream source cache", 
            "title": "OSG-Owned Software"
        }, 
        {
            "location": "/software/rpm-development-guide/#packaging-top-level-directory-organization", 
            "text": "The top levels of our Subversion directory hierarchy for packaging are as follows:   native/redhat/ SECTION / PACKAGE   where:     Symbol  Definition  Example      SECTION  Development section  Standard Subversion sections like  trunk  and  branches/*    PACKAGE  Our standardized name for a source package  ndt", 
            "title": "Packaging Top-Level Directory Organization"
        }, 
        {
            "location": "/software/rpm-development-guide/#package-directory-organization", 
            "text": "Within a source package directory, the following files (detailed in separate sections below) may exist:            README  text file  package notes, by and for developers    upstream/  directory  references to the upstream source cache and other kinds of upstream files    osg/  directory  overrides and patches of upstream files, plus new files, which contribute to the final OSG source package", 
            "title": "Package Directory Organization"
        }, 
        {
            "location": "/software/rpm-development-guide/#readme", 
            "text": "This is a free-form text file for developers to leave notes about the package. Please document anything interesting about how you procured the upstream source, the reasons for the modifications you made, or anything else people might need to know in order to maintain the package in the future. Please document the  why , not just the  what .", 
            "title": "README"
        }, 
        {
            "location": "/software/rpm-development-guide/#upstream", 
            "text": "Within the per-package directories of the revision control system, there must be a way to refer to cached files. This is done with small text files that (a) are named consistently, and (b) contain the location of the referenced file as its contents.  A reference file is named:   DESCRIPTION . TYPE .source   where:     Symbol  Definition  Example      DESCRIPTION  Descriptive label of the source of the referenced file  developer ,  epel ,  emi    TYPE  Type of referenced file  tarball ,  srpm     The contents of the file match the upstream source cache path defined above, without the prefix component:   PACKAGE / VERSION / FILE   In addition, the  .source  files may contain comments, which start with  #  and continue until the end of the line. It is useful to add the source of the upstream file into a comment.  For example, the reference file for  globus-common 's source tarball is named  epel.srpm.source  and might contain:  globus-common/16.4/globus-common-16.4-1.el6.src.rpm\n# Downloaded from  http://dl.fedoraproject.org/pub/epel/6/SRPMS/globus-common-16.4-1.el6.src.rpm", 
            "title": "upstream"
        }, 
        {
            "location": "/software/rpm-development-guide/#osg", 
            "text": "The  osg  directory contains files that are owned by the OSG Software Team and that are used to create the final, released source package. It may contain a variety of development files:   An RPM  .spec  file, which overrides any spec file from a referenced source  Patch ( .patch ) or replacement files, which override any same-named file from the top-level directory of a referenced source  Other files, which must be explicitly placed into the package by the spec file", 
            "title": "osg"
        }, 
        {
            "location": "/software/rpm-development-guide/#generated-directories", 
            "text": "The following directories may be generated by our build tool,  OSG-Build . They are not under revision control.           _upstream_srpm_contents/  expanded contents of a cached upstream source package    _upstream_tarball_contents/  expanded contents of all cached upstream source tarballs    _final_srpm_contents/  the final contents of the OSG source package    _build_results/  OSG source and binary packages resulting from a build    _quilt/  expanded, patched contents of the upstream sources, as generated by the  quilt  tool", 
            "title": "Generated directories"
        }, 
        {
            "location": "/software/rpm-development-guide/#95upstream95srpm95contents", 
            "text": "The  _upstream_srpm_contents  directory contains the files that are part of the upstream source package. It is a volatile record of the upstream source for developer use.", 
            "title": "_upstream_srpm_contents"
        }, 
        {
            "location": "/software/rpm-development-guide/#95upstream95tarball95contents", 
            "text": "The  _upstream_tarball_contents  directory contains the files that are part of the upstream source tarballs. It is generated by the package build tool if the  --full-extract  option is passed. It is not used for anything by the build tool, but meant as a convenience to allow the developer to look inside the upstream sources (for making patches, etc.).", 
            "title": "_upstream_tarball_contents"
        }, 
        {
            "location": "/software/rpm-development-guide/#95final95srpm95contents", 
            "text": "The  _final_srpm_contents  directory contains the final files that are part of the released source package. It is a volatile record of a build for developer use.", 
            "title": "_final_srpm_contents"
        }, 
        {
            "location": "/software/rpm-development-guide/#95build95results", 
            "text": "The  _build_results  directory contains the source and binary RPMs that are produced by a local build. It is a volatile record of a build for developer use.", 
            "title": "_build_results"
        }, 
        {
            "location": "/software/rpm-development-guide/#95quilt", 
            "text": "The  _quilt  directory contains the unpacked sources after they have been patched using the  quilt  utility. This allows easier patch development.", 
            "title": "_quilt"
        }, 
        {
            "location": "/software/rpm-development-guide/#packaging-organization-examples", 
            "text": "", 
            "title": "Packaging Organization Examples"
        }, 
        {
            "location": "/software/rpm-development-guide/#use-case-1-packaging-an-upstream-source-tarball", 
            "text": "When the OSG Software Team packages an upstream source tarball, for which there is no existing package, the source tarball is referenced with a .source file and we provide a spec file and, if necessary, patches. For example, RSV is provided as a source tarball only. Its package directory contains:   rsv/\n    osg/\n        rsv.spec\n    upstream/\n        developer.tarball.source", 
            "title": "Use Case 1: Packaging an Upstream Source Tarball"
        }, 
        {
            "location": "/software/rpm-development-guide/#use-case-2-passing-through-a-source-rpm", 
            "text": "When the OSG Software Team simply provides a copy of an existing source RPM, it is referenced with a .source file and that is it. For example, we do not modify the  globus-common  source RPM from EPEL. Its package directory contains:   globus-common/\n    upstream/\n        epel.srpm.source", 
            "title": "Use Case 2: Passing Through a Source RPM"
        }, 
        {
            "location": "/software/rpm-development-guide/#use-case-3-modifying-a-source-rpm", 
            "text": "When the OSG Software Team modifies an existing source RPM, it is referenced with a .source file and then all changes to the upstream source are contained in the  osg  directory. For example, we use this mechanism for the  globus-ftp-client  package, originally obtained from EPEL. Its package directory contains:   globus-ftp-client/\n    osg/\n        globus-ftp-client.spec\n        1853-ssh-bin.patch\n    upstream/\n        epel.srpm.source", 
            "title": "Use Case 3: Modifying a Source RPM"
        }, 
        {
            "location": "/software/rpm-development-guide/#build-process", 
            "text": "All necessary information to create the package will be committed to the VDT source code repository (see below)  The  OSG build tools  will take those files, create a source RPM, and submit it to our Koji build system   Developers may use  rpmbuild  and  mock  for faster iterative development before submitting the package to Koji.  osg-build  may be used as a wrapper script around  rpmbuild  and  mock .", 
            "title": "Build Process"
        }, 
        {
            "location": "/software/rpm-development-guide/#osg-software-repository", 
            "text": "OSG Operations maintains the Yum repositories that contain our source and binary RPMs at  https://repo.grid.iu.edu/osg/  and are mirrored at other institutions as well.", 
            "title": "OSG Software Repository"
        }, 
        {
            "location": "/software/rpm-development-guide/#release-levels", 
            "text": "Every package is classified into a release level based on the amount of testing it has undergone and our confidence in its stability. When a package is first built, it goes into the lowest level ( osg-development ). The members of the OSG Software and Release teams may promote packages through the release levels, as per our  Release Policy page .", 
            "title": "Release Levels"
        }, 
        {
            "location": "/software/rpm-development-guide/#packaging-conventions", 
            "text": "In addition to adhering to the  Fedora Packaging Guidelines  (FPG), we have a few rules and guidelines of our own:   When we pass-through an RPM and make any changes to it (so it has an updated package number), we construct the version-release as follows:  The version of the original RPM remains unchanged  The release is composed of three parts: ORIGINALRELEASE.OSGRELEASE  We add a distro tag based on the OSG major version and OS major version, e.g. \"osg33.el6\". (Use  %{?dist}  in the Release field)     Example: We copy package foobar-3.0.5-1 from somewhere. We need to patch it, so the full name-version-release (NVR) for OSG 3.3 on EL 6 becomes  foobar-3.0.5-1.1.osg33.el6  Note that we added \".1.osg33.el6\" to the release number. If we update our packaging (but still base on foobar-3.0.5-1), we change to \".2.osg33.el6\". In the spec file, this would look like:  Release :  1.2 %{?dist}", 
            "title": "Packaging Conventions"
        }, 
        {
            "location": "/software/rpm-development-guide/#packaging-for-multiple-distro-versions", 
            "text": "", 
            "title": "Packaging for Multiple Distro Versions"
        }, 
        {
            "location": "/software/rpm-development-guide/#conditionalizing-spec-files", 
            "text": "Some packages may need different build behavior between major versions of the OS; RPM conditional statements will be used to handle this.  The following macros are defined:     Name  Value (EL6)  Value (EL7)      %rhel  6  7    %el6  1  undefined  or  0    %el7  undefined  or  0  1     Here's how to use them:  %if  0%{?el6} # this code will be executed on EL 6 only  %endif  %if  0%{?el7} # this code will be executed on EL 7 only  %endif  %if  0%{?rhel}  = 7 # this code will be executed on EL 7 and newer  %endif   (There does not seem to be an  %elseif ).  The syntax  %{?el6}  expands to the value of the  %el6  macro if it is defined, and to the empty string if not; the  0  is there to keep the condition from being empty in the  %if  statement if the macro is not defined.", 
            "title": "Conditionalizing spec files"
        }, 
        {
            "location": "/software/upcoming-to-main/", 
            "text": "Promoting Packages from Upcoming to Main\n\n\nSometimes we move packages from Upcoming to the Main repositories in the middle of a release series. Once the Release Manager has given tentative approval for such a move:\n\n\nIf needed, move the software from upcoming to trunk and release using the usual process:\n\n\n\n\nMerge changes to the package in SVN from branches/upcoming to trunk.\n\n\nBuild the package from trunk.\n\n\nFollow the normal process to prepare a build for release (including development testing, promotion, etc.).\n\n\n\n\nOn release day, when the package has been released in the Main production repository, clean up the package from the upcoming repos:\n\n\n\n\nUntag from \nall upcoming repos\n the version of the package corresponding to the version that was released in main. (Do \nNOT\n untag from the osg-upcoming-elN-release-X.Y.Z tags)\n\n\n\n\nAlso, untag all equal or lesser NVRs (minus the dist tag) from all upcoming repos.\n\n\nIf you do not have the privileges to untag from upcoming-release, someone on the Release Team can help.\n\n\n(These steps are necessary to make sure Koji builds can't mistakenly use an older build from the upcoming repos).\n\n\n\n\nUnless there's a newer build in branches/upcoming than what was released, remove the package directory from branches/upcoming.", 
            "title": "Upcoming to Main"
        }, 
        {
            "location": "/software/upcoming-to-main/#promoting-packages-from-upcoming-to-main", 
            "text": "Sometimes we move packages from Upcoming to the Main repositories in the middle of a release series. Once the Release Manager has given tentative approval for such a move:  If needed, move the software from upcoming to trunk and release using the usual process:   Merge changes to the package in SVN from branches/upcoming to trunk.  Build the package from trunk.  Follow the normal process to prepare a build for release (including development testing, promotion, etc.).   On release day, when the package has been released in the Main production repository, clean up the package from the upcoming repos:   Untag from  all upcoming repos  the version of the package corresponding to the version that was released in main. (Do  NOT  untag from the osg-upcoming-elN-release-X.Y.Z tags)   Also, untag all equal or lesser NVRs (minus the dist tag) from all upcoming repos.  If you do not have the privileges to untag from upcoming-release, someone on the Release Team can help.  (These steps are necessary to make sure Koji builds can't mistakenly use an older build from the upcoming repos).   Unless there's a newer build in branches/upcoming than what was released, remove the package directory from branches/upcoming.", 
            "title": "Promoting Packages from Upcoming to Main"
        }, 
        {
            "location": "/policy/software-support/", 
            "text": "Software Team Support\n\n\nManaging OSG Software Tickets\n\n\nIncoming tickets (Operations staff)\n\n\nWhen a ticket arrives at the GOC and the Operations staff member decides that the ticket should be assigned to the Software Team, the operations staff member will do two things:\n\n\n\n\nThe ticket will be assigned to a pseudo-user called \"Software\".\n\n\nThe \"Next Action\" field will be set to \"SOFTWARE TRIAGE\".\n\n\nThe Software pseudo-user has an email list as its \"personal\" email address. This is: \n.\n\n\n\n\nTriage duty (Technology Area staff)\n\n\nAll OSG Technology Area Team members who are at least 50% on the will share triage duty (except Brian Bockelman). Each week (Monday through Friday), during normal work hours, there will be one person on triage duty. If you are on triage duty, this means:\n\n\n\n\nWatch the software incoming tickets. \nIf a ticket has not been assigned to a software team member, you assign it appropriately.\n You are responsible for assigning all incoming tickets that haven't been assigned. This includes tickets that have arrived over the weekend or were not handled by the previous person on triage duty.\n\n\nIf you can handle an incoming ticket, assign it to yourself and handle it. Leave the \"software\" user assigned to the ticket. Many tickets are common problems that most team members should be able to solve.\n\n\nIf you cannot handle an incoming ticket, collect initial details (versions, logs, etc...), and assign the ticket to the most appropriate software team member. Where appropriate, include people from other teams (i.e. security, operations, glidein...) Leave the \"software\" user assigned to the ticket.\n\n\nLook at assigned tickets. For tickets that are not being handled in a timely fashion, please remind the person that owns the ticket, or, if the ticket is waiting on the user, remind the user.\n\n\n\n\nPlease note: being on triage duty does \nnot\n mean that you must personally solve all new tickets. It means that you handle the easy tickets and assign the other tickets appropriately.\n\n\nNote that the software pseudo-user has an email address that is a mailing list: \n. If you find it convenient, you can sign up for the mailing list to see all the incoming tickets. Alain recommends you do this during your triage duty, but you do not need to stay subscribed when you are not on triage duty.\n\n\nAll the currently opened tickets assigned to the software team can be seen here: \nGOC Open Tickets\n\n\nFlow of tickets\n\n\nNote that if you follow the above, we will end up with three assignees to each ticket. This is the overall flow:\n\n\n\n\nA ticket arrives at the GOC, either via the ticket creator, or created by Operations in response to an email.\n\n\nASSIGNMENT #1:\n The ticket is assigned to an Operations member. They're in charge of ushering the ticket through its whole lifetime, though for software tickets they won't do a whole lot on the technical work. Note that some software tickets may not be assigned to us, because they might assign them to the VO support center. This is good.\n\n\nASSIGNMENT #2:\n The Operations member looks at the ticket and decides it's a software ticket. (They might do some upfront work if they can.) They then assign it to \"Software Support (Triage)\". We now have two people assigned to the ticket.\n\n\nWhen assigned to \"Software Support (Triage)\", all changes to the ticket are sent to \n, so we leave this pseudo-person on the ticket. Watching the email to this mailing list is a nice (but optional) way for you to see what's happening when you're on triage duty.\n\n\n\n\nASSIGNMENT #3:\n The person on triage duty assigns it to the right person from the software team. We now have three assignees:\n\n\n\n\nGOC member\n\n\nSoftware Support (Triage)\n\n\nThe Software team member who will handle the ticket\n\n\n\n\nInasmuch as possible, you should strive to handle the easier tickets and not pass them off to other people. For reference, see our \ntroubleshooting documents\n.\n\n\n\n\n\n\nUpdating the triage calendar\n\n\nThe calendar is hosted on Tim Cartwright\u2019s Google Calendar account. If you need privileges to edit, ask Brian L. To update\n\n\n\n\nUpdate checkout (\nGitHub\n)\n\n\nGenerate next rotation:\n./triage.py --generateNextRotation \n rotation.txt\n\n\n\n\n\n\n\nCheck and update assignments according to team member outages\n\n\n\n\nLoad triage assignments into Google Calendar:\n\n\n./triage.py --load rotation.txt\n\n\n\n\n\n\n\n\n\n\nTo subscribe to this calendar in your calendar program, use the iCal URL: \nhttps://www.google.com/calendar/ical/h5t4mns6omp49db1e4qtqrrf4g%40group.calendar.google.com/public/basic.ics\n\n\n\n\n\nHandling tickets\n\n\n\n\nWe need to take good care of our users. We are in a small community. Please be friendly and patient even when the user is frustrated or lacking in knowledge.\n\n\nAlways sign your ticket with your full name, so people know who is responding.\n\n\nIf it's easy for you, include a signature at the bottom of your response.\n\n\nRemember that you can tell people to use the \nosg-system-profiler\n to collect information. It can shorten the number of times you ask for information because it collects quite a bit for you.\n\n\nIf you run across a problem that has a chance of being hit by other users, consider:\n\n\nIs there a bug we should fix in the software? Or something we could improve in the software?\n\n\nIs there a way to improve our documentation?\n\n\nCan you extend our troubleshooting documents to help people track this down more quickly? Consider the troubleshooting documents to be as much for us as for our users.\n\n\n\n\n\n\n\n\nDirect email vs. support\n\n\nIf someone emails you directly for support, you have the choice of when to move it to a ticket. The recommended criteria are:\n\n\n\n\nIf it's easy to handle and you can definitely do it yourself, leave it in email.\n\n\nIf there's a chance that you can't do it in a timely fashion, turn it into a ticket.\n\n\nIf there's a chance that you might lose track of the email, turn it into a ticket.\n\n\nIf there's a chance that you might need help from others, turn it into a ticket.\n\n\nIf it's an unusual topic and other people would benefit from seeing the ticket (now or in the future), turn it into a ticket.\n\n\n\n\nGOC ticket vs. JIRA\n\n\nGOC Ticket is for user support. This is where we helping users debug, understand their problems, etc.\n\n\nJIRA is for tracking our work. It's meant for internal usage, not for user support.\n\n\nIn general, users should not ask for support via JIRA. A single user support ticket might result in zero, one, or multiple JIRA tickets. The user ticket may be closed even though the related JIRA tickets are open. (\"Hi, this is a bug that we can't fix for the next six months, but I've made an internal bug report you can see at ...\")", 
            "title": "Software Support"
        }, 
        {
            "location": "/policy/software-support/#software-team-support", 
            "text": "", 
            "title": "Software Team Support"
        }, 
        {
            "location": "/policy/software-support/#managing-osg-software-tickets", 
            "text": "", 
            "title": "Managing OSG Software Tickets"
        }, 
        {
            "location": "/policy/software-support/#incoming-tickets-operations-staff", 
            "text": "When a ticket arrives at the GOC and the Operations staff member decides that the ticket should be assigned to the Software Team, the operations staff member will do two things:   The ticket will be assigned to a pseudo-user called \"Software\".  The \"Next Action\" field will be set to \"SOFTWARE TRIAGE\".  The Software pseudo-user has an email list as its \"personal\" email address. This is:  .", 
            "title": "Incoming tickets (Operations staff)"
        }, 
        {
            "location": "/policy/software-support/#triage-duty-technology-area-staff", 
            "text": "All OSG Technology Area Team members who are at least 50% on the will share triage duty (except Brian Bockelman). Each week (Monday through Friday), during normal work hours, there will be one person on triage duty. If you are on triage duty, this means:   Watch the software incoming tickets.  If a ticket has not been assigned to a software team member, you assign it appropriately.  You are responsible for assigning all incoming tickets that haven't been assigned. This includes tickets that have arrived over the weekend or were not handled by the previous person on triage duty.  If you can handle an incoming ticket, assign it to yourself and handle it. Leave the \"software\" user assigned to the ticket. Many tickets are common problems that most team members should be able to solve.  If you cannot handle an incoming ticket, collect initial details (versions, logs, etc...), and assign the ticket to the most appropriate software team member. Where appropriate, include people from other teams (i.e. security, operations, glidein...) Leave the \"software\" user assigned to the ticket.  Look at assigned tickets. For tickets that are not being handled in a timely fashion, please remind the person that owns the ticket, or, if the ticket is waiting on the user, remind the user.   Please note: being on triage duty does  not  mean that you must personally solve all new tickets. It means that you handle the easy tickets and assign the other tickets appropriately.  Note that the software pseudo-user has an email address that is a mailing list:  . If you find it convenient, you can sign up for the mailing list to see all the incoming tickets. Alain recommends you do this during your triage duty, but you do not need to stay subscribed when you are not on triage duty.  All the currently opened tickets assigned to the software team can be seen here:  GOC Open Tickets", 
            "title": "Triage duty (Technology Area staff)"
        }, 
        {
            "location": "/policy/software-support/#flow-of-tickets", 
            "text": "Note that if you follow the above, we will end up with three assignees to each ticket. This is the overall flow:   A ticket arrives at the GOC, either via the ticket creator, or created by Operations in response to an email.  ASSIGNMENT #1:  The ticket is assigned to an Operations member. They're in charge of ushering the ticket through its whole lifetime, though for software tickets they won't do a whole lot on the technical work. Note that some software tickets may not be assigned to us, because they might assign them to the VO support center. This is good.  ASSIGNMENT #2:  The Operations member looks at the ticket and decides it's a software ticket. (They might do some upfront work if they can.) They then assign it to \"Software Support (Triage)\". We now have two people assigned to the ticket.  When assigned to \"Software Support (Triage)\", all changes to the ticket are sent to  , so we leave this pseudo-person on the ticket. Watching the email to this mailing list is a nice (but optional) way for you to see what's happening when you're on triage duty.   ASSIGNMENT #3:  The person on triage duty assigns it to the right person from the software team. We now have three assignees:   GOC member  Software Support (Triage)  The Software team member who will handle the ticket   Inasmuch as possible, you should strive to handle the easier tickets and not pass them off to other people. For reference, see our  troubleshooting documents .", 
            "title": "Flow of tickets"
        }, 
        {
            "location": "/policy/software-support/#updating-the-triage-calendar", 
            "text": "The calendar is hosted on Tim Cartwright\u2019s Google Calendar account. If you need privileges to edit, ask Brian L. To update   Update checkout ( GitHub )  Generate next rotation: ./triage.py --generateNextRotation   rotation.txt   Check and update assignments according to team member outages   Load triage assignments into Google Calendar:  ./triage.py --load rotation.txt      To subscribe to this calendar in your calendar program, use the iCal URL:  https://www.google.com/calendar/ical/h5t4mns6omp49db1e4qtqrrf4g%40group.calendar.google.com/public/basic.ics", 
            "title": "Updating the triage calendar"
        }, 
        {
            "location": "/policy/software-support/#handling-tickets", 
            "text": "We need to take good care of our users. We are in a small community. Please be friendly and patient even when the user is frustrated or lacking in knowledge.  Always sign your ticket with your full name, so people know who is responding.  If it's easy for you, include a signature at the bottom of your response.  Remember that you can tell people to use the  osg-system-profiler  to collect information. It can shorten the number of times you ask for information because it collects quite a bit for you.  If you run across a problem that has a chance of being hit by other users, consider:  Is there a bug we should fix in the software? Or something we could improve in the software?  Is there a way to improve our documentation?  Can you extend our troubleshooting documents to help people track this down more quickly? Consider the troubleshooting documents to be as much for us as for our users.", 
            "title": "Handling tickets"
        }, 
        {
            "location": "/policy/software-support/#direct-email-vs-support", 
            "text": "If someone emails you directly for support, you have the choice of when to move it to a ticket. The recommended criteria are:   If it's easy to handle and you can definitely do it yourself, leave it in email.  If there's a chance that you can't do it in a timely fashion, turn it into a ticket.  If there's a chance that you might lose track of the email, turn it into a ticket.  If there's a chance that you might need help from others, turn it into a ticket.  If it's an unusual topic and other people would benefit from seeing the ticket (now or in the future), turn it into a ticket.", 
            "title": "Direct email vs. support"
        }, 
        {
            "location": "/policy/software-support/#goc-ticket-vs-jira", 
            "text": "GOC Ticket is for user support. This is where we helping users debug, understand their problems, etc.  JIRA is for tracking our work. It's meant for internal usage, not for user support.  In general, users should not ask for support via JIRA. A single user support ticket might result in zero, one, or multiple JIRA tickets. The user ticket may be closed even though the related JIRA tickets are open. (\"Hi, this is a bug that we can't fix for the next six months, but I've made an internal bug report you can see at ...\")", 
            "title": "GOC ticket vs. JIRA"
        }, 
        {
            "location": "/software/effort-tracking/", 
            "text": "Effort Tracking\n\n\nThis page describes a simple plan for tracking effort in the OSG Technology teams.\n\n\nBasic Ideas\n\n\nAt its simplest, we would like to understand how much effort is spent on various OSG Technology activities over time. The focus is on having reasonably accurate, unbiased data. We might use the data later, for example, to hone future OSG proposals. And of course, all federal funding is subject to effort tracking.\n\n\nThere are just a few simple ideas to keep in mind:\n\n\n\n\n\n\nEach week, report your effort on OSG Technology activities\n\n\nUpdate your numbers in the effort tracking google spreadsheet (ask BrianL for access) and include a section in your weekly status report; here is an example:\n\n\nEFFORT\nExternal development: 63% \nSupport: 12% \nLeave: 20% \nOutside: 5\n\n\n\n\n\n\n\n\n\nFollow standard federal regulations for calculating effort\n (e.g., OMB Circular A-21)\n\n\nThe main idea is that \nall\n of your job-related activity for a week equals 100%, whether that is exactly 40 hours of work, a little less (subject to your local institution\u2019s rules), or more. This implies that the same hours worked could result in different effort percentages reported from week to week; for example, 4 hours in a 40-hour week is 10%, but 4 hours in a 50-hour week (which I hope is exceedingly rare) is 8%.\n\n\nReport 100% of your effort each week, but note that all effort outside of the Technology area falls into a single category. Unless you work at UW\u2013Madison, we do not need to know any details about your effort outside of the Technology area. (BrianL will talk to UW\u2013Madison folks about local expectations.)\n\n\nIf you are assigned to the Technology area for less than 100%, please report your actual Technology effort accurately. Workloads vary from week to week. For example, suppose you are 50% Technology in general, but you actually work 24 hours in a 40-hour week; you should report 60% effort for that week. The goal is to present reality, not what you think management wants to see.\n\n\n\n\n\n\nEffort is reported as \ninteger\n percentages, no less accurate than 5% intervals\n \n     So please do not report percentages like 43.21% and please do not round to the nearest 10%.\n\n\n\n\n\n\nEffort Categories\n\n\nHere are the categories in which to track effort:\n\n\n\n\n\n\n\n\nInvestigations\n\n\nWork on the Investigations team\n\n\n\n\n\n\n\n\n\n\nExternal\n\n\nSoftware work that (generally) benefits our users; e.g., creating packages; updating existing ones; designing, coding, and testing new tools, existing tools, patches, or our software components\n\n\n\n\n\n\nInternal\n\n\nSoftware work on tools that we use to get work done; e.g., working on osg-test (for now), osg-build, Koji maintenance, the UW or UC ITB instances\n\n\n\n\n\n\nDocumentation\n\n\nWork on our TWiki or Markdown documentation\n\n\n\n\n\n\nRelease\n\n\nRelease team activities, primarily acceptance testing and cutting releases\n\n\n\n\n\n\nSupport\n\n\nUser support, including working on GOC tickets, direct support emails, some JIRA tickets that are more support than development, etc. It might be tricky to decide when support work becomes development work; generally, once a support ticket turns into a JIRA ticket and goes through the normal development lifecycle, then the JIRA-based work is development. If there is still extensive communication with GOC ticket users, that is still support.\n\n\n\n\n\n\nManagement\n\n\nThis is mainly for team leads; e.g., managing team activities and tickets (generally); hiring; \nleading\n (not just attending) meetings\n\n\n\n\n\n\nEducation\n\n\nNot for general learning or training activities\n The OSG Education area is essentially part of the Software area, because many technology-area members contribute to the OSG School. So this category is for OSG School effort (or other sanctioned OSG Education activities.\n\n\n\n\n\n\nAdmin\n\n\nGeneral administrative activities that benefit the OSG Technology area but that do not fit elsewhere \u2014 \nuse sparingly!!\n\n\n\n\n\n\nOutside\n\n\nFor all activities outside of the OSG Technology area (Madison team members should provide extra details, see BrianL)\n\n\n\n\n\n\nLeave\n\n\nThis is for holidays, vacation, and sick leave; count a full day of leave as 8.0 hours, count a half day as 4.0 hours\n\n\n\n\n\n\n\n\nA few thoughts about tricky situations:\n\n\n\n\n\n\nMeetings. If a meeting is specific to one of the categories above, use that category. If the meeting is more general (e.g., the weekly Monday meeting, or the OSG AHM), amortize your time according to your usual breakdown by category. For example, someone who spends nearly all of their time working on development tasks should count the Monday meeting as development time.\n\n\n\n\n\n\nAdministrative activities. This is probably the trickiest category. It certainly covers any administrative work that pertains to your activity in the OSG Technology area. But what about administrative activities that pertain to your employment in general, and not to any particular activity? In that case, and that case only, you should amortize the administrative activity between \nAdmin\n and \nOutside\n according to either (a) your appointment percentages between OSG Technology and non-Technology activities, or (b) your actual percentages between OSG Technology and non-Technology activities.\n\n\n\n\n\n\nOutside (non-Technology) activities that benefit the OSG Technology area. The simplest approach is to amortize the time. The more correct approach is to figure out where credit will be given for the work; if the OSG Annual Report will describe the work in one of the Technology sections, then it should be a Technology category; otherwise not.\n\n\n\n\n\n\nLearning activities. Put short amounts of learning time in their relevant development category. For instance, if Igor is showing Edgar how to use GlideTester, that goes into \nInternal\n. But for longer training events, or for events that are less obviously related to day-to-day activities, mark the time as \nAdmin\n, and maybe add a comment explaining the activity.\n\n\n\n\n\n\nUltimately, if you are not sure how to deal with a situation, ask BrianL and he will make something up and document it here (generically) for future reference.", 
            "title": "Effort Tracking"
        }, 
        {
            "location": "/software/effort-tracking/#effort-tracking", 
            "text": "This page describes a simple plan for tracking effort in the OSG Technology teams.", 
            "title": "Effort Tracking"
        }, 
        {
            "location": "/software/effort-tracking/#basic-ideas", 
            "text": "At its simplest, we would like to understand how much effort is spent on various OSG Technology activities over time. The focus is on having reasonably accurate, unbiased data. We might use the data later, for example, to hone future OSG proposals. And of course, all federal funding is subject to effort tracking.  There are just a few simple ideas to keep in mind:    Each week, report your effort on OSG Technology activities  Update your numbers in the effort tracking google spreadsheet (ask BrianL for access) and include a section in your weekly status report; here is an example:  EFFORT\nExternal development: 63% \nSupport: 12% \nLeave: 20% \nOutside: 5    Follow standard federal regulations for calculating effort  (e.g., OMB Circular A-21)  The main idea is that  all  of your job-related activity for a week equals 100%, whether that is exactly 40 hours of work, a little less (subject to your local institution\u2019s rules), or more. This implies that the same hours worked could result in different effort percentages reported from week to week; for example, 4 hours in a 40-hour week is 10%, but 4 hours in a 50-hour week (which I hope is exceedingly rare) is 8%.  Report 100% of your effort each week, but note that all effort outside of the Technology area falls into a single category. Unless you work at UW\u2013Madison, we do not need to know any details about your effort outside of the Technology area. (BrianL will talk to UW\u2013Madison folks about local expectations.)  If you are assigned to the Technology area for less than 100%, please report your actual Technology effort accurately. Workloads vary from week to week. For example, suppose you are 50% Technology in general, but you actually work 24 hours in a 40-hour week; you should report 60% effort for that week. The goal is to present reality, not what you think management wants to see.    Effort is reported as  integer  percentages, no less accurate than 5% intervals  \n     So please do not report percentages like 43.21% and please do not round to the nearest 10%.", 
            "title": "Basic Ideas"
        }, 
        {
            "location": "/software/effort-tracking/#effort-categories", 
            "text": "Here are the categories in which to track effort:     Investigations  Work on the Investigations team      External  Software work that (generally) benefits our users; e.g., creating packages; updating existing ones; designing, coding, and testing new tools, existing tools, patches, or our software components    Internal  Software work on tools that we use to get work done; e.g., working on osg-test (for now), osg-build, Koji maintenance, the UW or UC ITB instances    Documentation  Work on our TWiki or Markdown documentation    Release  Release team activities, primarily acceptance testing and cutting releases    Support  User support, including working on GOC tickets, direct support emails, some JIRA tickets that are more support than development, etc. It might be tricky to decide when support work becomes development work; generally, once a support ticket turns into a JIRA ticket and goes through the normal development lifecycle, then the JIRA-based work is development. If there is still extensive communication with GOC ticket users, that is still support.    Management  This is mainly for team leads; e.g., managing team activities and tickets (generally); hiring;  leading  (not just attending) meetings    Education  Not for general learning or training activities  The OSG Education area is essentially part of the Software area, because many technology-area members contribute to the OSG School. So this category is for OSG School effort (or other sanctioned OSG Education activities.    Admin  General administrative activities that benefit the OSG Technology area but that do not fit elsewhere \u2014  use sparingly!!    Outside  For all activities outside of the OSG Technology area (Madison team members should provide extra details, see BrianL)    Leave  This is for holidays, vacation, and sick leave; count a full day of leave as 8.0 hours, count a half day as 4.0 hours     A few thoughts about tricky situations:    Meetings. If a meeting is specific to one of the categories above, use that category. If the meeting is more general (e.g., the weekly Monday meeting, or the OSG AHM), amortize your time according to your usual breakdown by category. For example, someone who spends nearly all of their time working on development tasks should count the Monday meeting as development time.    Administrative activities. This is probably the trickiest category. It certainly covers any administrative work that pertains to your activity in the OSG Technology area. But what about administrative activities that pertain to your employment in general, and not to any particular activity? In that case, and that case only, you should amortize the administrative activity between  Admin  and  Outside  according to either (a) your appointment percentages between OSG Technology and non-Technology activities, or (b) your actual percentages between OSG Technology and non-Technology activities.    Outside (non-Technology) activities that benefit the OSG Technology area. The simplest approach is to amortize the time. The more correct approach is to figure out where credit will be given for the work; if the OSG Annual Report will describe the work in one of the Technology sections, then it should be a Technology category; otherwise not.    Learning activities. Put short amounts of learning time in their relevant development category. For instance, if Igor is showing Edgar how to use GlideTester, that goes into  Internal . But for longer training events, or for events that are less obviously related to day-to-day activities, mark the time as  Admin , and maybe add a comment explaining the activity.    Ultimately, if you are not sure how to deal with a situation, ask BrianL and he will make something up and document it here (generically) for future reference.", 
            "title": "Effort Categories"
        }, 
        {
            "location": "/software/release-planning/", 
            "text": "Plans for Future Releases\n\n\nThis informal page is the mapping of \"technology goals\" (e.g., \"release software Foo version X\") to release numbers. It is meant to be updated as the releases evolve (and items are moved back in schedule). For package support policy between release series, see \nthis page\n.\n\n\nUnless explicitly noted, bullet points refer to software in the release repo.\n\n\nThis page is not meant to track minor bugfixes or updates -- rather, its focus should be new features.\n\n\nOSG 3.4 (May 2017)\n\n\n\n\n\n\n\n\nPackage(s)\n\n\nChange in osg-release\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nBeStMan2\n\n\nDrop\n\n\nRetirement policy\n\n\n\n\n\n\nedg-mkgridmap\n\n\nDrop\n\n\nSOFTWARE-2600\n\n\n\n\n\n\nfrontier-squid\n\n\nModify\n\n\nVersion 3\n\n\n\n\n\n\nglexec\n\n\nDrop\n\n\nSOFTWARE-2620\n\n\n\n\n\n\nGRAM\n\n\nDrop\n\n\nSOFTWARE-2530\n\n\n\n\n\n\nGUMS\n\n\nDrop\n\n\nRetirement policy\n, \nSOFTWARE-2600\n\n\n\n\n\n\njglobus\n\n\nDrop\n\n\nSOFTWARE-2606\n\n\n\n\n\n\nnetlogger\n\n\nDrop\n\n\n\n\n\n\n\n\nosg-ce\n\n\nModify\n\n\nDrop \nGridFTP\n, \ngums-client\n\n\n\n\n\n\nosg-info-services\n\n\nDrop\n\n\n\n\n\n\n\n\nosg-version\n\n\nDrop\n\n\n\n\n\n\n\n\nsingularity\n\n\nAdd\n\n\n\n\n\n\n\n\nvoms-admin-server\n\n\nDrop\n\n\nRetirement policy\n\n\n\n\n\n\n\n\nTrack OSG 3.4 updates through its \nJIRA epic\n.\n\n\nSupport Policy for OSG 3.3\n\n\nAccording to our \nrelease support policy\n and the release date of May 2017 for OSG 3.4, OSG 3.3 will receive routine software updates until November 2017 and critical updates until May 2018.\n\n\nPrevious Releases\n\n\n12 November 2013\n\n\n\n\nOSG 3.1\n\n\nHTCondor-CE with PBS\n\n\nosg-configure emits an ERROR if squid defaults are not changed (\"UNAVAILABLE\" is a valid change)\n\n\n\n\n\n\nOSG 3.2\n\n\nInitial release (\nhow to create\n)\n\n\nHDFS 2.0.0 (already done in Upcoming)\n\n\nHTCondor 8.0.4\n\n\nglideinWMS 3.2.0\n\n\nosg-info-services (Note: ReSS will likely be retired around year-end)\n\n\nOSG 3.1 updates\n\n\n\n\n\n\nUpcoming\n\n\nHTCondor 8.1 with unified RPM\n\n\nBOSCO\n\n\n\n\n\n\n\n\n10 December 2013\n\n\n\n\nOSG 3.2\n\n\nRSV-for-VOs\n\n\nSquid must be present on OSG-CE (??? what does this mean?)", 
            "title": "Release Planning"
        }, 
        {
            "location": "/software/release-planning/#plans-for-future-releases", 
            "text": "This informal page is the mapping of \"technology goals\" (e.g., \"release software Foo version X\") to release numbers. It is meant to be updated as the releases evolve (and items are moved back in schedule). For package support policy between release series, see  this page .  Unless explicitly noted, bullet points refer to software in the release repo.  This page is not meant to track minor bugfixes or updates -- rather, its focus should be new features.", 
            "title": "Plans for Future Releases"
        }, 
        {
            "location": "/software/release-planning/#osg-34-may-2017", 
            "text": "Package(s)  Change in osg-release  Notes      BeStMan2  Drop  Retirement policy    edg-mkgridmap  Drop  SOFTWARE-2600    frontier-squid  Modify  Version 3    glexec  Drop  SOFTWARE-2620    GRAM  Drop  SOFTWARE-2530    GUMS  Drop  Retirement policy ,  SOFTWARE-2600    jglobus  Drop  SOFTWARE-2606    netlogger  Drop     osg-ce  Modify  Drop  GridFTP ,  gums-client    osg-info-services  Drop     osg-version  Drop     singularity  Add     voms-admin-server  Drop  Retirement policy     Track OSG 3.4 updates through its  JIRA epic .", 
            "title": "OSG 3.4 (May 2017)"
        }, 
        {
            "location": "/software/release-planning/#support-policy-for-osg-33", 
            "text": "According to our  release support policy  and the release date of May 2017 for OSG 3.4, OSG 3.3 will receive routine software updates until November 2017 and critical updates until May 2018.", 
            "title": "Support Policy for OSG 3.3"
        }, 
        {
            "location": "/software/release-planning/#previous-releases", 
            "text": "", 
            "title": "Previous Releases"
        }, 
        {
            "location": "/software/release-planning/#12-november-2013", 
            "text": "OSG 3.1  HTCondor-CE with PBS  osg-configure emits an ERROR if squid defaults are not changed (\"UNAVAILABLE\" is a valid change)    OSG 3.2  Initial release ( how to create )  HDFS 2.0.0 (already done in Upcoming)  HTCondor 8.0.4  glideinWMS 3.2.0  osg-info-services (Note: ReSS will likely be retired around year-end)  OSG 3.1 updates    Upcoming  HTCondor 8.1 with unified RPM  BOSCO", 
            "title": "12 November 2013"
        }, 
        {
            "location": "/software/release-planning/#10-december-2013", 
            "text": "OSG 3.2  RSV-for-VOs  Squid must be present on OSG-CE (??? what does this mean?)", 
            "title": "10 December 2013"
        }, 
        {
            "location": "/software/globus-mass-update-procedure/", 
            "text": "Globus mass update procedure\n\n\nGlobus consists of many packages, which we tend to update at the same time. This requires extra work, primarily to prevent dependency issues.\n\n\nPrep work\n\n\nDocs\n\n\nCreate a spreadsheet or table of the builds. Table should have NVR, perhaps URL, status (not started, imported, built, tested), and comments (mostly to record if it was a simple pass-through or not).\n\n\nGet packages to update, using \nosg-outdated-epel-pkgs\n from \nopensciencegrid/tools\n.\n\n\nTo get in N-V-R format:\n\n\n[you@host]$\n ./osg-outdated-epel-pkgs \n|\n \n\\\n\n    egrep \n^(globus|myproxy|gsi)\n \n|\n \n\\\n\n    awk \nBEGIN {OFS=\n} {print $1, \n-\n, $3}\n\n\n\n\n\n\nor to split up N and V-R in a comma-separated way (which you can feed into a Google Sheet to turn it into two columns):\n\n\n[you@host]$\n ./osg-outdated-epel-pkgs \n|\n \n\\\n\n    egrep \n^(globus|myproxy|gsi)\n \n|\n \n\\\n\n    awk \nBEGIN {OFS=\n} {print $1, \n,\n, $3}\n\n\n\n\n\n\nSVN\n\n\nCreate a separate SVN branch and populate it with all the packages you will update. (Get the list from the doc created above).\n\n\n[you@uw]$\n svn mkdir file:///p/vdt/workspace/svn/native/redhat/branches/globus\n\n#\n## From a checkout, in native/redhat\n\n\n[you@uw]$\n \nfor\n x in \nPACKAGES\n;\n \ndo\n \n\\\n\n     svn copy \n$x\n branches/globus/\n${\nx\n#trunk/\n}\n;\n \n\\\n\n   \ndone\n\n\n\n\n\n\nKoji (Mat/Carl)\n\n\nThis requires a Koji administrator. Koji admins as of August 2017 are Mat Selmeci and Carl Edquist.\n\n\nEnsure Koji tags exist: a destination tag, and a build tag, one for each dver, e.g.:\n\n\n\n\nel6-globus\n\n\nel6-globus-build\n\n\nel7-globus\n\n\nel7-globus-build\n\n\n\n\nSet up tag inheritence: base the build tags off of the corresponding \ndist-el?-build\n tag. This is because we don't want old osg packages interfering with the new versions we're building. These may already exist -- check the \nel?-globus-build\n tags in the web interface.\n\n\n[you@host]$\n \nfor\n el in el6 el7\n;\n \ndo\n \n\\\n\n        osg-koji add-tag --parent\n=\ndist-\n$el\n-build \n\\\n\n            --arches\n=\nx86_64 \n$el\n-globus-build\n;\n \n\\\n\n    \ndone\n\n\n\n\n\n\nTag \nbuildsys-macros\n for the OSG release into the build tags:\n\n\n[you@host]$\n \nfor\n el in el6 el7\n;\n \ndo\n \n\\\n\n       \nbuildsys_macros_nvr\n=\n$(\nosg-koji -q list-tagged osg-3.4-\n$el\n-development \n\\\n\n         buildsys-macros --latest \n|\n awk \n{print $1}\n)\n;\n \n\\\n\n       osg-koji tag-pkg \n$el\n-globus-build \n$buildsys_macros_nvr\n;\n \n\\\n\n   \ndone\n\n\n\n\n\n\nEnsure Koji targets exist, one for each dver, e.g.:\n\n\n\n\nel6-globus (el6-globus-build \n el6-globus)\n\n\nel7-globus (el7-globus-build \n el7-globus)\n\n\nkojira-fake-el6-globus (el6-globus \n kojira-fake)\n\n\nkojira-fake-el7-globus (el7-globus \n kojira-fake)\n\n\n\n\n[you@host]$\n \nfor\n el in el6 el7\n;\n \ndo\n \n\\\n\n       osg-koji add-target \n$el\n-globus \n$el\n-globus-build \n$el\n-globus\n;\n \n\\\n\n       osg-koji add-target kojira-fake-\n$el\n-globus \n$el\n-globus kojira-fake\n;\n \n\\\n\n   \ndone\n\n\n\n\n\n\nIf basing the packages off of the Globus repos, add the Globus repos as external repos, and add them to the build tags (but not the dest tags).\n\n\nEdit \n/etc/koji-hub/plugins/sign.conf\n and set up the GPG signing for the RPMs. Run \n/etc/koji-hub/plugins/fix-permissions\n after editing the file.\n\n\nPer-package work\n\n\n\n\ncd into branches/globus\n\n\nDownload packages from \nhttp://dl.fedoraproject.org/pub/epel/6/SRPMS/\n\n\n\n\nA useful alias:\n\n\n[you@host]$\n \nalias\n osg-build-globus\n=\nosg-build koji --ktt el6-globus --ktt el7-globus\n\n\n\n\n\n\nStrict pass-through (no osg/ directory)\n\n\n\n\n\n\nRun:\n\n\n[you@uw]$\n osg-import-srpm \nURL\n\n\n[you@uw]$\n osg-build-globus --scratch \nPKG\n\n\n\n\n\n\n\n\n\n\nCommit - use a message like \"Update to 3.12-1 from EPEL (SOFTWARE-2197)\"\n\n\n\n\n\n\nDo a non-scratch build.\n\n\n\n\n\n\nNon-strict pass-through\n\n\n\n\n\n\nRun:\n\n\n[you@uw]$\n osg-import-srpm --diff3 \nURL\n\n\n\n\n\n\n\n\n\n\nFix merge conflicts in the spec file. If not already there, put a .1 after the Release number to mark the changes as ours.\n\n\n\n\n\n\nRun:\n\n\n[you@uw]$\n osg-build quilt \nPKG\n\n\n\n\n\n\n\n\n\n\nFix patches if necessary.\n\n\n\n\n\n\nRun:\n\n\n[you@uw\n]$\n osg-build-globus --scratch \nPKG\n\n\n\n\n\n\n\n\n\n\nCommit - use a message like \"Update to 8.29-1 from EPEL and merge OSG changes (SOFTWARE-2197)\"\n\n\n\n\n\n\nDo a non-scratch build.\n\n\n\n\n\n\nTesting\n\n\nCreate a yum \n.repo\n file similar to \nosg-minefield\n that installs from the \nel?-globus\n repos. Enable this and \nosg-minefield\n.\n\n\nEL7 example:\n\n\n[globus]\n\n\nname\n=\nglobus\n\n\nbaseurl\n=\nhttp://koji.chtc.wisc.edu/mnt/koji/repos/el7-globus/latest/$basearch/\n\n\nfailovermethod\n=\npriority\n\n\npriority\n=\n98\n\n\nenabled\n=\n1\n\n\ngpgcheck\n=\n0\n\n\ngpgkey\n=\nfile:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\n\n\nconsider_as_osg\n=\nyes\n\n\n\n\n\n\nMerge\n\n\nKoji (Mat/Carl)\n\n\nThis requires a Koji administrator. Koji admins as of August 2017 are Mat Selmeci and Carl Edquist.\n\n\n\n\nUntag broken versions that we don't want to ship.\n\n\nUse \nmove-pkg\n:\n[you@host\n]$\n \nfor\n el in el6 el7\n;\n \ndo\n \n\\\n\n        osg-koji -q list-tagged \n${\nel\n}\n-globus \n|\n \n\\\n\n            awk \n{print $1}\n \n \n${\nel\n}\n-tagged.txt\n;\n \n\\\n\n    \ndone\n\n\n#\n## Check the txts if they look sane\n\n\n[you@host\n]$\n \nfor\n el in el6 el7\n;\n \ndo\n \n\\\n\n        xargs -a \n${\nel\n}\n-tagged.txt \n\\\n\n            osg-koji move-pkg \n${\nel\n}\n-globus \n\\\n\n                osg-3.3-\n${\nel\n}\n-development\n;\n \n\\\n\n    \ndone\n\n\n\n\n\n\n\n\n\n\nSVN\n\n\n\n\nMerge from \ntrunk\n to \nbranches/globus\n first, to pick up any globus changes that may have happened in trunk.\n\n\nMerge from \nbranches/globus\n to \ntrunk\n.\n\n\nMove \nbranches/globus\n to \ntags/globus-\nDATE\n.", 
            "title": "Globus Mass Update Procedure"
        }, 
        {
            "location": "/software/globus-mass-update-procedure/#globus-mass-update-procedure", 
            "text": "Globus consists of many packages, which we tend to update at the same time. This requires extra work, primarily to prevent dependency issues.", 
            "title": "Globus mass update procedure"
        }, 
        {
            "location": "/software/globus-mass-update-procedure/#prep-work", 
            "text": "", 
            "title": "Prep work"
        }, 
        {
            "location": "/software/globus-mass-update-procedure/#docs", 
            "text": "Create a spreadsheet or table of the builds. Table should have NVR, perhaps URL, status (not started, imported, built, tested), and comments (mostly to record if it was a simple pass-through or not).  Get packages to update, using  osg-outdated-epel-pkgs  from  opensciencegrid/tools .  To get in N-V-R format:  [you@host]$  ./osg-outdated-epel-pkgs  |   \\ \n    egrep  ^(globus|myproxy|gsi)   |   \\ \n    awk  BEGIN {OFS= } {print $1,  - , $3}   or to split up N and V-R in a comma-separated way (which you can feed into a Google Sheet to turn it into two columns):  [you@host]$  ./osg-outdated-epel-pkgs  |   \\ \n    egrep  ^(globus|myproxy|gsi)   |   \\ \n    awk  BEGIN {OFS= } {print $1,  , , $3}", 
            "title": "Docs"
        }, 
        {
            "location": "/software/globus-mass-update-procedure/#svn", 
            "text": "Create a separate SVN branch and populate it with all the packages you will update. (Get the list from the doc created above).  [you@uw]$  svn mkdir file:///p/vdt/workspace/svn/native/redhat/branches/globus # ## From a checkout, in native/redhat  [you@uw]$   for  x in  PACKAGES ;   do   \\ \n     svn copy  $x  branches/globus/ ${ x #trunk/ } ;   \\ \n    done", 
            "title": "SVN"
        }, 
        {
            "location": "/software/globus-mass-update-procedure/#koji-matcarl", 
            "text": "This requires a Koji administrator. Koji admins as of August 2017 are Mat Selmeci and Carl Edquist.  Ensure Koji tags exist: a destination tag, and a build tag, one for each dver, e.g.:   el6-globus  el6-globus-build  el7-globus  el7-globus-build   Set up tag inheritence: base the build tags off of the corresponding  dist-el?-build  tag. This is because we don't want old osg packages interfering with the new versions we're building. These may already exist -- check the  el?-globus-build  tags in the web interface.  [you@host]$   for  el in el6 el7 ;   do   \\ \n        osg-koji add-tag --parent = dist- $el -build  \\ \n            --arches = x86_64  $el -globus-build ;   \\ \n     done   Tag  buildsys-macros  for the OSG release into the build tags:  [you@host]$   for  el in el6 el7 ;   do   \\ \n        buildsys_macros_nvr = $( osg-koji -q list-tagged osg-3.4- $el -development  \\ \n         buildsys-macros --latest  |  awk  {print $1} ) ;   \\ \n       osg-koji tag-pkg  $el -globus-build  $buildsys_macros_nvr ;   \\ \n    done   Ensure Koji targets exist, one for each dver, e.g.:   el6-globus (el6-globus-build   el6-globus)  el7-globus (el7-globus-build   el7-globus)  kojira-fake-el6-globus (el6-globus   kojira-fake)  kojira-fake-el7-globus (el7-globus   kojira-fake)   [you@host]$   for  el in el6 el7 ;   do   \\ \n       osg-koji add-target  $el -globus  $el -globus-build  $el -globus ;   \\ \n       osg-koji add-target kojira-fake- $el -globus  $el -globus kojira-fake ;   \\ \n    done   If basing the packages off of the Globus repos, add the Globus repos as external repos, and add them to the build tags (but not the dest tags).  Edit  /etc/koji-hub/plugins/sign.conf  and set up the GPG signing for the RPMs. Run  /etc/koji-hub/plugins/fix-permissions  after editing the file.", 
            "title": "Koji (Mat/Carl)"
        }, 
        {
            "location": "/software/globus-mass-update-procedure/#per-package-work", 
            "text": "cd into branches/globus  Download packages from  http://dl.fedoraproject.org/pub/epel/6/SRPMS/   A useful alias:  [you@host]$   alias  osg-build-globus = osg-build koji --ktt el6-globus --ktt el7-globus", 
            "title": "Per-package work"
        }, 
        {
            "location": "/software/globus-mass-update-procedure/#strict-pass-through-no-osg-directory", 
            "text": "Run:  [you@uw]$  osg-import-srpm  URL  [you@uw]$  osg-build-globus --scratch  PKG     Commit - use a message like \"Update to 3.12-1 from EPEL (SOFTWARE-2197)\"    Do a non-scratch build.", 
            "title": "Strict pass-through (no osg/ directory)"
        }, 
        {
            "location": "/software/globus-mass-update-procedure/#non-strict-pass-through", 
            "text": "Run:  [you@uw]$  osg-import-srpm --diff3  URL     Fix merge conflicts in the spec file. If not already there, put a .1 after the Release number to mark the changes as ours.    Run:  [you@uw]$  osg-build quilt  PKG     Fix patches if necessary.    Run:  [you@uw ]$  osg-build-globus --scratch  PKG     Commit - use a message like \"Update to 8.29-1 from EPEL and merge OSG changes (SOFTWARE-2197)\"    Do a non-scratch build.", 
            "title": "Non-strict pass-through"
        }, 
        {
            "location": "/software/globus-mass-update-procedure/#testing", 
            "text": "Create a yum  .repo  file similar to  osg-minefield  that installs from the  el?-globus  repos. Enable this and  osg-minefield .  EL7 example:  [globus]  name = globus  baseurl = http://koji.chtc.wisc.edu/mnt/koji/repos/el7-globus/latest/$basearch/  failovermethod = priority  priority = 98  enabled = 1  gpgcheck = 0  gpgkey = file:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG  consider_as_osg = yes", 
            "title": "Testing"
        }, 
        {
            "location": "/software/globus-mass-update-procedure/#merge", 
            "text": "", 
            "title": "Merge"
        }, 
        {
            "location": "/software/globus-mass-update-procedure/#koji-matcarl_1", 
            "text": "This requires a Koji administrator. Koji admins as of August 2017 are Mat Selmeci and Carl Edquist.   Untag broken versions that we don't want to ship.  Use  move-pkg : [you@host ]$   for  el in el6 el7 ;   do   \\ \n        osg-koji -q list-tagged  ${ el } -globus  |   \\ \n            awk  {print $1}     ${ el } -tagged.txt ;   \\ \n     done  # ## Check the txts if they look sane  [you@host ]$   for  el in el6 el7 ;   do   \\ \n        xargs -a  ${ el } -tagged.txt  \\ \n            osg-koji move-pkg  ${ el } -globus  \\ \n                osg-3.3- ${ el } -development ;   \\ \n     done", 
            "title": "Koji (Mat/Carl)"
        }, 
        {
            "location": "/software/globus-mass-update-procedure/#svn_1", 
            "text": "Merge from  trunk  to  branches/globus  first, to pick up any globus changes that may have happened in trunk.  Merge from  branches/globus  to  trunk .  Move  branches/globus  to  tags/globus- DATE .", 
            "title": "SVN"
        }, 
        {
            "location": "/software/git-software-development/", 
            "text": "Git software development workflow\n\n\nThis document describes the development workflow for OSG software packages kept in GitHub. It is intended for people who wish to contribute to OSG software.\n\n\nGit and GitHub basics\n\n\nIf you are unfamiliar with Git and GitHub, the GitHub website has a good series of tutorials at \nhttps://help.github.com/categories/bootcamp/\n\n\nGetting shell access to GitHub\n\n\nThere are multiple ways of authenticating to GitHub from the shell. This section will cover using SSH keys. This is no longer the method recommended by GitHub, but is easier to set up for someone with existing SSH experience.\n\n\nThe instructions here are derived from \nGitHub's own instructions on using SSH keys\n.\n\n\nCreating a new SSH key (optional but recommended)\n\n\nIf you already have an SSH keypair in your \n~/.ssh\n directory that you want to use for GitHub, you may skip this step. It is more secure, however, to create a new keypair specifically for use with GitHub.\n\n\nThe instructions below will create an SSH public/private key pair with the private key stored in \n~/.ssh/id_github\n and public key stored in \n~/.ssh/id_github.pub\n.\n\n\nGenerating the key\n\n\nUse \nssh-keygen\n to generate the SSH keypair. For \nEMAIL_ADDRESS\n, use the email address associated with your GitHub account.\n\n\n[user@client ~ ] $\n ssh-keygen -t rsa -b \n4096\n -f ~/.ssh/id_github -C \nEMAIL_ADDRESS\n\n\n\n\n\n\nConfiguring SSH to use the key for GitHub\n\n\nMake sure SSH uses the new key by default to access GitHub. Create or edit \n~/.ssh/config\n and append the following lines:\n\n\nHost github.com\nIdentityFile \nYOUR_HOME_DIR\n/.ssh/id_github\n\n\n\n\n\nAdding the SSH public key to GitHub\n\n\nUsing the GitHub web interface:\n\n\n\n\nOn the upper right of the screen, click on your profile picture\n\n\nIn the menu that pops up, click \"Settings\"\n\n\nOn the left-hand sidebar, click \"SSH and GPG keys\"\n\n\nIn the top right of the \"SSH keys\" box, click \"New SSH key\"\n\n\nIn the \"Title\" field of the dialog that pops up, enter a descriptive name for the key\n\n\nOpen the public key file (e.g. \n~/.ssh/id_github.pub\n (don't forget the \n.pub\n)) in a text editor and copy its full contents to the clipboard\n\n\nIn the \"Key\" field, paste the public key\n\n\nBelow the \"Key\" field, click \"Add SSH key\"\n\n\n\n\nYou should see your new key in the \"SSH keys\" list.\n\n\nTesting that shell access works\n\n\nTo verify you can authenticate to GitHub using SSH, SSH to \ngit@github.com\n. You should see a message that 'you've successfully authenticated, but GitHub does not provide shell access.'\n\n\nContribution workflow\n\n\nWe use the standard GitHub \npull request\n workflow for making contributions to OSG software.\n\n\nIf you've never contributed to this project on GitHub before, do the following steps first:\n\n\n\n\nUsing the GitHub web interface, fork the repo you wish to contribute to.\n\n\n\n\nMake a clone of your forked repo on your local machine.\n\n\n[user@client ~ ] $\n git clone \ngit@github.com\n:\nUSERNAME/PROJECT\n\n\n\n\n\n\n\n\nNote\n\n\nIf you get a \"Permission denied\" error, your public key may not be set up with GitHub -- please see the \"Getting shell access to GitHub\" section above.\n\n\nIf you get some other error, \nthe GitHub page on SSH\n may contain useful information on troubleshooting.\n\n\n\n\n\n\n\n\nOnce you have your local repo, do the following:\n\n\n\n\n\n\nCreate a branch to hold changes that are related to the issue you are working on. Give the branch a name that will remind you of its purpose, such as \nsw2345-pathchange\n\n\n[user@client ~ ] $\n git checkout -b \nBRANCH\n\n\n\n\n\n\n\n\n\n\nMake your commits to this branch, then push the branch to your repo on GitHub.\n\n\n[user@client ~ ] $\n git push origin \nBRANCH\n\n\n\n\n\n\n\n\n\n\nSelect your branch in the GitHub web interface, then create a \"pull request\" against the original repo. Add a good description of your change into the message for the pull request. Enter a JIRA ticket number in the message to automatically link the pull request to the JIRA ticket.\n\n\n\n\n\n\nRequest a review from the drop down menu on the right and wait for your pull request to be reviewed by a software team member.\n\n\n\n\nIf the team member accepts your changes, they will merge your pull request, and your changes will be incorporated upstream. You may then delete the branch you created your pull request from.\n\n\nIf your changes are rejected, then you may make additional changes to the branch that your pull request is for. Once you push the changes from your local repo to your GitHub repo, they will automatically be added to the pull request.\n\n\n\n\n\n\n\n\nRelease workflow\n\n\nThis section is intended for OSG Software team members or the primary developers of a software project (i.e. those that make releases). Some of the steps require direct write access the GitHub repo for the project owned by \nopensciencegrid\n. (If you can approve pull requests, you have write access).\n\n\nA release of a software is created from your local clone of a software project. Before you release, you need to make sure your local clone is in sync with the GitHub repo owned by \nopensciencegrid\n (the OSG repo):\n\n\n\n\n\n\nIf you haven't already, add the OSG repo as a \"remote\" to your repo:\n\n\n[user@client ~ ] $\n git remote add upstream git@github.com:opensciencegrid/\nPROJECT\n\n\n\n\n\n\n\n\n\n\nFetch changes from the OSG repo:\n\n\n[user@client ~ ] $\n git fetch upstream\n\n\n\n\n\n\n\n\n\nCompare your branch you are releasing from (probably \nmaster\n) to its copy in the OSG repo:\n\n\n[user@client ~ ] $\n git checkout master\n;\n git diff upstream/master\n\n\n\n\n\nThere should be no differences.\n\n\n\n\n\n\nOnce this is done, release the software as you usually do. This process varies from one project to another, but often it involves running \nmake upstream\n or similar. Check your project's \nREADME\n file for instructions.\n\n\n\n\nTest your software.\n\n\n\n\nTag the commit that you made the release from. Git release tags are conventionally called \nVERSION\n, where \nVERSION\n is the version of the software you are releasing. So if you're releasing version 1.3.0, you would create the tag \nv1.3.0\n.\n\n\n\n\nNote\n\n\nOnce a tag has been pushed to the OSG repo, it should not be changed. Be sure the commit you want to tag is the final one you made the release from.\n\n\n\n\n\n\n\n\nCreate the tag in your local repo:\n\n\n[user@client ~ ] $\n git tag \nTAG\n\n\n\n\n\n\n\n\n\n\nPush the tag to your own GitHub repo:\n\n\n[user@client ~ ] $\n git push origin \nTAG\n\n\n\n\n\n\n\n\n\n\nPush the tag to the OSG repo:\n\n\n[user@client ~ ] $\n git push upstream \nTAG", 
            "title": "Git Software Development Process"
        }, 
        {
            "location": "/software/git-software-development/#git-software-development-workflow", 
            "text": "This document describes the development workflow for OSG software packages kept in GitHub. It is intended for people who wish to contribute to OSG software.", 
            "title": "Git software development workflow"
        }, 
        {
            "location": "/software/git-software-development/#git-and-github-basics", 
            "text": "If you are unfamiliar with Git and GitHub, the GitHub website has a good series of tutorials at  https://help.github.com/categories/bootcamp/", 
            "title": "Git and GitHub basics"
        }, 
        {
            "location": "/software/git-software-development/#getting-shell-access-to-github", 
            "text": "There are multiple ways of authenticating to GitHub from the shell. This section will cover using SSH keys. This is no longer the method recommended by GitHub, but is easier to set up for someone with existing SSH experience.  The instructions here are derived from  GitHub's own instructions on using SSH keys .", 
            "title": "Getting shell access to GitHub"
        }, 
        {
            "location": "/software/git-software-development/#creating-a-new-ssh-key-optional-but-recommended", 
            "text": "If you already have an SSH keypair in your  ~/.ssh  directory that you want to use for GitHub, you may skip this step. It is more secure, however, to create a new keypair specifically for use with GitHub.  The instructions below will create an SSH public/private key pair with the private key stored in  ~/.ssh/id_github  and public key stored in  ~/.ssh/id_github.pub .", 
            "title": "Creating a new SSH key (optional but recommended)"
        }, 
        {
            "location": "/software/git-software-development/#generating-the-key", 
            "text": "Use  ssh-keygen  to generate the SSH keypair. For  EMAIL_ADDRESS , use the email address associated with your GitHub account.  [user@client ~ ] $  ssh-keygen -t rsa -b  4096  -f ~/.ssh/id_github -C  EMAIL_ADDRESS", 
            "title": "Generating the key"
        }, 
        {
            "location": "/software/git-software-development/#configuring-ssh-to-use-the-key-for-github", 
            "text": "Make sure SSH uses the new key by default to access GitHub. Create or edit  ~/.ssh/config  and append the following lines:  Host github.com\nIdentityFile  YOUR_HOME_DIR /.ssh/id_github", 
            "title": "Configuring SSH to use the key for GitHub"
        }, 
        {
            "location": "/software/git-software-development/#adding-the-ssh-public-key-to-github", 
            "text": "Using the GitHub web interface:   On the upper right of the screen, click on your profile picture  In the menu that pops up, click \"Settings\"  On the left-hand sidebar, click \"SSH and GPG keys\"  In the top right of the \"SSH keys\" box, click \"New SSH key\"  In the \"Title\" field of the dialog that pops up, enter a descriptive name for the key  Open the public key file (e.g.  ~/.ssh/id_github.pub  (don't forget the  .pub )) in a text editor and copy its full contents to the clipboard  In the \"Key\" field, paste the public key  Below the \"Key\" field, click \"Add SSH key\"   You should see your new key in the \"SSH keys\" list.", 
            "title": "Adding the SSH public key to GitHub"
        }, 
        {
            "location": "/software/git-software-development/#testing-that-shell-access-works", 
            "text": "To verify you can authenticate to GitHub using SSH, SSH to  git@github.com . You should see a message that 'you've successfully authenticated, but GitHub does not provide shell access.'", 
            "title": "Testing that shell access works"
        }, 
        {
            "location": "/software/git-software-development/#contribution-workflow", 
            "text": "We use the standard GitHub  pull request  workflow for making contributions to OSG software.  If you've never contributed to this project on GitHub before, do the following steps first:   Using the GitHub web interface, fork the repo you wish to contribute to.   Make a clone of your forked repo on your local machine.  [user@client ~ ] $  git clone  git@github.com : USERNAME/PROJECT    Note  If you get a \"Permission denied\" error, your public key may not be set up with GitHub -- please see the \"Getting shell access to GitHub\" section above.  If you get some other error,  the GitHub page on SSH  may contain useful information on troubleshooting.     Once you have your local repo, do the following:    Create a branch to hold changes that are related to the issue you are working on. Give the branch a name that will remind you of its purpose, such as  sw2345-pathchange  [user@client ~ ] $  git checkout -b  BRANCH     Make your commits to this branch, then push the branch to your repo on GitHub.  [user@client ~ ] $  git push origin  BRANCH     Select your branch in the GitHub web interface, then create a \"pull request\" against the original repo. Add a good description of your change into the message for the pull request. Enter a JIRA ticket number in the message to automatically link the pull request to the JIRA ticket.    Request a review from the drop down menu on the right and wait for your pull request to be reviewed by a software team member.   If the team member accepts your changes, they will merge your pull request, and your changes will be incorporated upstream. You may then delete the branch you created your pull request from.  If your changes are rejected, then you may make additional changes to the branch that your pull request is for. Once you push the changes from your local repo to your GitHub repo, they will automatically be added to the pull request.", 
            "title": "Contribution workflow"
        }, 
        {
            "location": "/software/git-software-development/#release-workflow", 
            "text": "This section is intended for OSG Software team members or the primary developers of a software project (i.e. those that make releases). Some of the steps require direct write access the GitHub repo for the project owned by  opensciencegrid . (If you can approve pull requests, you have write access).  A release of a software is created from your local clone of a software project. Before you release, you need to make sure your local clone is in sync with the GitHub repo owned by  opensciencegrid  (the OSG repo):    If you haven't already, add the OSG repo as a \"remote\" to your repo:  [user@client ~ ] $  git remote add upstream git@github.com:opensciencegrid/ PROJECT     Fetch changes from the OSG repo:  [user@client ~ ] $  git fetch upstream    Compare your branch you are releasing from (probably  master ) to its copy in the OSG repo:  [user@client ~ ] $  git checkout master ;  git diff upstream/master  There should be no differences.    Once this is done, release the software as you usually do. This process varies from one project to another, but often it involves running  make upstream  or similar. Check your project's  README  file for instructions.   Test your software.   Tag the commit that you made the release from. Git release tags are conventionally called  VERSION , where  VERSION  is the version of the software you are releasing. So if you're releasing version 1.3.0, you would create the tag  v1.3.0 .   Note  Once a tag has been pushed to the OSG repo, it should not be changed. Be sure the commit you want to tag is the final one you made the release from.     Create the tag in your local repo:  [user@client ~ ] $  git tag  TAG     Push the tag to your own GitHub repo:  [user@client ~ ] $  git push origin  TAG     Push the tag to the OSG repo:  [user@client ~ ] $  git push upstream  TAG", 
            "title": "Release workflow"
        }, 
        {
            "location": "/software/osg-build-tools/", 
            "text": "OSG Build Tools\n\n\nThis page documents the tools used for RPM development for the OSG Software Stack. See \nthe RPM development guide\n for the principles on which these tools are based.\n\n\nThe tools are distributed in the \nosg-build\n RPM in our repositories, but can also be used from a Git clone of \nopensciencegrid/osg-build on GitHub\n.\n\n\nThis page is up-to-date as of \nosg-build\n version 1.10.1.\n\n\nThe tools\n\n\nosg-build\n\n\nOverview\n\n\nThis is the primary tool used in building source and binary RPMs.\n\n\n\n\nosg-build \nTASK\n [options] \nPACKAGE DIRECTORY\n [...]\n\n\n\n\npackage_directory\n is a directory containing an \nosg/\n and/or an \nupstream/\n subdirectory. See \nthe RPM development guide\n for how these directories are organized.\n\n\nTasks\n\n\nkoji\n\n\nPrebuilds the final source package, then builds it remotely using the Koji instance hosted at UW-Madison. \nhttps://koji.chtc.wisc.edu\n By default, the resulting RPMs will end up in the osg-minefield repositories based on the most recent OSG major version (e.g. 3.4). You may specify a different set of repos with \n--repo\n, described later. RPMs from the osg-minefield repositories are regularly pulled to the osg-development repositories hosted by the GOC at \nhttp://repo.grid.iu.edu\n Unless you specify otherwise (by passing \n--el6\n, \n--el7\n or specifying a different koji tag/target), the package will be built for both el6 and el7. This is the method used to build final versions of packages you expect to ship.\n\n\nlint\n\n\nPrebuilds the final source package, then runs \nrpmlint\n on it to check for various problems. You will need to have \nrpmlint\n installed. People on UW CSL machines should add \n/p/vdt/workspace/rpmlint\n to their $PATH.\n\n\nmock\n\n\nPrebuilds the final source package, then builds it locally using \nmock\n, and stores the resulting source and binary RPMs in the package-specific \n_build_results\n directory.\n\n\nprebuild\n\n\nPrebuilds the final source package from upstream sources (if any) and local files (if any). May create or overwrite the \n_upstream_srpm_contents\n and \n_final_srpm_contents\n directories.\n\n\nprepare\n\n\nPrebuilds the final source package, then calls \nrpmbuild -bp\n on the result, extracting and patching the source files (and performing any other steps defined in the \n%prep\n section of the spec file. The resulting sources will be under \n_final_srpm_contents\n.\n\n\nrpmbuild\n\n\nPrebuilds the final source package, then builds it locally using \nrpmbuild\n, and stores the resulting source and binary RPMs in the package-specific \n_build_results\n directory.\n\n\nquilt\n\n\nCollects the upstream local sources and spec file, then calls \nquilt setup\n on the spec file, extracting the source files and adding the patches to a quilt series file. See \nQuilt documentation (PDF link)\n for more information on quilt; also look at the example in the Usage Patterns section below. Similar to \nprepare\n (in fact, \nquilt\n calls \nrpmbuild -bp\n behind the scenes), but the source tree is in pre-patch state, and various quilt commands can be used to apply and modify patches. Unpacks into \n_quilt\n as of \nosg-build-1.2.2\n or \n_final_srpm_contents\n in previous versions. Requires \nquilt\n. People on UW CSL machines should add \n/p/vdt/workspace/quilt/bin\n to their \n$PATH\n, and \n/p/vdt/workspace/quilt/share/man\n to their \n$MANPATH\n.\n\n\nOptions\n\n\nThis section lists the command-line options.\n\n\n--help\n\n\nPrints the built-in usage information and exits without doing anything else.\n\n\n--version\n\n\nPrints the version of \nosg-build\n and exits without doing anything else.\n\n\nCommon Options\n\n\n-a, --autoclean, --no-autoclean\n\n\nBefore each build, clean out the contents of the underscore directories (_build_results, _final_srpm_contents, _upstream_srpm_contents, _upstream_tarball_contents). If the directories are not cleaned up, earlier builds of a package may interfere with later ones. \n--no-autoclean\n will disable this.\n\n\nDefault is \ntrue\n.\n\n\nHas no effect with the \n--vcs\n flag.\n\n\n-c, --cache-prefix \nprefix\n\n\nSets the \nprefix\n for upstream source cache references. The prefix must be a valid URI starting with either \nhttp\n, \nhttps\n, or \nfile\n, or one of the following special values:\n\n\n\n\nAFS (corresponds to \nfile:///p/vdt/public/html/upstream\n, which is the location of the VDT cache using AFS from a UW CS machine).\n\n\nVDT (corresponds to \nhttp://vdt.cs.wisc.edu/upstream\n, which is the location of the VDT cache from off-site).\n\n\nAUTO (AFS if available, VDT if not)\n\n\n\n\nThe upstream source cache must be organized as described above. All files referenced by \n.source\n files in the affected packages must exist in the cache, or a runtime error will occur.\n\n\nDefault is \nAUTO\n.\n\n\nHas no effect with the \n--vcs\n flag.\n\n\n--el6, --el7, --redhat-release \nversion\n (Config: redhat_release)\n\n\nSets the distro version to build for. This affects the %dist tag, the mock config, and the default koji tag and target (unless otherwise specified).\n\n\n--el6\n is equivalent to \n--redhat-release 6\n\n\n--el7\n is equivalent to \n--redhat-release 7\n\n\n--loglevel \nloglevel\n\n\nSets the verbosity of the script. Valid values are: \ndebug\n, \ninfo\n, \nwarning\n, \nerror\n and \ncritical\n.\n\n\nDefault is \ninfo\n.\n\n\n-q, --quiet\n\n\nDo not display as much information. Equivalent to \n--loglevel warning\n\n\n-v, --verbose\n\n\nDisplay more information. Equivalent to \n--loglevel debug\n\n\n-w, --working-directory \npath\n\n\nUse \npath\n as the root directory of the files created by the script. For example, if \npath\n is \n$HOME/working\n, and the package being built is \nndt\n, the following tree will be created:\n\n\n\n\n$HOME/working/ndt/_upstream_srpm_contents\n\n\n$HOME/working/ndt/_upstream_tarball_contents\n\n\n$HOME/working/ndt/_final_srpm_contents\n\n\n$HOME/working/ndt/_build_results\n\n\n\n\nIf \npath\n is \nTEMP\n, a randomly named directory under \n/tmp\n is used as the working directory.\n\n\nThe default setting is to use the package directory as the working directory.\n\n\nHas no effect with the \n--vcs\n flag.\n\n\nOptions specific to prebuild task\n\n\n--full-extract\n\n\nIf set, all upstream tarballs will be extracted into \n_upstream_tarball_contents/\n during the prebuild step. This flag is now mostly redundant with the \nprepare\n and \nquilt\n tasks.\n\n\nOptions specific to rpmbuild and mock tasks\n\n\n--distro-tag \ndist\n\n\nSets the distribution tag added on to the end of the release in the RPM ( \nrpmbuild\n and \nmock\n tasks only ).\n\n\nDefault is \n.osg.el6\n or \n.osg.el7\n\n\n-t, --target-arch \narch\n\n\nSpecify an architecture to build packages for ( \nrpmbuild\n and \nmock\n tasks only ).\n\n\nDefault is unspecified, which builds for the current machine architecture.\n\n\nOptions specific to mock task\n\n\n--mock-clean, --no-mock-clean\n\n\nEnable/disable deletion of the mock buildroot after a successful build.\n\n\nDefault is \ntrue\n.\n\n\n-m, --mock-config \npath\n\n\nSpecifies the \nmock\n configuration file to use. This file details how to set up the build environment used by mock for the build, including Yum repositories from which to install dependencies and certain predefined variables (e.g., the distribution tag \n%dist\n).\n\n\nSee also \n--mock-config-from-koji\n.\n\n\n--mock-config-from-koji \nbuild tag\n\n\nCreates a mock config from a Koji build tag. This is the most accurate way to replicate the build environment that Koji will provide (outside of Koji). The build tag is based on the distro version (el6, el7) and the OSG major version (3.3, 3.4). For 3.4 on el6, it is: \nosg-3.4-el6-build\n Also requires the Koji command-line tools (package \nkoji\n), obtainable from the osg repositories. Since this uses koji, some of the koji-specific options may apply, namely: \n--koji-backend\n, \n--koji-login\n, and \n--koji-wrapper\n.\n\n\nOptions specific to koji task\n\n\n--dry-run\n\n\nDo not actually run koji, merely show the command(s) that will be run. For debugging purposes.\n\n\n--getfiles, --get-files\n\n\nFor scratch builds without \n--vcs\n only. Download the resulting RPMs and logs from the build into the \n_build_results\n directory.\n\n\n-k, --kojilogin, --koji-login \nlogin\n\n\nSets the login to use for the koji task. This should most likely be your CN. If not specified, will extract it from your client cert (\n~/.osg-koji/client.crt\n or \n~/.koji/client.crt\n).\n\n\n--koji-target \ntarget\n\n\nThe koji target to use for building.\n\n\nDefault is \nosg-el6\n for el6 and \nosg-el7\n for el7.\n\n\n--koji-tag \ntag\n\n\nThe koji tag to add packages to. See the \nKoji Workflow guide\n for more information on the terminology. The special value \nTARGET\n uses the destination tag defined in the koji target.\n\n\nDefault is \nosg-el6\n or \nosg-el7\n.\n\n\n--ktt, --koji-tag-and-target \narg\n\n\nShorthand for setting both --koji-tag and --koji-target to \narg\n.\n\n\n--koji-wrapper, --no-koji-wrapper\n\n\nEnable/disable use of the \nosg-koji\n wrapper script around koji. See below for a description of \nosg-koji\n.\n\n\nDefault is \ntrue\n.\n\n\n--koji-backend \nbackend\n\n\nSpecifies the method osg-build will use to interface with Koji. This can be \nshell\n or \nkojilib\n.\n\n\n--wait, --no-wait, --nowait\n\n\nWait for koji tasks to finish. Bad for running multiple builds in a single command, since you will have to type in your passphrase for the first one, wait for it to complete, then type in your passphrase for the second one, wait for it to complete, etc. If you want to wait for multiple tasks to finish, use the \nkoji watch-task\n command or look at the website \nhttps://koji.chtc.wisc.edu\n.\n\n\n--wait\n used to be the default until \nosg-build-1.1.3\n\n\n--regen-repos\n\n\nStart a \nregen-repo\n koji task on the build tag after each koji build, to update the build repository used for the next build. Not useful unless you are launching multiple builds. This enables you to launch builds that depend on each other. Doesn't work too well with \n--no-wait\n, since the next build may be started before the regen-repo task is complete. Waiting will keep the next build from being queued until the regen-repo is complete.\n\n\n--scratch, --no-scratch\n\n\nPerform scratch builds. A scratch build does not go into a repository, but the name-version-release (NVR) of the created RPMs are not considered used, so the build may be modified and repeated without needing a release bump. This has the same use case as the mock task: creating packages that you want to test before releasing. If you do not have a machine with mock set up, or want to test exactly the environment that Koji provides, scratch builds might be more convenient.\n\n\n--vcs, --no-vcs, --svn, --no-svn\n\n\nHave Koji check the package out from a version control system instead of creating an SRPM on the local machine and submitting that to Koji. Currently, SVN and Git are supported. If this flag is specified, you may use SVN URLs or URL@Revision pairs to specify the packages to build. You may continue specify package directories from an SVN checkout, in which case osg-build will use \nsvn info\n to find the right URL@Revision pair to use and warn you about uncommitted changes. osg-build will also warn you about an outdated working directory.\n\n\n--vcs\n defaults to \ntrue\n for non-scratch builds, and \nfalse\n for scratch builds.\n\n\n--repo=\ndestination repository\n, --upcoming\n\n\nSelects the repositories (osg-3.3, upcoming, etc.) to build packages for. Currently valid repositories are:\n\n\n\n\n\n\n\n\nRepository\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nosg\n\n\nOSG Software development repos for trunk (this is the default)\n\n\n\n\n\n\nosg-3.3 (or just 3.3)\n\n\nOSG Software development repos for 3.2 branch\n\n\n\n\n\n\nupcoming\n\n\nOSG Software development repos for upcoming branch\n\n\n\n\n\n\ninternal\n\n\nOSG Software internal branch\n\n\n\n\n\n\nhcc\n\n\nHolland Computing Center (Nebraska) testing repos\n\n\n\n\n\n\n\n\n--upcoming\n is an alias for \n--repo=upcoming\n\n\nNote that the repo selection affects which VCS paths you are allowed to build from. For example, you are not allowed to build from branches/osg-3.3 (from the OSG SVN) into the 'osg' repo, or from HCC's git repositories into the 'upcoming' repo.\n\n\nkoji-tag-diff\n\n\nThis script displays the differences between the latest packages in two koji tags.\n\n\nExample invocation: \nkoji-tag-diff osg-3.4-el6-development osg-3.4-el7-testing\n\n\nThis prints the packages that are in osg-3.4-el6-development but not in osg-3.4-el7-testing, or vice versa.\n\n\nosg-build-test\n\n\nThis script runs automated tests for \nosg-build\n. Only a few tests have been implemented so far.\n\n\nosg-import-srpm\n\n\nThis is a script to fetch an SRPM from a remote site, copy it into the upstream cache on AFS, and create an SVN package dir (if needed) with an \nupstream/*.source\n file. By default it will put downloaded files into the VDT upstream cache (/p/vdt/public/html/upstream), but you can pass \n--upstream-root=\nUPSTREAM DIR\n to put them somewhere else. If called with the \n--extract-spec\n or \n-e\n argument, it will extract the spec file from the SRPM and place it into the \nosg\n subdir in SVN. If called with the \n--diff-spec\n or \n-d\n argument, it will extract the spec file and compare it to the existing spec file in the \nosg\n subdir. \nThe script hasn't been touched in a while and needs a good deal of cleanup.\n A planned feature is to allow doing a three-way diff between the existing RPM before OSG modifications, the new RPM before OSG modifications and the existing RPM after OSG modifications.\n\n\nosg-koji\n\n\nThis is a wrapper script around the \nkoji\n command line tool. It automatically specifies parameters to access the OSG's koji instance, and forces SSL authentication. It takes the same parameters as \nkoji\n and passes them on.\n\n\nAn additional command, \nosg-koji setup\n exists, which performs the following tasks:\n\n\n\n\nCreate a koji configuration in \n~/.osg-koji\n\n\nCreate a CA bundle for verifying the server.\n    Use either files in \n/etc/grid-security/certificates\n, or (if those are not found), from files downloaded from the DOEGrids and DigiCert sites.\n\n\nCreate a client cert file. This can be a symlink to your grid proxy, or it can be a file created from your grid public and private key files.\n    The location of those files can be specified by the \n--usercert\n and \n--userkey\n arguments.\n    If unspecified, \nusercert\n defaults to \n~/.globus/usercert.pem\n, and \nuserkey\n defaults to \n~/.globus/userkey.pem\n.\n\n\n\n\nosg-promote\n\n\nOverview\n\n\nRun this script to push packages from one set of repos to another (e.g. from development to testing), according to the OSG software promotion guidelines.\n\n\nOnce the packages are promoted, the script will generate code to cut and paste into a JIRA comment.\n\n\nSynopsis\n\n\n\n\nosg-promote [-r|--route \nROUTE\n]... [options] \nPACKAGE OR BUILD\n [...]\n\n\n\n\nExamples\n\n\n\n\n\n\nPromote the latest build of \nosg-version\n to testing for the current release series  \n\n\nosg-promote -r testing osg-version\n\n\n\n\n\n\n\n\n\nPromote the latest builds of \nosg-ce\n to testing for the 3.3 and 3.4 release series  \n\n\nosg-promote -r 3.3-testing -r 3.4-testing osg-ce\n\n\n\n\n\n\n\n\n\nPromote \nosg-build-1.5.0-1\n to testing for the current release series  \n\n\nosg-promote -r testing osg-build-1.5.0-1\n\n\n\n\n\n\n\n\n\nArguments\n\n\n-h\n\n\nDisplay help and a list of valid routes.\n\n\npackage or build\n\n\nA package (e.g. \nosg-version\n) or build (e.g. \nosg-version-3.3.0-1.osg33.el6\n) to promote. You may omit the dist tag (the \n.osg33.el6\n part).\n\n\nIf a package is specified, the most recent version of that package will be promoted.\n\n\nIf a build is specified, that build and the build that has the same \nversion\n-\nrelease\n for the other distro version(s) will be promoted. That is, if you specify the route \n3.3-testing\n and the build \nfoo-1-1\n, then \nfoo-1-1.osg33.el6\n and \nfoo-1-1.osg33.el7\n will be promoted.\n\n\nThis may be specified multiple times, to promote multiple packages. The NVRs of each set of builds for a package \nmust\n match.\n\n\n-r \nROUTE\n, --route \nROUTE\n\n\nThe promotion route to use. Use \nosg-promote -h\n to get a list of valid routes. This may be specified multiple times. For example, to promote for both 3.4 and 3.3, pass \n-r 3.4-testing -r 3.3-testing\n.\n\n\nIf not specified, the \ntesting\n route is used, which corresponds to the testing route for the latest release series.\n\n\n-n, --dry-run\n\n\nDo not promote, just show what would be done.\n\n\n--el6-only / --el7-only\n\n\nOnly promote packages for el6 / el7.\n\n\n--no-el6 / --no-el7\n\n\nDo not promote packages for el6 / el7.\n\n\n--ignore-rejects\n\n\nIgnore rejections due to version mismatch between dvers or missing package for one dver.\n\n\n--regen\n\n\nRegenerate the destination repos after promoting.\n\n\n-y, --assume-yes\n\n\nDo not prompt before promotion.\n\n\nCommon Usage Patterns\n\n\nVerify that all files necessary to build the package are in the right place\n\n\nRun \nosg-build prebuild \nPACKAGEDIR\n.\n\n\nFetch and extract all source files for examination\n\n\nRun \nosg-build prebuild --full-extract \nPACKAGEDIR\n. Look inside the \n_upstream_tarball_contents\n directory.\n\n\nGet a post-patch version of the upstream sources for examination\n\n\nRun \nosg-build prepare \nPACKAGEDIR\n. Look inside the \n_build_results\n directory.\n\n\nSee which patches work with a new version of a package, update or remove them\n\n\n\n\nPlace the new source tarball into the upstream cache, edit the version in the spec file and *.sources files as necessary\n\n\nRun \nosg-build quilt \nPACKAGEDIR\n.\n\n\nEnter the extracted sources inside the \n_final_srpm_contents\n directory. You should see a file called \nseries\n and a symlink called \npatches\n.\n\n\nType \nquilt series\n to get a list of patches in order of application.\n\n\nType \nquilt push\n to apply the next patch.\n\n\nIf the patch applies cleanly, continue.\n\n\nIf the patch applies with some fuzz, type \nquilt refresh\n to update the offsets in the patch.\n\n\nIf the patch does not apply and you wish to remove it, type \nquilt delete \nPATCH NAME\n (delete only removes it from the series file, not the disk)\n\n\nIf the patch does not apply and you wish to fix it, either type \nquilt push -f\n to interactively apply the patch, or \nquilt delete \nPATCH NAME\n the patch and use \nquilt new\n / \nquilt edit\n / \nquilt refresh\n to edit files and make a new patch from your changes. Consult the \nquilt(1)\n manpage for more info.\n\n\n\n\n\n\nIf you have a new patch, run \nquilt import \nPATCHFILE\n to add the patch to the series file, and run \nquilt push\n to apply it.\n\n\nIf you have changes to make to the source code that you want to save as a patch, type \nquilt new \nPATCHNAME\n, edit the files, type \nquilt add \nFILE\n on each file you edited, then type \nquilt refresh\n to recreate the patch.\n\n\nOnce you're all done, copy the patches in the \npatches/\n directory to the \nosg/\n dir in SVN, run \nquilt series\n to get the application order and update the spec file accordingly.\n\n\n\n\nSee if a package builds successfully for OSG 3.4\n\n\n\n\nIf you have all the build dependencies of the package installed, run \nosg-build rpmbuild \nPACKAGEDIR\n. The resulting RPMs will be in the \n_build_results\n directory.\n\n\nIf you do not have all the build dependencies installed, or want to make sure you specified all of the necessary ones and the package builds from a clean environment, run \nosg-build mock --mock-config-from-koji osg-3.4-el6-build \nPACKAGEDIR\n. The resulting RPMs will be in the \n_build_results\n directory.\n\n\nIf you do not have mock installed, or want to exactly replicate the build environment in Koji, run \nosg-build koji --scratch \nPACKAGEDIR\n. You may download the resulting RPMs from kojiweb \nhttps://koji.chtc.wisc.edu/koji\n or pass \n--getfiles\n to \nosg-build koji\n and they will get downloaded to the \n_build_results\n directory.\n\n\n\n\nCheck for potential errors in a package\n\n\nRun \nosg-build lint \nPACKAGEDIR\n.\n\n\nCreate and test a final build of a package for all platforms for upcoming\n\n\n\n\nsvn commit\n your changes in \nbranches/upcoming\n.\n\n\nType \nosg-build koji --repo=upcoming \nPACKAGEDIR\n\n\nWait for the \nosg-upcoming-minefield\n repos to be regenerated containing the new version of your package. You can run \nosg-koji wait-repo osg-upcoming-el\nX\n-development --build=\nPACKAGENAME-VERSION-RELEASE\n and wait for that process to finish (substitute \n6\n or \n7\n for \nX\n). Or, you can just check kojiweb \nhttps://koji.chtc.wisc.edu/koji/tasks\n.\n\n\nOn your test machine, make sure the \nosg-upcoming-minefield\n repo is enabled (edit \n/etc/yum.repos.d/osg-upcoming-minefield.repo\n or \n/etc/yum.repos.d/osg-el6-upcoming-minefield.repo\n). Clean your cache (\nyum clean all; yum clean expire-cache\n).\n\n\nInstall your software, see if it works.\n\n\n\n\nPromote the latest build of a package to testing for the current OSG release series\n\n\nRun \nosg-promote -r testing \nPACKAGE\n\n\nPromote the latest build of a package to testing for the 3.3 and 3.4 release series\n\n\nRun \nosg-promote -r 3.3-testing -r 3.4-testing \nPACKAGE", 
            "title": "OSG Build Tools"
        }, 
        {
            "location": "/software/osg-build-tools/#osg-build-tools", 
            "text": "This page documents the tools used for RPM development for the OSG Software Stack. See  the RPM development guide  for the principles on which these tools are based.  The tools are distributed in the  osg-build  RPM in our repositories, but can also be used from a Git clone of  opensciencegrid/osg-build on GitHub .  This page is up-to-date as of  osg-build  version 1.10.1.", 
            "title": "OSG Build Tools"
        }, 
        {
            "location": "/software/osg-build-tools/#the-tools", 
            "text": "", 
            "title": "The tools"
        }, 
        {
            "location": "/software/osg-build-tools/#osg-build", 
            "text": "", 
            "title": "osg-build"
        }, 
        {
            "location": "/software/osg-build-tools/#overview", 
            "text": "This is the primary tool used in building source and binary RPMs.   osg-build  TASK  [options]  PACKAGE DIRECTORY  [...]   package_directory  is a directory containing an  osg/  and/or an  upstream/  subdirectory. See  the RPM development guide  for how these directories are organized.", 
            "title": "Overview"
        }, 
        {
            "location": "/software/osg-build-tools/#tasks", 
            "text": "", 
            "title": "Tasks"
        }, 
        {
            "location": "/software/osg-build-tools/#koji", 
            "text": "Prebuilds the final source package, then builds it remotely using the Koji instance hosted at UW-Madison.  https://koji.chtc.wisc.edu  By default, the resulting RPMs will end up in the osg-minefield repositories based on the most recent OSG major version (e.g. 3.4). You may specify a different set of repos with  --repo , described later. RPMs from the osg-minefield repositories are regularly pulled to the osg-development repositories hosted by the GOC at  http://repo.grid.iu.edu  Unless you specify otherwise (by passing  --el6 ,  --el7  or specifying a different koji tag/target), the package will be built for both el6 and el7. This is the method used to build final versions of packages you expect to ship.", 
            "title": "koji"
        }, 
        {
            "location": "/software/osg-build-tools/#lint", 
            "text": "Prebuilds the final source package, then runs  rpmlint  on it to check for various problems. You will need to have  rpmlint  installed. People on UW CSL machines should add  /p/vdt/workspace/rpmlint  to their $PATH.", 
            "title": "lint"
        }, 
        {
            "location": "/software/osg-build-tools/#mock", 
            "text": "Prebuilds the final source package, then builds it locally using  mock , and stores the resulting source and binary RPMs in the package-specific  _build_results  directory.", 
            "title": "mock"
        }, 
        {
            "location": "/software/osg-build-tools/#prebuild", 
            "text": "Prebuilds the final source package from upstream sources (if any) and local files (if any). May create or overwrite the  _upstream_srpm_contents  and  _final_srpm_contents  directories.", 
            "title": "prebuild"
        }, 
        {
            "location": "/software/osg-build-tools/#prepare", 
            "text": "Prebuilds the final source package, then calls  rpmbuild -bp  on the result, extracting and patching the source files (and performing any other steps defined in the  %prep  section of the spec file. The resulting sources will be under  _final_srpm_contents .", 
            "title": "prepare"
        }, 
        {
            "location": "/software/osg-build-tools/#rpmbuild", 
            "text": "Prebuilds the final source package, then builds it locally using  rpmbuild , and stores the resulting source and binary RPMs in the package-specific  _build_results  directory.", 
            "title": "rpmbuild"
        }, 
        {
            "location": "/software/osg-build-tools/#quilt", 
            "text": "Collects the upstream local sources and spec file, then calls  quilt setup  on the spec file, extracting the source files and adding the patches to a quilt series file. See  Quilt documentation (PDF link)  for more information on quilt; also look at the example in the Usage Patterns section below. Similar to  prepare  (in fact,  quilt  calls  rpmbuild -bp  behind the scenes), but the source tree is in pre-patch state, and various quilt commands can be used to apply and modify patches. Unpacks into  _quilt  as of  osg-build-1.2.2  or  _final_srpm_contents  in previous versions. Requires  quilt . People on UW CSL machines should add  /p/vdt/workspace/quilt/bin  to their  $PATH , and  /p/vdt/workspace/quilt/share/man  to their  $MANPATH .", 
            "title": "quilt"
        }, 
        {
            "location": "/software/osg-build-tools/#options", 
            "text": "This section lists the command-line options.", 
            "title": "Options"
        }, 
        {
            "location": "/software/osg-build-tools/#-help", 
            "text": "Prints the built-in usage information and exits without doing anything else.", 
            "title": "--help"
        }, 
        {
            "location": "/software/osg-build-tools/#-version", 
            "text": "Prints the version of  osg-build  and exits without doing anything else.", 
            "title": "--version"
        }, 
        {
            "location": "/software/osg-build-tools/#common-options", 
            "text": "", 
            "title": "Common Options"
        }, 
        {
            "location": "/software/osg-build-tools/#-a-autoclean-no-autoclean", 
            "text": "Before each build, clean out the contents of the underscore directories (_build_results, _final_srpm_contents, _upstream_srpm_contents, _upstream_tarball_contents). If the directories are not cleaned up, earlier builds of a package may interfere with later ones.  --no-autoclean  will disable this.  Default is  true .  Has no effect with the  --vcs  flag.", 
            "title": "-a, --autoclean, --no-autoclean"
        }, 
        {
            "location": "/software/osg-build-tools/#-c-cache-prefix-prefix", 
            "text": "Sets the  prefix  for upstream source cache references. The prefix must be a valid URI starting with either  http ,  https , or  file , or one of the following special values:   AFS (corresponds to  file:///p/vdt/public/html/upstream , which is the location of the VDT cache using AFS from a UW CS machine).  VDT (corresponds to  http://vdt.cs.wisc.edu/upstream , which is the location of the VDT cache from off-site).  AUTO (AFS if available, VDT if not)   The upstream source cache must be organized as described above. All files referenced by  .source  files in the affected packages must exist in the cache, or a runtime error will occur.  Default is  AUTO .  Has no effect with the  --vcs  flag.", 
            "title": "-c, --cache-prefix prefix"
        }, 
        {
            "location": "/software/osg-build-tools/#-el6-el7-redhat-release-version-config-redhat95release", 
            "text": "Sets the distro version to build for. This affects the %dist tag, the mock config, and the default koji tag and target (unless otherwise specified).  --el6  is equivalent to  --redhat-release 6  --el7  is equivalent to  --redhat-release 7", 
            "title": "--el6, --el7, --redhat-release version (Config: redhat_release)"
        }, 
        {
            "location": "/software/osg-build-tools/#-loglevel-loglevel", 
            "text": "Sets the verbosity of the script. Valid values are:  debug ,  info ,  warning ,  error  and  critical .  Default is  info .", 
            "title": "--loglevel loglevel"
        }, 
        {
            "location": "/software/osg-build-tools/#-q-quiet", 
            "text": "Do not display as much information. Equivalent to  --loglevel warning", 
            "title": "-q, --quiet"
        }, 
        {
            "location": "/software/osg-build-tools/#-v-verbose", 
            "text": "Display more information. Equivalent to  --loglevel debug", 
            "title": "-v, --verbose"
        }, 
        {
            "location": "/software/osg-build-tools/#-w-working-directory-path", 
            "text": "Use  path  as the root directory of the files created by the script. For example, if  path  is  $HOME/working , and the package being built is  ndt , the following tree will be created:   $HOME/working/ndt/_upstream_srpm_contents  $HOME/working/ndt/_upstream_tarball_contents  $HOME/working/ndt/_final_srpm_contents  $HOME/working/ndt/_build_results   If  path  is  TEMP , a randomly named directory under  /tmp  is used as the working directory.  The default setting is to use the package directory as the working directory.  Has no effect with the  --vcs  flag.", 
            "title": "-w, --working-directory path"
        }, 
        {
            "location": "/software/osg-build-tools/#options-specific-to-prebuild-task", 
            "text": "", 
            "title": "Options specific to prebuild task"
        }, 
        {
            "location": "/software/osg-build-tools/#-full-extract", 
            "text": "If set, all upstream tarballs will be extracted into  _upstream_tarball_contents/  during the prebuild step. This flag is now mostly redundant with the  prepare  and  quilt  tasks.", 
            "title": "--full-extract"
        }, 
        {
            "location": "/software/osg-build-tools/#options-specific-to-rpmbuild-and-mock-tasks", 
            "text": "", 
            "title": "Options specific to rpmbuild and mock tasks"
        }, 
        {
            "location": "/software/osg-build-tools/#-distro-tag-dist", 
            "text": "Sets the distribution tag added on to the end of the release in the RPM (  rpmbuild  and  mock  tasks only ).  Default is  .osg.el6  or  .osg.el7", 
            "title": "--distro-tag dist"
        }, 
        {
            "location": "/software/osg-build-tools/#-t-target-arch-arch", 
            "text": "Specify an architecture to build packages for (  rpmbuild  and  mock  tasks only ).  Default is unspecified, which builds for the current machine architecture.", 
            "title": "-t, --target-arch arch"
        }, 
        {
            "location": "/software/osg-build-tools/#options-specific-to-mock-task", 
            "text": "", 
            "title": "Options specific to mock task"
        }, 
        {
            "location": "/software/osg-build-tools/#-mock-clean-no-mock-clean", 
            "text": "Enable/disable deletion of the mock buildroot after a successful build.  Default is  true .", 
            "title": "--mock-clean, --no-mock-clean"
        }, 
        {
            "location": "/software/osg-build-tools/#-m-mock-config-path", 
            "text": "Specifies the  mock  configuration file to use. This file details how to set up the build environment used by mock for the build, including Yum repositories from which to install dependencies and certain predefined variables (e.g., the distribution tag  %dist ).  See also  --mock-config-from-koji .", 
            "title": "-m, --mock-config path"
        }, 
        {
            "location": "/software/osg-build-tools/#-mock-config-from-koji-build-tag", 
            "text": "Creates a mock config from a Koji build tag. This is the most accurate way to replicate the build environment that Koji will provide (outside of Koji). The build tag is based on the distro version (el6, el7) and the OSG major version (3.3, 3.4). For 3.4 on el6, it is:  osg-3.4-el6-build  Also requires the Koji command-line tools (package  koji ), obtainable from the osg repositories. Since this uses koji, some of the koji-specific options may apply, namely:  --koji-backend ,  --koji-login , and  --koji-wrapper .", 
            "title": "--mock-config-from-koji build tag"
        }, 
        {
            "location": "/software/osg-build-tools/#options-specific-to-koji-task", 
            "text": "", 
            "title": "Options specific to koji task"
        }, 
        {
            "location": "/software/osg-build-tools/#-dry-run", 
            "text": "Do not actually run koji, merely show the command(s) that will be run. For debugging purposes.", 
            "title": "--dry-run"
        }, 
        {
            "location": "/software/osg-build-tools/#-getfiles-get-files", 
            "text": "For scratch builds without  --vcs  only. Download the resulting RPMs and logs from the build into the  _build_results  directory.", 
            "title": "--getfiles, --get-files"
        }, 
        {
            "location": "/software/osg-build-tools/#-k-kojilogin-koji-login-login", 
            "text": "Sets the login to use for the koji task. This should most likely be your CN. If not specified, will extract it from your client cert ( ~/.osg-koji/client.crt  or  ~/.koji/client.crt ).", 
            "title": "-k, --kojilogin, --koji-login login"
        }, 
        {
            "location": "/software/osg-build-tools/#-koji-target-target", 
            "text": "The koji target to use for building.  Default is  osg-el6  for el6 and  osg-el7  for el7.", 
            "title": "--koji-target target"
        }, 
        {
            "location": "/software/osg-build-tools/#-koji-tag-tag", 
            "text": "The koji tag to add packages to. See the  Koji Workflow guide  for more information on the terminology. The special value  TARGET  uses the destination tag defined in the koji target.  Default is  osg-el6  or  osg-el7 .", 
            "title": "--koji-tag tag"
        }, 
        {
            "location": "/software/osg-build-tools/#-ktt-koji-tag-and-target-arg", 
            "text": "Shorthand for setting both --koji-tag and --koji-target to  arg .", 
            "title": "--ktt, --koji-tag-and-target arg"
        }, 
        {
            "location": "/software/osg-build-tools/#-koji-wrapper-no-koji-wrapper", 
            "text": "Enable/disable use of the  osg-koji  wrapper script around koji. See below for a description of  osg-koji .  Default is  true .", 
            "title": "--koji-wrapper, --no-koji-wrapper"
        }, 
        {
            "location": "/software/osg-build-tools/#-koji-backend-backend", 
            "text": "Specifies the method osg-build will use to interface with Koji. This can be  shell  or  kojilib .", 
            "title": "--koji-backend backend"
        }, 
        {
            "location": "/software/osg-build-tools/#-wait-no-wait-nowait", 
            "text": "Wait for koji tasks to finish. Bad for running multiple builds in a single command, since you will have to type in your passphrase for the first one, wait for it to complete, then type in your passphrase for the second one, wait for it to complete, etc. If you want to wait for multiple tasks to finish, use the  koji watch-task  command or look at the website  https://koji.chtc.wisc.edu .  --wait  used to be the default until  osg-build-1.1.3", 
            "title": "--wait, --no-wait, --nowait"
        }, 
        {
            "location": "/software/osg-build-tools/#-regen-repos", 
            "text": "Start a  regen-repo  koji task on the build tag after each koji build, to update the build repository used for the next build. Not useful unless you are launching multiple builds. This enables you to launch builds that depend on each other. Doesn't work too well with  --no-wait , since the next build may be started before the regen-repo task is complete. Waiting will keep the next build from being queued until the regen-repo is complete.", 
            "title": "--regen-repos"
        }, 
        {
            "location": "/software/osg-build-tools/#-scratch-no-scratch", 
            "text": "Perform scratch builds. A scratch build does not go into a repository, but the name-version-release (NVR) of the created RPMs are not considered used, so the build may be modified and repeated without needing a release bump. This has the same use case as the mock task: creating packages that you want to test before releasing. If you do not have a machine with mock set up, or want to test exactly the environment that Koji provides, scratch builds might be more convenient.", 
            "title": "--scratch, --no-scratch"
        }, 
        {
            "location": "/software/osg-build-tools/#-vcs-no-vcs-svn-no-svn", 
            "text": "Have Koji check the package out from a version control system instead of creating an SRPM on the local machine and submitting that to Koji. Currently, SVN and Git are supported. If this flag is specified, you may use SVN URLs or URL@Revision pairs to specify the packages to build. You may continue specify package directories from an SVN checkout, in which case osg-build will use  svn info  to find the right URL@Revision pair to use and warn you about uncommitted changes. osg-build will also warn you about an outdated working directory.  --vcs  defaults to  true  for non-scratch builds, and  false  for scratch builds.", 
            "title": "--vcs, --no-vcs, --svn, --no-svn"
        }, 
        {
            "location": "/software/osg-build-tools/#-repodestination-repository-upcoming", 
            "text": "Selects the repositories (osg-3.3, upcoming, etc.) to build packages for. Currently valid repositories are:     Repository  Description      osg  OSG Software development repos for trunk (this is the default)    osg-3.3 (or just 3.3)  OSG Software development repos for 3.2 branch    upcoming  OSG Software development repos for upcoming branch    internal  OSG Software internal branch    hcc  Holland Computing Center (Nebraska) testing repos     --upcoming  is an alias for  --repo=upcoming  Note that the repo selection affects which VCS paths you are allowed to build from. For example, you are not allowed to build from branches/osg-3.3 (from the OSG SVN) into the 'osg' repo, or from HCC's git repositories into the 'upcoming' repo.", 
            "title": "--repo=destination repository, --upcoming"
        }, 
        {
            "location": "/software/osg-build-tools/#koji-tag-diff", 
            "text": "This script displays the differences between the latest packages in two koji tags.  Example invocation:  koji-tag-diff osg-3.4-el6-development osg-3.4-el7-testing  This prints the packages that are in osg-3.4-el6-development but not in osg-3.4-el7-testing, or vice versa.", 
            "title": "koji-tag-diff"
        }, 
        {
            "location": "/software/osg-build-tools/#osg-build-test", 
            "text": "This script runs automated tests for  osg-build . Only a few tests have been implemented so far.", 
            "title": "osg-build-test"
        }, 
        {
            "location": "/software/osg-build-tools/#osg-import-srpm", 
            "text": "This is a script to fetch an SRPM from a remote site, copy it into the upstream cache on AFS, and create an SVN package dir (if needed) with an  upstream/*.source  file. By default it will put downloaded files into the VDT upstream cache (/p/vdt/public/html/upstream), but you can pass  --upstream-root= UPSTREAM DIR  to put them somewhere else. If called with the  --extract-spec  or  -e  argument, it will extract the spec file from the SRPM and place it into the  osg  subdir in SVN. If called with the  --diff-spec  or  -d  argument, it will extract the spec file and compare it to the existing spec file in the  osg  subdir.  The script hasn't been touched in a while and needs a good deal of cleanup.  A planned feature is to allow doing a three-way diff between the existing RPM before OSG modifications, the new RPM before OSG modifications and the existing RPM after OSG modifications.", 
            "title": "osg-import-srpm"
        }, 
        {
            "location": "/software/osg-build-tools/#osg-koji", 
            "text": "This is a wrapper script around the  koji  command line tool. It automatically specifies parameters to access the OSG's koji instance, and forces SSL authentication. It takes the same parameters as  koji  and passes them on.  An additional command,  osg-koji setup  exists, which performs the following tasks:   Create a koji configuration in  ~/.osg-koji  Create a CA bundle for verifying the server.\n    Use either files in  /etc/grid-security/certificates , or (if those are not found), from files downloaded from the DOEGrids and DigiCert sites.  Create a client cert file. This can be a symlink to your grid proxy, or it can be a file created from your grid public and private key files.\n    The location of those files can be specified by the  --usercert  and  --userkey  arguments.\n    If unspecified,  usercert  defaults to  ~/.globus/usercert.pem , and  userkey  defaults to  ~/.globus/userkey.pem .", 
            "title": "osg-koji"
        }, 
        {
            "location": "/software/osg-build-tools/#osg-promote", 
            "text": "", 
            "title": "osg-promote"
        }, 
        {
            "location": "/software/osg-build-tools/#overview_1", 
            "text": "Run this script to push packages from one set of repos to another (e.g. from development to testing), according to the OSG software promotion guidelines.  Once the packages are promoted, the script will generate code to cut and paste into a JIRA comment.", 
            "title": "Overview"
        }, 
        {
            "location": "/software/osg-build-tools/#synopsis", 
            "text": "osg-promote [-r|--route  ROUTE ]... [options]  PACKAGE OR BUILD  [...]", 
            "title": "Synopsis"
        }, 
        {
            "location": "/software/osg-build-tools/#examples", 
            "text": "Promote the latest build of  osg-version  to testing for the current release series    osg-promote -r testing osg-version    Promote the latest builds of  osg-ce  to testing for the 3.3 and 3.4 release series    osg-promote -r 3.3-testing -r 3.4-testing osg-ce    Promote  osg-build-1.5.0-1  to testing for the current release series    osg-promote -r testing osg-build-1.5.0-1", 
            "title": "Examples"
        }, 
        {
            "location": "/software/osg-build-tools/#arguments", 
            "text": "", 
            "title": "Arguments"
        }, 
        {
            "location": "/software/osg-build-tools/#-h", 
            "text": "Display help and a list of valid routes.", 
            "title": "-h"
        }, 
        {
            "location": "/software/osg-build-tools/#package-or-build", 
            "text": "A package (e.g.  osg-version ) or build (e.g.  osg-version-3.3.0-1.osg33.el6 ) to promote. You may omit the dist tag (the  .osg33.el6  part).  If a package is specified, the most recent version of that package will be promoted.  If a build is specified, that build and the build that has the same  version - release  for the other distro version(s) will be promoted. That is, if you specify the route  3.3-testing  and the build  foo-1-1 , then  foo-1-1.osg33.el6  and  foo-1-1.osg33.el7  will be promoted.  This may be specified multiple times, to promote multiple packages. The NVRs of each set of builds for a package  must  match.", 
            "title": "package or build"
        }, 
        {
            "location": "/software/osg-build-tools/#-r-route-route-route", 
            "text": "The promotion route to use. Use  osg-promote -h  to get a list of valid routes. This may be specified multiple times. For example, to promote for both 3.4 and 3.3, pass  -r 3.4-testing -r 3.3-testing .  If not specified, the  testing  route is used, which corresponds to the testing route for the latest release series.", 
            "title": "-r ROUTE, --route ROUTE"
        }, 
        {
            "location": "/software/osg-build-tools/#-n-dry-run", 
            "text": "Do not promote, just show what would be done.", 
            "title": "-n, --dry-run"
        }, 
        {
            "location": "/software/osg-build-tools/#-el6-only-el7-only", 
            "text": "Only promote packages for el6 / el7.", 
            "title": "--el6-only / --el7-only"
        }, 
        {
            "location": "/software/osg-build-tools/#-no-el6-no-el7", 
            "text": "Do not promote packages for el6 / el7.", 
            "title": "--no-el6 / --no-el7"
        }, 
        {
            "location": "/software/osg-build-tools/#-ignore-rejects", 
            "text": "Ignore rejections due to version mismatch between dvers or missing package for one dver.", 
            "title": "--ignore-rejects"
        }, 
        {
            "location": "/software/osg-build-tools/#-regen", 
            "text": "Regenerate the destination repos after promoting.", 
            "title": "--regen"
        }, 
        {
            "location": "/software/osg-build-tools/#-y-assume-yes", 
            "text": "Do not prompt before promotion.", 
            "title": "-y, --assume-yes"
        }, 
        {
            "location": "/software/osg-build-tools/#common-usage-patterns", 
            "text": "", 
            "title": "Common Usage Patterns"
        }, 
        {
            "location": "/software/osg-build-tools/#verify-that-all-files-necessary-to-build-the-package-are-in-the-right-place", 
            "text": "Run  osg-build prebuild  PACKAGEDIR .", 
            "title": "Verify that all files necessary to build the package are in the right place"
        }, 
        {
            "location": "/software/osg-build-tools/#fetch-and-extract-all-source-files-for-examination", 
            "text": "Run  osg-build prebuild --full-extract  PACKAGEDIR . Look inside the  _upstream_tarball_contents  directory.", 
            "title": "Fetch and extract all source files for examination"
        }, 
        {
            "location": "/software/osg-build-tools/#get-a-post-patch-version-of-the-upstream-sources-for-examination", 
            "text": "Run  osg-build prepare  PACKAGEDIR . Look inside the  _build_results  directory.", 
            "title": "Get a post-patch version of the upstream sources for examination"
        }, 
        {
            "location": "/software/osg-build-tools/#see-which-patches-work-with-a-new-version-of-a-package-update-or-remove-them", 
            "text": "Place the new source tarball into the upstream cache, edit the version in the spec file and *.sources files as necessary  Run  osg-build quilt  PACKAGEDIR .  Enter the extracted sources inside the  _final_srpm_contents  directory. You should see a file called  series  and a symlink called  patches .  Type  quilt series  to get a list of patches in order of application.  Type  quilt push  to apply the next patch.  If the patch applies cleanly, continue.  If the patch applies with some fuzz, type  quilt refresh  to update the offsets in the patch.  If the patch does not apply and you wish to remove it, type  quilt delete  PATCH NAME  (delete only removes it from the series file, not the disk)  If the patch does not apply and you wish to fix it, either type  quilt push -f  to interactively apply the patch, or  quilt delete  PATCH NAME  the patch and use  quilt new  /  quilt edit  /  quilt refresh  to edit files and make a new patch from your changes. Consult the  quilt(1)  manpage for more info.    If you have a new patch, run  quilt import  PATCHFILE  to add the patch to the series file, and run  quilt push  to apply it.  If you have changes to make to the source code that you want to save as a patch, type  quilt new  PATCHNAME , edit the files, type  quilt add  FILE  on each file you edited, then type  quilt refresh  to recreate the patch.  Once you're all done, copy the patches in the  patches/  directory to the  osg/  dir in SVN, run  quilt series  to get the application order and update the spec file accordingly.", 
            "title": "See which patches work with a new version of a package, update or remove them"
        }, 
        {
            "location": "/software/osg-build-tools/#see-if-a-package-builds-successfully-for-osg-34", 
            "text": "If you have all the build dependencies of the package installed, run  osg-build rpmbuild  PACKAGEDIR . The resulting RPMs will be in the  _build_results  directory.  If you do not have all the build dependencies installed, or want to make sure you specified all of the necessary ones and the package builds from a clean environment, run  osg-build mock --mock-config-from-koji osg-3.4-el6-build  PACKAGEDIR . The resulting RPMs will be in the  _build_results  directory.  If you do not have mock installed, or want to exactly replicate the build environment in Koji, run  osg-build koji --scratch  PACKAGEDIR . You may download the resulting RPMs from kojiweb  https://koji.chtc.wisc.edu/koji  or pass  --getfiles  to  osg-build koji  and they will get downloaded to the  _build_results  directory.", 
            "title": "See if a package builds successfully for OSG 3.4"
        }, 
        {
            "location": "/software/osg-build-tools/#check-for-potential-errors-in-a-package", 
            "text": "Run  osg-build lint  PACKAGEDIR .", 
            "title": "Check for potential errors in a package"
        }, 
        {
            "location": "/software/osg-build-tools/#create-and-test-a-final-build-of-a-package-for-all-platforms-for-upcoming", 
            "text": "svn commit  your changes in  branches/upcoming .  Type  osg-build koji --repo=upcoming  PACKAGEDIR  Wait for the  osg-upcoming-minefield  repos to be regenerated containing the new version of your package. You can run  osg-koji wait-repo osg-upcoming-el X -development --build= PACKAGENAME-VERSION-RELEASE  and wait for that process to finish (substitute  6  or  7  for  X ). Or, you can just check kojiweb  https://koji.chtc.wisc.edu/koji/tasks .  On your test machine, make sure the  osg-upcoming-minefield  repo is enabled (edit  /etc/yum.repos.d/osg-upcoming-minefield.repo  or  /etc/yum.repos.d/osg-el6-upcoming-minefield.repo ). Clean your cache ( yum clean all; yum clean expire-cache ).  Install your software, see if it works.", 
            "title": "Create and test a final build of a package for all platforms for upcoming"
        }, 
        {
            "location": "/software/osg-build-tools/#promote-the-latest-build-of-a-package-to-testing-for-the-current-osg-release-series", 
            "text": "Run  osg-promote -r testing  PACKAGE", 
            "title": "Promote the latest build of a package to testing for the current OSG release series"
        }, 
        {
            "location": "/software/osg-build-tools/#promote-the-latest-build-of-a-package-to-testing-for-the-33-and-34-release-series", 
            "text": "Run  osg-promote -r 3.3-testing -r 3.4-testing  PACKAGE", 
            "title": "Promote the latest build of a package to testing for the 3.3 and 3.4 release series"
        }, 
        {
            "location": "/software/resurrecting-epel-packages/", 
            "text": "Resurrecting EPEL RPMs\n\n\nYou will need to be a Koji admin to do these steps. \n\n\n[user@client ~] $\n osg-koji --list-permissions --mine\n\n\n\n\n\nWill tell you if you're an admin or not. Current Koji admins are the Madison team and Brian Bockelman.\n\n\n\n\n\n\n\n\nEPEL version\n\n\nEPEL Koji tag\n\n\nOur Koji tag\n\n\n\n\n\n\n\n\n\n\n5\n\n\ndist-5E-epel\n\n\nepelrescue-el5\n\n\n\n\n\n\n6\n\n\ndist-6E-epel\n\n\nepelrescue-el6\n\n\n\n\n\n\n7\n\n\nepel7\n\n\nepelrescue-el7\n\n\n\n\n\n\n\n\n\n\n\n\nDetermine the NVR of the build containing the RPM of the package you want.\n\n\nUse the Fedora/EPEL Koji web interface (\nhttps://koji.fedoraproject.org\n) to search for it.\n\n\nYou can use the search box in the upper right to look for packages, builds, or RPMs; it accepts shell wildcards.\n\n\nEPEL builds have .el5, .el6, or .el7 in the dist tag.\n\n\n\n\n\n\nDownload \nall\n RPMs for \nall\n architectures we care about (i386, i486, i586, i686, x86_64, noarch), including the .src.rpm and the debuginfo rpms.\n\n\nYou have three options for the downloads:\n\n\n\n\nUse the links in the web interface\n\n\nUse the koji command-line interface against the Fedora koji:\n\n\nDownload \nfedora-koji.conf\n, attached to this page\n\n\nRun \nkoji --noauth -c fedora-koji.conf download-build --debuginfo \nBUILD_NVR\n\n\nDelete RPMs for architectures we do not care about (see list above)\n\n\n\n\n\n\nDig around in \nhttps://kojipkgs.fedoraproject.org/packages/\n\n\n\n\n\n\n\n\nOn your development machine:\n\n\n\n\n\n\nImportant:\n Verify that all of the RPMs are signed:\n\n\n[root@client ~] #\n rpm -K *.rpm \n|\n grep -iv gpg\n\n\n\n\n\nshould be empty\n\n\nIf not, \nSTOP\n and sign them using the OSG RPM key -- talk to Mat\n\n\n\n\n\n\nImport the RPMs themselves into the Koji system\n\n\n[user@client ~] $\n osg-koji import \nRPM_DIRECTORY\n/*.rpm\n\n\n\n\n\nThey will not be in any tags at this point\n\n\n\n\n\n\nAdd the package to the whitelist for our koji tag:\n\n\n[user@client ~] $\n osg-koji add-pkg \nOUR_KOJI_TAG\n \nPACKAGE\n --owner\n=\nYOUR_KOJI_USERNAME\n\n\n\n\n\n\n\n\n\n\nActually tag the builds:\n\n\n[user@client ~] $\n osg-koji tag-pkg \nOUR_KOJI_TAG\n \nBUILD\n\n\n\n\n\n\n\n\n\n\nCheck the Tasks tab in Koji to see if kojira has started regening the repos -- it might take a few minutes to kick in.\n\n\nIf it doesn't, do it manually (if you're doing multiple packages, save this step until you're done with all of them):\n\n\nfor repo in osg-{3.1,3.2,3.3,upcoming}-el5-{build,development,testing,release,prerelease,release-build}; do\n   osg-koji regen-repo --nowait $repo\ndone\n\n\n\n\n\n\n\n\n\nMake a test VM and install the package from minefield to test that it is actually present.\n\n\n\n\nUpdate the epelrescue RPMs table below\n\n\n\n\nRemoving resurrected RPMs\n\n\nIn case the RPM appeared back in EPEL, or we no longer need it, here's how to remove it from the epelrescue tags so we're not overriding the EPEL version:\n\n\n\n\n\n\nFind out the NVR of the build:\n\n\n[user@client ~] $\n osg-koji list-tagged \nOUR_KOJI_TAG\n \nPACKAGE\n\n\n\n\n\n\n\n\n\n\nUntag the packages:\n\n\n[user@client ~] $\n osg-koji untag-pkg \nOUR_KOJI_TAG\n \nBUILD\n\n\n\n\n\n\n\n\n\n\nWhy you should not use block-pkg\n\n\nEPEL removes their packages by using 'koji block-pkg', which leaves the package and the builds in the tag, but prevents it from appearing in the repos. We cannot do that, because blocks are inherited and this will mess up our build repos. This is what happened in one case:\n\n\n\n\nEPEL removed rpmdevtools, which is a necessary package for all builds. I resurrected it into epelrescue-el5.\n\n\nLater, EPEL put rpmdevtools back into their repos, so it no longer needed to be in epelrescue-el5.\n\n\nI used block-pkg on rpmdevtools in epelrescue-el5, thinking that the package could remain tagged, but will stay out of our repos, and the EPEL package would be used instead.\n\n\nThe block not only hid our rpmdevtools, it hid EPEL's rpmdevtools as well, preventing us from being able to build.\n\n\nI unblocked the rpmdevtools, and just untagged the build instead, regenerated our build repos, and we could build again.\n\n\n\n\nPolicy for epelrescue tags\n\n\nhttps://jira.opensciencegrid.org/browse/SOFTWARE-2046\n\n\nTable of epelrescue RPMs\n\n\n\n\n\n\n\n\nPackage\n\n\nDistro version\n\n\nDate added\n\n\nReason added\n\n\nDate removed\n\n\n\n\n\n\n\n\n\n\npython-six-1.7.3-1.el6\n\n\n6\n\n\n2015-08-12\n\n\nDep of osg-build (via mock)\n\n\n2015-10-14\n\n\n\n\n\n\npython-argparse-1.2.1-2.el6\n\n\n6\n\n\n2015-09-23\n\n\nDep of osg-wn-client (via gfal2)\n\n\n2015-10-14\n\n\n\n\n\n\npython-backports-ssl_match_hostname-3.4.0.2-4.el6\n\n\n6\n\n\n2015-09-23\n\n\nDep of osg-build (via mock)\n\n\n2015-10-14\n\n\n\n\n\n\npython-requests-1.1.0-4.el6\n\n\n6\n\n\n2015-09-23\n\n\nDep of osg-build (via mock)\n\n\n2015-10-14\n\n\n\n\n\n\npython-urllib3-1.5-7.el6\n\n\n6\n\n\n2015-09-23\n\n\nDep of osg-build (via mock)\n\n\n2015-10-14\n\n\n\n\n\n\n\n\nFinding out if a package is still needed in epelrescue\n\n\nSet \n$pkg\n to the name of a package to test (e.g. \npython-six\n), and \n$rhel\n set to the RHEL version you're testing for (e.g. \n5\n, \n6\n, or \n7\n).\n\n\nUsing Carl's \ncentos-srpms\n, \nscientific-srpms\n, \nslf-srpms\n scripts:\n\n\nfor script in centos-srpms scientific-srpms slf-srpms; do\n       echo -n $script \n: \n\n       $script -$rhel $pkg | grep . || echo none\ndone\n\n\n\n\n\nA dry run of removing the package:\n\n\nosg-koji untag-pkg -n --all epelrescue-el$rhel $pkg\n\n\n\n\n\nRemove the \n-n\n when the output of that looks fine.", 
            "title": "Resurrecting Epel Packages"
        }, 
        {
            "location": "/software/resurrecting-epel-packages/#resurrecting-epel-rpms", 
            "text": "You will need to be a Koji admin to do these steps.   [user@client ~] $  osg-koji --list-permissions --mine  Will tell you if you're an admin or not. Current Koji admins are the Madison team and Brian Bockelman.     EPEL version  EPEL Koji tag  Our Koji tag      5  dist-5E-epel  epelrescue-el5    6  dist-6E-epel  epelrescue-el6    7  epel7  epelrescue-el7       Determine the NVR of the build containing the RPM of the package you want.  Use the Fedora/EPEL Koji web interface ( https://koji.fedoraproject.org ) to search for it.  You can use the search box in the upper right to look for packages, builds, or RPMs; it accepts shell wildcards.  EPEL builds have .el5, .el6, or .el7 in the dist tag.    Download  all  RPMs for  all  architectures we care about (i386, i486, i586, i686, x86_64, noarch), including the .src.rpm and the debuginfo rpms.  You have three options for the downloads:   Use the links in the web interface  Use the koji command-line interface against the Fedora koji:  Download  fedora-koji.conf , attached to this page  Run  koji --noauth -c fedora-koji.conf download-build --debuginfo  BUILD_NVR  Delete RPMs for architectures we do not care about (see list above)    Dig around in  https://kojipkgs.fedoraproject.org/packages/     On your development machine:    Important:  Verify that all of the RPMs are signed:  [root@client ~] #  rpm -K *.rpm  |  grep -iv gpg  should be empty  If not,  STOP  and sign them using the OSG RPM key -- talk to Mat    Import the RPMs themselves into the Koji system  [user@client ~] $  osg-koji import  RPM_DIRECTORY /*.rpm  They will not be in any tags at this point    Add the package to the whitelist for our koji tag:  [user@client ~] $  osg-koji add-pkg  OUR_KOJI_TAG   PACKAGE  --owner = YOUR_KOJI_USERNAME     Actually tag the builds:  [user@client ~] $  osg-koji tag-pkg  OUR_KOJI_TAG   BUILD     Check the Tasks tab in Koji to see if kojira has started regening the repos -- it might take a few minutes to kick in.  If it doesn't, do it manually (if you're doing multiple packages, save this step until you're done with all of them):  for repo in osg-{3.1,3.2,3.3,upcoming}-el5-{build,development,testing,release,prerelease,release-build}; do\n   osg-koji regen-repo --nowait $repo\ndone    Make a test VM and install the package from minefield to test that it is actually present.   Update the epelrescue RPMs table below", 
            "title": "Resurrecting EPEL RPMs"
        }, 
        {
            "location": "/software/resurrecting-epel-packages/#removing-resurrected-rpms", 
            "text": "In case the RPM appeared back in EPEL, or we no longer need it, here's how to remove it from the epelrescue tags so we're not overriding the EPEL version:    Find out the NVR of the build:  [user@client ~] $  osg-koji list-tagged  OUR_KOJI_TAG   PACKAGE     Untag the packages:  [user@client ~] $  osg-koji untag-pkg  OUR_KOJI_TAG   BUILD", 
            "title": "Removing resurrected RPMs"
        }, 
        {
            "location": "/software/resurrecting-epel-packages/#why-you-should-not-use-block-pkg", 
            "text": "EPEL removes their packages by using 'koji block-pkg', which leaves the package and the builds in the tag, but prevents it from appearing in the repos. We cannot do that, because blocks are inherited and this will mess up our build repos. This is what happened in one case:   EPEL removed rpmdevtools, which is a necessary package for all builds. I resurrected it into epelrescue-el5.  Later, EPEL put rpmdevtools back into their repos, so it no longer needed to be in epelrescue-el5.  I used block-pkg on rpmdevtools in epelrescue-el5, thinking that the package could remain tagged, but will stay out of our repos, and the EPEL package would be used instead.  The block not only hid our rpmdevtools, it hid EPEL's rpmdevtools as well, preventing us from being able to build.  I unblocked the rpmdevtools, and just untagged the build instead, regenerated our build repos, and we could build again.", 
            "title": "Why you should not use block-pkg"
        }, 
        {
            "location": "/software/resurrecting-epel-packages/#policy-for-epelrescue-tags", 
            "text": "https://jira.opensciencegrid.org/browse/SOFTWARE-2046", 
            "title": "Policy for epelrescue tags"
        }, 
        {
            "location": "/software/resurrecting-epel-packages/#table-of-epelrescue-rpms", 
            "text": "Package  Distro version  Date added  Reason added  Date removed      python-six-1.7.3-1.el6  6  2015-08-12  Dep of osg-build (via mock)  2015-10-14    python-argparse-1.2.1-2.el6  6  2015-09-23  Dep of osg-wn-client (via gfal2)  2015-10-14    python-backports-ssl_match_hostname-3.4.0.2-4.el6  6  2015-09-23  Dep of osg-build (via mock)  2015-10-14    python-requests-1.1.0-4.el6  6  2015-09-23  Dep of osg-build (via mock)  2015-10-14    python-urllib3-1.5-7.el6  6  2015-09-23  Dep of osg-build (via mock)  2015-10-14", 
            "title": "Table of epelrescue RPMs"
        }, 
        {
            "location": "/software/resurrecting-epel-packages/#finding-out-if-a-package-is-still-needed-in-epelrescue", 
            "text": "Set  $pkg  to the name of a package to test (e.g.  python-six ), and  $rhel  set to the RHEL version you're testing for (e.g.  5 ,  6 , or  7 ).  Using Carl's  centos-srpms ,  scientific-srpms ,  slf-srpms  scripts:  for script in centos-srpms scientific-srpms slf-srpms; do\n       echo -n $script  :  \n       $script -$rhel $pkg | grep . || echo none\ndone  A dry run of removing the package:  osg-koji untag-pkg -n --all epelrescue-el$rhel $pkg  Remove the  -n  when the output of that looks fine.", 
            "title": "Finding out if a package is still needed in epelrescue"
        }, 
        {
            "location": "/software/koji-mass-rebuilds/", 
            "text": "Mass RPM Rebuilds for a new Build Target in Koji\n\n\nWhenever we move to a new OSG series (OSG 3.3) and/or a new RHEL version (EL7), we want to make new builds for all of our packages in the new koji build target (osg-3.3-el7). Due to tricky build dependencies and unexpected build failures, this can be a messy task; and in the past we have gone about it in an ad-hoc manner.\n\n\nThis document will discuss some of the aspects of the task and issues involved, some possible approaches, and ultimately a proposal for a general tool or procedure for doing our mass rebuilds.\n\n\nNew RHEL version vs new OSG series\n\n\nNew RHEL version\n\n\nFor a new RHEL version, we start with no osg packages to build against, so we are forced to build things in dependency order. Figuring out the dependency order is possibly the most difficult (or interesting) part of doing mass rebuilds -- more on that later.\n\n\nNew OSG series\n\n\nFor a new OSG series within an existing RHEL version, we have more options. While it's possible to \"start from scratch\" the same way we would with a new RHEL version and build everything in dependency order, this is not really necessary if we take advantage of existing builds from the previous series.\n\n\nA prior step is to determine the package list for the new series -- this will be some combination of Upcoming and the current release series, minus any packages pruned for the new series. This should also be reflected in the new trunk packaging area. All the current builds for packages in that list (from upcoming + current series) can be tagged into the new *-development (or *-build) repos. This should make all of the build dependencies available for mass rebuilding the new series all at once (osg-build koji *).\n\n\nAfter some consideration, I wholeheartedly endorse this approach for new OSG series -- for all but academic exercises. Rebuilding in dependency order when all the dependencies are already built just seems like wasted effort.\n\n\nDoing scratch builds of everything first\n\n\nBefore doing the mass rebuilds in a new build target, it seems to be a good idea to do scratch builds of all the packages in the current series first. (Or, at least the ones we intend to bring into the new build target.) This will give us a chance to see any build failures that have crept in (possibly due to upstream changes in the OS or EPEL), and fix them first if desired, but in any case avoid the confusion of seeing the failures for the first time in the new build target.\n\n\nDoing mass scratch rebuilds for an existing series is easy, as they can all be done at once.\n\n\nRelatedly, doing a round of scratch builds \nafter\n successfully building all packages into a new build target can also be useful, because it can reveal dependency issues only present in the new set of builds. Doing developer test installs or a round of VMU tests may also uncover any runtime dependency issues.\n\n\nOptions for calculating build dependencies\n\n\nWe can get dependency information from a number of places:\n\n\n\n\nscraping .spec files for Requires/BuildRequires/Provides and \n%package\n names\n\n\nquerying existing rpms directly on koji-hub and our OS/EPEL mirrors (\nrpm -q\n)\n\n\nquerying srpms from \nosg-build prebuild\n directly for build requirements\n\n\ninspecting previous buildroots to determine resolved build dependencies\n\n\nuse \nrepoquery\n to determine whatrequires/whatprovides for packages\n\n\nuse \nyum-builddep\n to find packages with all build requirements available\n\n\nusing the repodata (primary+filelists) from rpm repositories, including:\n\n\nupcoming + 3.X development + external repos (Centos/EPEL/JPackage), OR\n\n\nosg-upcoming-elX-build, which includes them all\n\n\n\n\nOne important aspect is that the runtime requirements are also relevant for determining build requirements, since a build will require installing all of the runtime requirements of the packages required for the build.\n\n\nThat is, \n(A BuildRequires B) and (B Requires C)\n implies \nA BuildRequires C\n.\n\n\nCombined with the fact that runtime requirements are transitive, that is, \n(A Requires B) and (B Requires C)\n implies \nA Requires C\n, computing build requirements is a recursive operation, which can be many levels deep.\n\n\nAnother question to keep in mind is whether to use versioned requires/provides (i.e., BuildRequires xyz \n= 1.2-3) or to only pay attention to the package/capability names. Similarly, whether to pay any attention to conflicts/obsoletes. These would add complexity to anything except the standard tools (repoquery, yum-builddep) which already take these things into account. (And we may get pretty far even without paying attention to versions.)\n\n\nNote also that the dependencies/capabilities for a given package often varies between different rhel versions.\n\n\nPre-computing (predictive) vs just-in-time\n\n\nTwo different approaches to determining dependency order for building are:\n\n\n\n\npre compute all dependencies based on an existing series/rhel version, OR\n\n\ncompute which remaining packages have all build reqs satisfied now\n\n\n\n\nThe first approach has the benefit of being able to determine the packages that need to be built in order to accomplish a smaller subset goal first -- for example, to be able to install osg-wn-client. (And, if there are problems with resolving certain dependencies (say with osg-wn-client again), it will become apparent earlier, as opposed to not until all possible-to-build packages have been built.) The limitation of this approach is that the predicted set of files/capabilities that a binary package will provide may differ between osg series/rhel versions, and as a result may be inaccurate for the new build target.\n\n\nThe second approach provides somewhat more confidence about being able to correctly determine which packages should be buildable at any point in time, but (as mentioned above) it is a bit more in the dark about seeing the bigger picture of the dependency graph or being able to build subsets of targets.\n\n\nIt may be useful to have both options available -- building from the list in the second approach, but using the first mechanism to have a better picture of where things are at, or perhaps to steer toward finishing a certain subset of packages first.\n\n\nPackage list closure, pruning\n\n\nAt some point (either in the planning stage or after building packages into the new build target), we need to ensure that the new osg series/rhel version contains all of its install requirements for all of its packages. It would probably suffice to do a VMU run that installs each package (perhaps individually, to avoid conflicts).\n\n\nBut if we go about it more analytically, we may also get, as a result, a list of packages which we previously only maintained for the purpose of building our other packages (ie, that were never required at runtime for any use cases that we cared about), which now, in the new target, are no longer build requirements (directly or indirectly) for any packages that we care about installing. Packages in this category could be reviewed to also be dropped from the new build target.\n\n\nProposal / Recommendations\n\n\nAs mentioned earlier, my recommendation is that we treat a new OSG series differently than a new RHEL version.\n\n\nFor a new OSG series:\n\n\n\n\n\n\nupdate native/redhat packaging area to reflect packages for new series, including upcoming + trunk - removed packages\n\n\n\n\n\n\ntag existing builds of packages in new list into the new development tag (eg, for osg-3.3-el6, tag the .osgup.el6 and .osg32.el6 builds into osg-3.3-el6-development)\n\n\n\n\n\n\nbuild all packages in new packaging area into new build target at once\n\n\n\n\n\n\nfor all successful builds, remove corresponding old builds (eg, .osgup/.osg32) from the new tag (osg-3.3-el6-development)\n\n\n\n\n\n\nFor a new RHEL version:\n\n\n\n\npull the repodata from the relevant \n*-build\n repo from koji:\n\n\n\n\nfor pre-computing, use a build repo from an existing rhel version:\n\n\nhttps://koji.chtc.wisc.edu/mnt/koji/repos/osg-3.2-el6-build/latest/x86_64/repodata/\n\n\nfor just-in-time, use the new build repo:\n\n\nhttps://koji.chtc.wisc.edu/mnt/koji/repos/osg-3.2-el8-build/latest/x86_64/repodata/\n\n\nthe primary and filelists (sqlite) files can be used to get runtime requires and provides. (Note that this includes packages from the relevant external repos, also.)\n\n\n\n\ngenerate srpms repodata for the current set of packages to build, with osg-build prebuild and createrepo.\n\n\n\n\nthe primary (sqlite) file can be used to get build-requires.\n\n\n\n\nuse sql to resolve direct dependencies at the package name level:\n\n\nsrc-pkg: bin-pkg (BuildRequires)\n\n\nbin-pkg: bin-pkg (Requires)\n\n\nbin-pkg: src-pkg (bin-pkg comes from which src-pkg? only needed for pre-computing dependencies)\n\n\n\n\n\n\nresolve this list into a full list of recursive build dependencies.\n\n\n\n\nSince this is recursive, there is no way to do it in a fixed number of sql queries. However the above input list is already directly consumable by Make, which is designed to handle recursive dependencies just like this. Or we can write a new tool to do it in python.\n\n\n\n\nbuild ready-to-be-built packages\n\n\nupdate our copy of the repodata from the regen'ed \n*-build\n repo, as often as new versions become available\n\n\nupdate our dependency lists\n\n\nrepeat until all packages are built", 
            "title": "Koji Mass Rebuilds"
        }, 
        {
            "location": "/software/koji-mass-rebuilds/#mass-rpm-rebuilds-for-a-new-build-target-in-koji", 
            "text": "Whenever we move to a new OSG series (OSG 3.3) and/or a new RHEL version (EL7), we want to make new builds for all of our packages in the new koji build target (osg-3.3-el7). Due to tricky build dependencies and unexpected build failures, this can be a messy task; and in the past we have gone about it in an ad-hoc manner.  This document will discuss some of the aspects of the task and issues involved, some possible approaches, and ultimately a proposal for a general tool or procedure for doing our mass rebuilds.", 
            "title": "Mass RPM Rebuilds for a new Build Target in Koji"
        }, 
        {
            "location": "/software/koji-mass-rebuilds/#new-rhel-version-vs-new-osg-series", 
            "text": "", 
            "title": "New RHEL version vs new OSG series"
        }, 
        {
            "location": "/software/koji-mass-rebuilds/#new-rhel-version", 
            "text": "For a new RHEL version, we start with no osg packages to build against, so we are forced to build things in dependency order. Figuring out the dependency order is possibly the most difficult (or interesting) part of doing mass rebuilds -- more on that later.", 
            "title": "New RHEL version"
        }, 
        {
            "location": "/software/koji-mass-rebuilds/#new-osg-series", 
            "text": "For a new OSG series within an existing RHEL version, we have more options. While it's possible to \"start from scratch\" the same way we would with a new RHEL version and build everything in dependency order, this is not really necessary if we take advantage of existing builds from the previous series.  A prior step is to determine the package list for the new series -- this will be some combination of Upcoming and the current release series, minus any packages pruned for the new series. This should also be reflected in the new trunk packaging area. All the current builds for packages in that list (from upcoming + current series) can be tagged into the new *-development (or *-build) repos. This should make all of the build dependencies available for mass rebuilding the new series all at once (osg-build koji *).  After some consideration, I wholeheartedly endorse this approach for new OSG series -- for all but academic exercises. Rebuilding in dependency order when all the dependencies are already built just seems like wasted effort.", 
            "title": "New OSG series"
        }, 
        {
            "location": "/software/koji-mass-rebuilds/#doing-scratch-builds-of-everything-first", 
            "text": "Before doing the mass rebuilds in a new build target, it seems to be a good idea to do scratch builds of all the packages in the current series first. (Or, at least the ones we intend to bring into the new build target.) This will give us a chance to see any build failures that have crept in (possibly due to upstream changes in the OS or EPEL), and fix them first if desired, but in any case avoid the confusion of seeing the failures for the first time in the new build target.  Doing mass scratch rebuilds for an existing series is easy, as they can all be done at once.  Relatedly, doing a round of scratch builds  after  successfully building all packages into a new build target can also be useful, because it can reveal dependency issues only present in the new set of builds. Doing developer test installs or a round of VMU tests may also uncover any runtime dependency issues.", 
            "title": "Doing scratch builds of everything first"
        }, 
        {
            "location": "/software/koji-mass-rebuilds/#options-for-calculating-build-dependencies", 
            "text": "We can get dependency information from a number of places:   scraping .spec files for Requires/BuildRequires/Provides and  %package  names  querying existing rpms directly on koji-hub and our OS/EPEL mirrors ( rpm -q )  querying srpms from  osg-build prebuild  directly for build requirements  inspecting previous buildroots to determine resolved build dependencies  use  repoquery  to determine whatrequires/whatprovides for packages  use  yum-builddep  to find packages with all build requirements available  using the repodata (primary+filelists) from rpm repositories, including:  upcoming + 3.X development + external repos (Centos/EPEL/JPackage), OR  osg-upcoming-elX-build, which includes them all   One important aspect is that the runtime requirements are also relevant for determining build requirements, since a build will require installing all of the runtime requirements of the packages required for the build.  That is,  (A BuildRequires B) and (B Requires C)  implies  A BuildRequires C .  Combined with the fact that runtime requirements are transitive, that is,  (A Requires B) and (B Requires C)  implies  A Requires C , computing build requirements is a recursive operation, which can be many levels deep.  Another question to keep in mind is whether to use versioned requires/provides (i.e., BuildRequires xyz  = 1.2-3) or to only pay attention to the package/capability names. Similarly, whether to pay any attention to conflicts/obsoletes. These would add complexity to anything except the standard tools (repoquery, yum-builddep) which already take these things into account. (And we may get pretty far even without paying attention to versions.)  Note also that the dependencies/capabilities for a given package often varies between different rhel versions.", 
            "title": "Options for calculating build dependencies"
        }, 
        {
            "location": "/software/koji-mass-rebuilds/#pre-computing-predictive-vs-just-in-time", 
            "text": "Two different approaches to determining dependency order for building are:   pre compute all dependencies based on an existing series/rhel version, OR  compute which remaining packages have all build reqs satisfied now   The first approach has the benefit of being able to determine the packages that need to be built in order to accomplish a smaller subset goal first -- for example, to be able to install osg-wn-client. (And, if there are problems with resolving certain dependencies (say with osg-wn-client again), it will become apparent earlier, as opposed to not until all possible-to-build packages have been built.) The limitation of this approach is that the predicted set of files/capabilities that a binary package will provide may differ between osg series/rhel versions, and as a result may be inaccurate for the new build target.  The second approach provides somewhat more confidence about being able to correctly determine which packages should be buildable at any point in time, but (as mentioned above) it is a bit more in the dark about seeing the bigger picture of the dependency graph or being able to build subsets of targets.  It may be useful to have both options available -- building from the list in the second approach, but using the first mechanism to have a better picture of where things are at, or perhaps to steer toward finishing a certain subset of packages first.", 
            "title": "Pre-computing (predictive) vs just-in-time"
        }, 
        {
            "location": "/software/koji-mass-rebuilds/#package-list-closure-pruning", 
            "text": "At some point (either in the planning stage or after building packages into the new build target), we need to ensure that the new osg series/rhel version contains all of its install requirements for all of its packages. It would probably suffice to do a VMU run that installs each package (perhaps individually, to avoid conflicts).  But if we go about it more analytically, we may also get, as a result, a list of packages which we previously only maintained for the purpose of building our other packages (ie, that were never required at runtime for any use cases that we cared about), which now, in the new target, are no longer build requirements (directly or indirectly) for any packages that we care about installing. Packages in this category could be reviewed to also be dropped from the new build target.", 
            "title": "Package list closure, pruning"
        }, 
        {
            "location": "/software/koji-mass-rebuilds/#proposal-recommendations", 
            "text": "As mentioned earlier, my recommendation is that we treat a new OSG series differently than a new RHEL version.", 
            "title": "Proposal / Recommendations"
        }, 
        {
            "location": "/software/koji-mass-rebuilds/#for-a-new-osg-series", 
            "text": "update native/redhat packaging area to reflect packages for new series, including upcoming + trunk - removed packages    tag existing builds of packages in new list into the new development tag (eg, for osg-3.3-el6, tag the .osgup.el6 and .osg32.el6 builds into osg-3.3-el6-development)    build all packages in new packaging area into new build target at once    for all successful builds, remove corresponding old builds (eg, .osgup/.osg32) from the new tag (osg-3.3-el6-development)", 
            "title": "For a new OSG series:"
        }, 
        {
            "location": "/software/koji-mass-rebuilds/#for-a-new-rhel-version", 
            "text": "pull the repodata from the relevant  *-build  repo from koji:   for pre-computing, use a build repo from an existing rhel version:  https://koji.chtc.wisc.edu/mnt/koji/repos/osg-3.2-el6-build/latest/x86_64/repodata/  for just-in-time, use the new build repo:  https://koji.chtc.wisc.edu/mnt/koji/repos/osg-3.2-el8-build/latest/x86_64/repodata/  the primary and filelists (sqlite) files can be used to get runtime requires and provides. (Note that this includes packages from the relevant external repos, also.)   generate srpms repodata for the current set of packages to build, with osg-build prebuild and createrepo.   the primary (sqlite) file can be used to get build-requires.   use sql to resolve direct dependencies at the package name level:  src-pkg: bin-pkg (BuildRequires)  bin-pkg: bin-pkg (Requires)  bin-pkg: src-pkg (bin-pkg comes from which src-pkg? only needed for pre-computing dependencies)    resolve this list into a full list of recursive build dependencies.   Since this is recursive, there is no way to do it in a fixed number of sql queries. However the above input list is already directly consumable by Make, which is designed to handle recursive dependencies just like this. Or we can write a new tool to do it in python.   build ready-to-be-built packages  update our copy of the repodata from the regen'ed  *-build  repo, as often as new versions become available  update our dependency lists  repeat until all packages are built", 
            "title": "For a new RHEL version:"
        }, 
        {
            "location": "/software/create-vo-client/", 
            "text": "Creating the VO Client package\n\n\nOverview\n\n\nThe VO Client package is just a conversion of the tarball created by GOC into the RPM format.\n\n\nIn order to build the RPM, one needs:\n\n\n\n\nThe tarball containing the:\n\n\nvomses\n file\n\n\nedg-mkgridmap.conf\n file\n\n\ngums.config.template\n file\n\n\nvomsdir\n directory tree, containing the lsc files.\n\n\n\n\n\n\nThe RPM spec file\n\n\n\n\nMaking the tarball\n\n\nTo make the tarball:\n\n\n\n\nstart with a clean directory\n\n\ncopy in the \nvomses\n, \ngums.template\n, and \nedg-mkgridmap.conf\n files and rename the edg-mkgridmap file:\n\n\n\n\n\n\n\n      wget http://repo.grid.iu.edu/pacman/tarballs/vo-package/vomses\n      wget http://repo.grid.iu.edu/pacman/tarballs/vo-package/edg-mkgridmap.osg\n      mv edg-mkgridmap.osg edg-mkgridmap.conf\n      wget http://repo.grid.iu.edu/pacman/tarballs/vo-package/gums.template # TODO UNSURE\n\n\n\n\n\n\n\nCreate the vomsdir directory by downloading the .lsc files\n\n\n\n\n\n\n\n     wget --recursive --no-host-directories --cut-dirs=3 -A \n*.lsc\n http://repo.grid.iu.edu/pacman/tarballs/vo-package/vomsdir\n\n\n\n\n\n\n\nIn a separate directory, unpack the \nold\n vo-client tarball (from the upstream source cache)\n\n\ndiff the two directories, and compare the changes to the changelog posted at \nhttps://twiki.grid.iu.edu/bin/view/Operations/PackageV44\n (replace '44' with the current version)\n\n\n\n\n\n\n\n\n\nFollow the instructions in the attached \ngums-template-conversion.txt\n file to convert it from GUMS 1.1 (1.2?) format to GUMS 1.3 format. Name the result \ngums.config.template\n. See also the \nAutomated GUMS Conversion\n section below for a scripted version of this step.\n\n\nMove the files into a subdirectory to include in the tarball:\n\n\n\n\n\n\n\n      VERSION=44  # set appropriately\n      mkdir vo-client-$VERSION\n      mv vomses gums.config.template edg-mkgridmap.conf vomsdir vo-client-$VERSION/\n      tar -czf vo-client-$VERSION-osg.tar.gz vo-client-$VERSION/\n\n\n\n\n\nUpload the tarball into the \nupstream source cache\n, in the \nvo-client/VERSION/\n directory.\n\n\nAutomated GUMS Conversion\n\n\nThe above \ninstructions\n outline a procedure for converting the osg gums.config template from GUMS 1.1 format to 1.3 format. Because setting up a GUMS instance for this can be time consuming and tricky to get right, a script was written to automate the procedure on a Fermi VM. The script lives in svn under: \n$SVN/software/tools/convert-osg-gums-template-for-vo-client.sh\n .\n\n\nTo use it:\n\n\n\n\nCreate a new Fermi VM (el5 or el6)\n\n\nCopy the script and the new \ngums.template\n to be converted to the /root homedir on the VM.\n\n\nLog into the VM as root, make sure the script is executable, and run against the gums template:\n\n\n\n\n\n\n\n      $ ssh root@el6-vo-client\n      # wget https://vdt.cs.wisc.edu/svn/software/tools/convert-osg-gums-template-for-vo-client.sh\n      # chmod +x convert-osg-gums-template-for-vo-client.sh\n      # ./convert-osg-gums-template-for-vo-client.sh gums.template\n\n\n\n\n\n\n\nIt takes a little while to install and set up gums and related packages, but if it succeeds, you should see a message that says \"User group has been saved.\", and a file \ngums.config.template\n should be written in the current directory.\n\n\nThe newly converted \ngums.config.template\n should be compared to the old version of that file (from the previous vo-client package) to ensure that the only the differences are the changes for this release. (I have had to manually strip the extra test account stuff.) The 'meld' program is a nice graphical diff tool that I use for comparing them.\n\n\n\n\nRPM spec file maintenance\n\n\nThe OSG RPM spec file is \nmaintained in Subversion\n.\n\n\nThe VO Client package is located in \nnative/redhat/trunk/vo-client\n\n\nThere are two files that need to be maintained:\n\n\n\n\nosg/vo-client.spec\n - This is the RPM spec file proper. One needs to update the version (and/or the release number) every time a new RPM is created.\n\n\nupstream/release_tarball.source\n - This file contains the relative path of the tarball within the \nupstream source cache\n. Since the tarball file name will change with every new RPM version, this file has to be changed accordingly.\n\n\n\n\nRPM building\n\n\nAfter installing the \nosg-build tools\n, check out a clean copy from svn, then:\n\n\n\n\nosg-build prebuild .\n\n\nOnce there are no errors, run \nosg-build koji . --scratch\n This can be done without making any permanent change.\n\n\nOnce that builds successfully, run \nosg-build koji .\n This is permanent, unlike when you ran with \n--scratch\n. You cannot rebuild this version of the RPM again - you must bump the release number and edit the changelog.\n\n\n\n\nThis will push the RPMs into the OSG development repository. Koji requires additional setup compared to rpmbuild; \nsee the documentation here\n.\n\n\nPromotion to testing and release:\n\n\nPolicies\n\n\nRead \nRelease Policy\n.\n\n\nThese should be synchronized internally with other GOC update activities.", 
            "title": "Creating the VO Client Package"
        }, 
        {
            "location": "/software/create-vo-client/#creating-the-vo-client-package", 
            "text": "", 
            "title": "Creating the VO Client package"
        }, 
        {
            "location": "/software/create-vo-client/#overview", 
            "text": "The VO Client package is just a conversion of the tarball created by GOC into the RPM format.  In order to build the RPM, one needs:   The tarball containing the:  vomses  file  edg-mkgridmap.conf  file  gums.config.template  file  vomsdir  directory tree, containing the lsc files.    The RPM spec file", 
            "title": "Overview"
        }, 
        {
            "location": "/software/create-vo-client/#making-the-tarball", 
            "text": "To make the tarball:   start with a clean directory  copy in the  vomses ,  gums.template , and  edg-mkgridmap.conf  files and rename the edg-mkgridmap file:          wget http://repo.grid.iu.edu/pacman/tarballs/vo-package/vomses\n      wget http://repo.grid.iu.edu/pacman/tarballs/vo-package/edg-mkgridmap.osg\n      mv edg-mkgridmap.osg edg-mkgridmap.conf\n      wget http://repo.grid.iu.edu/pacman/tarballs/vo-package/gums.template # TODO UNSURE   Create the vomsdir directory by downloading the .lsc files         wget --recursive --no-host-directories --cut-dirs=3 -A  *.lsc  http://repo.grid.iu.edu/pacman/tarballs/vo-package/vomsdir   In a separate directory, unpack the  old  vo-client tarball (from the upstream source cache)  diff the two directories, and compare the changes to the changelog posted at  https://twiki.grid.iu.edu/bin/view/Operations/PackageV44  (replace '44' with the current version)     Follow the instructions in the attached  gums-template-conversion.txt  file to convert it from GUMS 1.1 (1.2?) format to GUMS 1.3 format. Name the result  gums.config.template . See also the  Automated GUMS Conversion  section below for a scripted version of this step.  Move the files into a subdirectory to include in the tarball:          VERSION=44  # set appropriately\n      mkdir vo-client-$VERSION\n      mv vomses gums.config.template edg-mkgridmap.conf vomsdir vo-client-$VERSION/\n      tar -czf vo-client-$VERSION-osg.tar.gz vo-client-$VERSION/  Upload the tarball into the  upstream source cache , in the  vo-client/VERSION/  directory.", 
            "title": "Making the tarball"
        }, 
        {
            "location": "/software/create-vo-client/#automated-gums-conversion", 
            "text": "The above  instructions  outline a procedure for converting the osg gums.config template from GUMS 1.1 format to 1.3 format. Because setting up a GUMS instance for this can be time consuming and tricky to get right, a script was written to automate the procedure on a Fermi VM. The script lives in svn under:  $SVN/software/tools/convert-osg-gums-template-for-vo-client.sh  .  To use it:   Create a new Fermi VM (el5 or el6)  Copy the script and the new  gums.template  to be converted to the /root homedir on the VM.  Log into the VM as root, make sure the script is executable, and run against the gums template:          $ ssh root@el6-vo-client\n      # wget https://vdt.cs.wisc.edu/svn/software/tools/convert-osg-gums-template-for-vo-client.sh\n      # chmod +x convert-osg-gums-template-for-vo-client.sh\n      # ./convert-osg-gums-template-for-vo-client.sh gums.template   It takes a little while to install and set up gums and related packages, but if it succeeds, you should see a message that says \"User group has been saved.\", and a file  gums.config.template  should be written in the current directory.  The newly converted  gums.config.template  should be compared to the old version of that file (from the previous vo-client package) to ensure that the only the differences are the changes for this release. (I have had to manually strip the extra test account stuff.) The 'meld' program is a nice graphical diff tool that I use for comparing them.", 
            "title": "Automated GUMS Conversion"
        }, 
        {
            "location": "/software/create-vo-client/#rpm-spec-file-maintenance", 
            "text": "The OSG RPM spec file is  maintained in Subversion .  The VO Client package is located in  native/redhat/trunk/vo-client  There are two files that need to be maintained:   osg/vo-client.spec  - This is the RPM spec file proper. One needs to update the version (and/or the release number) every time a new RPM is created.  upstream/release_tarball.source  - This file contains the relative path of the tarball within the  upstream source cache . Since the tarball file name will change with every new RPM version, this file has to be changed accordingly.", 
            "title": "RPM spec file maintenance"
        }, 
        {
            "location": "/software/create-vo-client/#rpm-building", 
            "text": "After installing the  osg-build tools , check out a clean copy from svn, then:   osg-build prebuild .  Once there are no errors, run  osg-build koji . --scratch  This can be done without making any permanent change.  Once that builds successfully, run  osg-build koji .  This is permanent, unlike when you ran with  --scratch . You cannot rebuild this version of the RPM again - you must bump the release number and edit the changelog.   This will push the RPMs into the OSG development repository. Koji requires additional setup compared to rpmbuild;  see the documentation here .", 
            "title": "RPM building"
        }, 
        {
            "location": "/software/create-vo-client/#promotion-to-testing-and-release", 
            "text": "", 
            "title": "Promotion to testing and release:"
        }, 
        {
            "location": "/software/create-vo-client/#policies", 
            "text": "Read  Release Policy .  These should be synchronized internally with other GOC update activities.", 
            "title": "Policies"
        }, 
        {
            "location": "/release/cut-sw-release/", 
            "text": "Note\n\n\nIf you are performing a data release, please follow the instructions \nhere\n\n\n\n\nHow to Cut a Software Release\n\n\nThis document details the process for releasing new OSG Release version(s). This document does NOT discuss the policy for deciding what goes into a release, which can be found \nhere\n.\n\n\nDue to the length of time that this process takes, it is recommended to do the release over three or more days to allow for errors to be corrected and tests to be run.\n\n\nRequirements\n\n\n\n\nUser certificate registered with OSG's koji with build and release team privileges\n\n\nAn account on UW CS machines (e.g. \nlibrary\n, \ningwe\n) to access UW's AFS\n\n\nrelease-tools\n scripts in your \nPATH\n (\nGitHub\n)\n\n\nosg-build\n scripts in your \nPATH\n (installed via OSG yum repos or \nsource\n)\n\n\n\n\nPick the Version Number\n\n\nThe rest of this document makes references to \nVERSION(S)\n and \nNON-UPCOMING VERSIONS(S)\n, which refer to a space-delimited list of OSG version(s) and that same list minus \nupcoming\n (e.g. \n3.3.28 3.4.3 upcoming\n and \n3.3.28 3.4.3\n). If you are unsure about either the version or revision, please consult the release manager.\n\n\nDay 0: Generate Preliminary Release List\n\n\nThe release manager often needs a tentative list of packages to be released. This is done by finding the package differences between osg-testing and the current release.\n\n\nStep 1: Update the osg-version RPM\n\n\nFor each release (excluding upcoming), update the version number in the osg-version RPM's spec file and build it in koji:\n\n\n#\n If building \nfor\n the latest release out of trunk\n\n[user@client ~] $\n osg-build koji osg-version\n\n#\n If building \nfor\n an older release out of a branch:\n\n[user@client ~] $\n osg-build koji --repo\n=\nMAJOR VERSION\n osg-version\n\n\n\n\n\nWhere \nMAJOR VERSION\n is of the format \nx.y\n (e.g. \n3.2\n).\n\n\nStep 2: Promote osg-version and generate the release list\n\n\nRun \n0-generate-pkg-list\n from a machine that has your koji-registered user certificate:\n\n\n[user@client ~] $\n git clone https://github.com/opensciencegrid/release-tools.git\n\n[user@client ~] $\n \ncd\n release-tools\n\n[user@client ~] $\n \n0\n-generate-pkg-list \nVERSION\n(\nS\n)\n\n\n\n\n\n\nDay 1: Verify Pre-Release and Generate Tarballs\n\n\nThis section is to be performed 1-2 days before the release (as designated by the release manager) to perform last checks of the release and create the client tarballs.\n\n\nStep 1: Verify Pre-Release\n\n\nCompare the list of packages already in pre-release to the final list for the release put together by the OSG Release Coordinator (who should have updated \nrelease-list\n in git). To do this, run the \n1-verify-prerelease\n script from git:\n\n\n[user@client ~] $\n \n1\n-verify-prerelease \nVERSION\n(\nS\n)\n\n\n\n\n\n\nIf there are any discrepancies consult the release manager. You may have to tag or untag packages with the \nosg-koji\n tool.\n\n\n\n\nNote\n\n\nPlease verify that the \nosg-version\n RPM is in your set of packages for the release! Also verify that if there is a new version of the \nosg-tested-internal\n RPM, then it is included in the release as well!\n\n\n\n\nStep 2: Test Pre-Release in VM Universe\n\n\nTo test pre-release, you will be kicking off a manual VM universe test run from \nosghost.chtc.wisc.edu\n.\n\n\n\n\nEnsure that you meet the \npre-requisites\n for submitting VM universe test runs\n\n\n\n\nPrepare the test suite by running:\n\n\n[user@client ~] $ osg-run-tests \nTesting OSG pre-release\n\n\n\n\n\n\n\n\n\n\ncd\n into the directory specified in the output of the previous command\n\n\n\n\ncd\n into \nparameters.d\n and remove all files within it except for \nosg33-el6.yaml\n and \nosg33-el7.yaml\n\n\n\n\nEdit \nosg33.yaml\n so that it reads:\n\n\nplatforms\n:\n\n  \n-\n \ncentos_6_x86_64\n\n  \n-\n \nrhel_6_x86_64\n\n  \n-\n \nsl_6_x86_64\n\n  \n-\n \ncentos_7_x86_64\n\n  \n-\n \nrhel_7_x86_64\n\n  \n-\n \nsl_7_x86_64\n\n\n\nsources\n:\n\n  \n-\n \nopensciencegrid\n:\nmaster\n;\n \n3\n.\n3\n;\n \nosg-prerelease\n\n  \n-\n \nopensciencegrid\n:\nmaster\n;\n \n3\n.\n3\n;\n \nosg\n \n \nosg-prerelease\n\n\n\npackage_sets\n:\n\n  \n-\n \nlabel\n:\n \nAll\n \n(\njava\n)\n\n    \nselinux\n:\n \nTrue\n\n    \nosg_java\n:\n \nTrue\n\n    \npackages\n:\n\n      \n-\n \nosg-tested-internal\n\n\n\n\n\n\n\n\n\n\nEdit \nosg34.yaml\n so that it reads:\n\n\nplatforms\n:\n\n  \n-\n \ncentos_6_x86_64\n\n  \n-\n \nrhel_6_x86_64\n\n  \n-\n \nsl_6_x86_64\n\n  \n-\n \ncentos_7_x86_64\n\n  \n-\n \nrhel_7_x86_64\n\n  \n-\n \nsl_7_x86_64\n\n\n\nsources\n:\n\n  \n-\n \nopensciencegrid\n:\nmaster\n;\n \n3\n.\n4\n;\n \nosg-prerelease\n\n  \n-\n \nopensciencegrid\n:\nmaster\n;\n \n3\n.\n4\n;\n \nosg\n \n \nosg-prerelease\n\n  \n-\n \nopensciencegrid\n:\nmaster\n;\n \n3\n.\n3\n;\n \nosg\n \n \n3\n.\n4\n/\nosg-prerelease\n\n  \n-\n \nopensciencegrid\n:\nmaster\n;\n \n3\n.\n4\n;\n \nosg-prerelease\n,\n \nosg-upcoming-prerelease\n,\n \nosg-upcoming\n\n  \n-\n \nopensciencegrid\n:\nmaster\n;\n \n3\n.\n4\n;\n \nosg\n \n \nosg-prerelease\n,\n \nosg-upcoming-prerelease\n,\n \nosg-upcoming\n\n\n\npackage_sets\n:\n\n  \n-\n \nlabel\n:\n \nAll\n\n    \nselinux\n:\n \nTrue\n\n    \nosg_java\n:\n \nFalse\n\n    \npackages\n:\n\n      \n-\n \nosg-tested-internal\n\n\n\n\n\n\nIf you are not releasing packages into \nupcoming\n, delete the \nupcoming\n-related lines in the \nsources\n section.\n\n\n\n\n\n\ncd\n back into the root directory of the test run (e.g. \ncd ..\n)\n\n\n\n\nSubmit the DAG:\ncondor_submit_dag master-run.dag\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nIf there are failures, consult the release-manager before proceeding.\n\n\n\n\nStep 3: Regenerate the build repositories\n\n\nTo avoid 404 errors when retrieving packages, it's necessary to regenerate the build repositories. Run the following script from a machine with your koji-registered user certificate:\n\n\n[user@client ~] $\n \n1\n-regen-repos \nNON-UPCOMING VERSION\n(\nS\n)\n\n\n\n\n\n\nStep 4: Create the client tarballs\n\n\nCreate the client tarballs as root on an EL6 fermicloud machine using the relevant script from git:\n\n\n[root@client ~] $\n git clone https://github.com/opensciencegrid/release-tools.git\n\n[root@client ~] $\n \ncd\n release-tools\n\n[root@client ~] $\n \n1\n-client-tarballs \nNON-UPCOMING VERSION\n(\nS\n)\n\n\n\n\n\n\nYou should get up to 8 tarballs for each version (excluding upcoming), 25-55 megs each and they should all have the version number in the name.\n\n\nStep 5: Briefly test the client tarballs\n\n\nAs an \nunprivileged user\n, extract each tarball into a separate directory. Make sure osg-post-install works. Make sure \nosgrun osg-version\n works by running the following tests, replacing \nNON-UPCOMING VERSION(S)\n with the appropriate version numbers:\n\n\ndotest \n()\n \n{\n\n    \nfile\n=\n$dir\n/\n$client\n-\n$ver\n-1.\n$rhel\n.\n$arch\n.tar.gz\n    mkdir -p \n$rhel\n-\n$arch\n\n    \npushd\n \n$rhel\n-\n$arch\n\n    tar xzf ../\n$file\n\n    \n$client\n/osg/osg-post-install\n    \n$client\n/osgrun osg-version\n    \npopd\n\n    rm -rf \n$rhel\n-\n$arch\n\n\n}\n\n\n\npushd\n /tmp\n\n\nfor\n client in osg-afs-client osg-wn-client\n;\n \ndo\n\n    \nfor\n ver in \nNON-UPCOMING VERSION\n(\nS\n)\n;\n \ndo\n\n        \nfor\n rhel in el6 el7\n;\n \ndo\n\n            \narch\n=\nx86_64\n            \ndir\n=\ntarballs/\n${\nver\n%.*\n}\n/\n$arch\n\n            dotest\n        \ndone\n\n    \ndone\n\n\ndone\n\n\n\nclient\n=\nosg-wn-client\n\narch\n=\ni386\n\nrhel\n=\nel6\n\nfor\n ver in \nNON-UPCOMING VERSION\n(\nS\n)\n;\n \ndo\n\n    \ndir\n=\ntarballs/\n${\nver\n%.*\n}\n/\n$arch\n\n    dotest\n\ndone\n\n\n\n\n\n\nIf you have time, try some of the binaries, such as grid-proxy-init.\n\n\n\n\nNote\n\n\nWe need to automate this and have it run on the proper architectures and version of RHEL.\n\n\n\n\nStep 6: Update the UW AFS installation of the tarball client\n\n\nThe UW keeps an install of the tarball client in \n/p/vdt/workspace/tarball-client\n on the UW's AFS. To update it, run the following commands:\n\n\nfor\n ver in \nNON-UPCOMING VERSION\n(\nS\n)\n;\n \ndo\n\n    /p/vdt/workspace/tarball-client/afs-install-tarball-client \n$ver\n\n\ndone\n\n\n\n\n\n\nStep 7: Wait\n\n\nWait for clearance. The OSG Release Coordinator (in consultation with the Software Team and any testers) need to sign off on the update before it is released. If you are releasing things over two days, this is a good place to stop for the day.\n\n\nDay 2: Pushing the Release\n\n\n\n\nNote\n\n\nFor the second phase of the release, try to complete it earlier in the day rather than later. The GOC would like to send out the release announcement prior to 3 p.m. Eastern time.\n\n\n\n\nStep 1: Push from pre-release to release\n\n\nThis script moves the packages into release, clones releases into new version-specific release repos, locks the repos and regenerates them. Afterwards, it produces \n*release-note*\n files that should be used to update the release note pages. Clone it from the github repo and run the script:\n\n\n[user@client ~] $\n \n2\n-create-release \nVERSION\n(\nS\n)\n\n\n\n\n\n\n\n\n*.txt\n files are also created and it should be verified that they've been moved to /p/vdt/public/html/release-info/ on UW's AFS.\n\n\nFor each release version, use the \n*release-note*\n files to update the relevant sections of the release note pages.\n\n\n\n\nStep 2: Upload the client tarballs\n\n\nAsk Tim Theisen, Brian Lin, or someone with privileges on the \ngrid.iu.edu\n repo servers to upload the tarballs with the following procedure:\n\n\nOn a CS machine\n\n\nfor\n ver in \nNON-UPCOMING VERSION\n(\nS\n)\n;\n \ndo\n\n    \nmajor_ver\n=\n`\nsed \ns/.[0-9]*$//\n \n \n$ver\n`\n\n    \ncd\n /p/vdt/public/html/tarball-client\n    ssh jump.grid.iu.edu mkdir /tmp/\n$ver\n/\n    scp -p \n$major_ver\n/*/osg-wn-client-\n$ver\n*gz jump.grid.iu.edu:/tmp/\n$ver\n/\n\ndone\n\n\n\n\n\n\nOn jump.grid.iu.edu\n\n\nfor\n ver in \nNON-UPCOMING VERSION\n(\nS\n)\n;\n \ndo\n\n    scp -pr /tmp/\n$ver\n repo1:/tmp/\n    scp -pr /tmp/\n$ver\n repo2:/tmp/\n    rm -rf /tmp/\n$ver\n\n\ndone\n\n\n\n\n\n\nOn repo1/repo2 (as root)\n\n\nYou can ssh to repo1 and repo2 from jump.grid.iu.edu; you will need to do this procedure on both systems.\n\n\nsudo su -\n\n\n\n\n\n\nfor\n ver in \nNON-UPCOMING VERSION\n(\nS\n)\n;\n \ndo\n\n    \nmajor_ver\n=\n`\nsed \ns/.[0-9]*$//\n \n \n$ver\n`\n\n    mv /tmp/\n$ver\n /usr/local/repo/tarball-install/\n$major_ver\n/\n    rm -f /usr/local/repo/tarball-install/\n$major_ver\n/*latest*\n\ndone\n\n/root/mk-sims.sh\n\nfor\n ver in \nNON-UPCOMING VERSION\n(\nS\n)\n;\n \ndo\n\n    \nmajor_ver\n=\n`\nsed \ns/.[0-9]*$//\n \n \n$ver\n`\n\n    ls -l /usr/local/repo/tarball-install/\n$major_ver\n/*latest* \n# verify the symlinks are correct\n\n\ndone\n\n\n\n\n\n\nStep 3: Install the tarballs into OASIS\n\n\n\n\nNote\n\n\nYou must be an OASIS manager of the \nmis\n VO to do these steps. Known managers as of 2014-07-22: Mat, Tim C, Tim T, Brian L. \n\n\n\n\nGet the uploader script from Git and run it with \nosgrun\n from the UW AFS install of the tarball client you made earlier. On a UW CSL machine:\n\n\ncd\n /tmp\ngit clone --depth \n1\n file:///p/vdt/workspace/git/repo/tarball-client.git\n\nfor\n ver in \nNON-UPCOMING VERSION\n(\nS\n)\n;\n \ndo\n\n    /p/vdt/workspace/tarball-client/current/sys/osgrun /tmp/tarball-client/upload-tarballs-to-oasis \n$ver\n\n\ndone\n\n\n\n\n\n\nThe script will automatically ssh you to oasis-login.opensciencegrid.org and give you instructions to complete the process.\n\n\nStep 4: Remove old UW AFS installations of the tarball client\n\n\nTo keep space usage down, remove tarball client installations and symlinks under \n/p/vdt/workspace/tarball-client\n on UW's AFS that are more than 2 months old. The following command will remove them:\n\n\n[user@client ~] $\n find /p/vdt/workspace/tarball-client -maxdepth \n1\n -mtime +60 -name \n3\n\\*\n -ls -exec rm -rf \n{}\n \n\\;\n\n\n\n\n\n\nStep 5: Update the Docker WN client\n\n\nUpdate the GitHub repo at \nopensciencegrid/docker-osg-wn\n using the \nupdate-all\n script found in \nopensciencegrid/docker-osg-wn-scripts\n. This requires push access to the \nopensciencegrid/docker-osg-wn\n repo.\n\n\nInstructions for using the script:\n\n\ngit clone git@github.com:opensciencegrid/docker-osg-wn-scripts.git\n\n\ngit clone git@github.com:opensciencegrid/docker-osg-wn.git\n\n\ndocker-osg-wn-scripts/update-all docker-osg-wn\n\n\ncd docker-osg-wn\n\n\n#\n Verify everything looks fine and run the \ngit push\n \ncommand\n\n\n#\n that \nupdate-all\n should have printed\n\n\n\n\n\nStep 6: Announce the release\n\n\nThe following instructions are meant for the release manager (or interim release manager). If you are not the release manager, let the release manager know that they can announce the release.\n\n\n\n\n\n\nThe release manager writes the release announcement and send it out. Here is a sample, replace \nHIGHLIGHTED TEXT\n with the appropriate values:\n\n\n Subject: Announcing OSG Software version \nVERSION(S)\n\n\n We are pleased to announce OSG Software version \nVERSION(S)\n!\n\n This is the new OSG Software distributed via RPMs for:\n\n * Scientific Linux 6 and 7\n * CentOS 6 and 7\n * Red Hat Enterprise Linux 6 and 7\n\n This release affects the \nSET OF METAPACKAGES (client, compute element, etc...)\n. Changes include:\n\n * Major change 1\n * Major change 2\n * Major change 3\n\n Release notes and pointers to more documentation can be found at:\n\n \nLINK TO RELEASE NOTES\n\n\n Need help? Let us know:\n\n https://www.opensciencegrid.org/bin/view/Documentation/Release3/HelpProcedure\n\n We welcome feedback on this release\n\n\n\n\n\n\n\n\n\nThe release manager emails the announcement to \nvdt-discuss@opensciencegrid.org\n\n\n\n\nThe release manager asks the GOC to distribute the announcement by \nopening a ticket\n\n\nThe release manager closes the tickets marked 'Ready for Release' in the release's JIRA filter using the 'bulk change' function. Uncheck the box that reads \"Send mail for this update\"\n\n\nThe release manager updates the recent/scheduled release tables on the Software/Release \nhomepage", 
            "title": "How to Cut a Release"
        }, 
        {
            "location": "/release/cut-sw-release/#how-to-cut-a-software-release", 
            "text": "This document details the process for releasing new OSG Release version(s). This document does NOT discuss the policy for deciding what goes into a release, which can be found  here .  Due to the length of time that this process takes, it is recommended to do the release over three or more days to allow for errors to be corrected and tests to be run.", 
            "title": "How to Cut a Software Release"
        }, 
        {
            "location": "/release/cut-sw-release/#requirements", 
            "text": "User certificate registered with OSG's koji with build and release team privileges  An account on UW CS machines (e.g.  library ,  ingwe ) to access UW's AFS  release-tools  scripts in your  PATH  ( GitHub )  osg-build  scripts in your  PATH  (installed via OSG yum repos or  source )", 
            "title": "Requirements"
        }, 
        {
            "location": "/release/cut-sw-release/#pick-the-version-number", 
            "text": "The rest of this document makes references to  VERSION(S)  and  NON-UPCOMING VERSIONS(S) , which refer to a space-delimited list of OSG version(s) and that same list minus  upcoming  (e.g.  3.3.28 3.4.3 upcoming  and  3.3.28 3.4.3 ). If you are unsure about either the version or revision, please consult the release manager.", 
            "title": "Pick the Version Number"
        }, 
        {
            "location": "/release/cut-sw-release/#day-0-generate-preliminary-release-list", 
            "text": "The release manager often needs a tentative list of packages to be released. This is done by finding the package differences between osg-testing and the current release.", 
            "title": "Day 0: Generate Preliminary Release List"
        }, 
        {
            "location": "/release/cut-sw-release/#step-1-update-the-osg-version-rpm", 
            "text": "For each release (excluding upcoming), update the version number in the osg-version RPM's spec file and build it in koji:  #  If building  for  the latest release out of trunk [user@client ~] $  osg-build koji osg-version #  If building  for  an older release out of a branch: [user@client ~] $  osg-build koji --repo = MAJOR VERSION  osg-version  Where  MAJOR VERSION  is of the format  x.y  (e.g.  3.2 ).", 
            "title": "Step 1: Update the osg-version RPM"
        }, 
        {
            "location": "/release/cut-sw-release/#step-2-promote-osg-version-and-generate-the-release-list", 
            "text": "Run  0-generate-pkg-list  from a machine that has your koji-registered user certificate:  [user@client ~] $  git clone https://github.com/opensciencegrid/release-tools.git [user@client ~] $   cd  release-tools [user@client ~] $   0 -generate-pkg-list  VERSION ( S )", 
            "title": "Step 2: Promote osg-version and generate the release list"
        }, 
        {
            "location": "/release/cut-sw-release/#day-1-verify-pre-release-and-generate-tarballs", 
            "text": "This section is to be performed 1-2 days before the release (as designated by the release manager) to perform last checks of the release and create the client tarballs.", 
            "title": "Day 1: Verify Pre-Release and Generate Tarballs"
        }, 
        {
            "location": "/release/cut-sw-release/#step-1-verify-pre-release", 
            "text": "Compare the list of packages already in pre-release to the final list for the release put together by the OSG Release Coordinator (who should have updated  release-list  in git). To do this, run the  1-verify-prerelease  script from git:  [user@client ~] $   1 -verify-prerelease  VERSION ( S )   If there are any discrepancies consult the release manager. You may have to tag or untag packages with the  osg-koji  tool.   Note  Please verify that the  osg-version  RPM is in your set of packages for the release! Also verify that if there is a new version of the  osg-tested-internal  RPM, then it is included in the release as well!", 
            "title": "Step 1: Verify Pre-Release"
        }, 
        {
            "location": "/release/cut-sw-release/#step-2-test-pre-release-in-vm-universe", 
            "text": "To test pre-release, you will be kicking off a manual VM universe test run from  osghost.chtc.wisc.edu .   Ensure that you meet the  pre-requisites  for submitting VM universe test runs   Prepare the test suite by running:  [user@client ~] $ osg-run-tests  Testing OSG pre-release     cd  into the directory specified in the output of the previous command   cd  into  parameters.d  and remove all files within it except for  osg33-el6.yaml  and  osg33-el7.yaml   Edit  osg33.yaml  so that it reads:  platforms : \n   -   centos_6_x86_64 \n   -   rhel_6_x86_64 \n   -   sl_6_x86_64 \n   -   centos_7_x86_64 \n   -   rhel_7_x86_64 \n   -   sl_7_x86_64  sources : \n   -   opensciencegrid : master ;   3 . 3 ;   osg-prerelease \n   -   opensciencegrid : master ;   3 . 3 ;   osg     osg-prerelease  package_sets : \n   -   label :   All   ( java ) \n     selinux :   True \n     osg_java :   True \n     packages : \n       -   osg-tested-internal     Edit  osg34.yaml  so that it reads:  platforms : \n   -   centos_6_x86_64 \n   -   rhel_6_x86_64 \n   -   sl_6_x86_64 \n   -   centos_7_x86_64 \n   -   rhel_7_x86_64 \n   -   sl_7_x86_64  sources : \n   -   opensciencegrid : master ;   3 . 4 ;   osg-prerelease \n   -   opensciencegrid : master ;   3 . 4 ;   osg     osg-prerelease \n   -   opensciencegrid : master ;   3 . 3 ;   osg     3 . 4 / osg-prerelease \n   -   opensciencegrid : master ;   3 . 4 ;   osg-prerelease ,   osg-upcoming-prerelease ,   osg-upcoming \n   -   opensciencegrid : master ;   3 . 4 ;   osg     osg-prerelease ,   osg-upcoming-prerelease ,   osg-upcoming  package_sets : \n   -   label :   All \n     selinux :   True \n     osg_java :   False \n     packages : \n       -   osg-tested-internal   If you are not releasing packages into  upcoming , delete the  upcoming -related lines in the  sources  section.    cd  back into the root directory of the test run (e.g.  cd .. )   Submit the DAG: condor_submit_dag master-run.dag     Note  If there are failures, consult the release-manager before proceeding.", 
            "title": "Step 2: Test Pre-Release in VM Universe"
        }, 
        {
            "location": "/release/cut-sw-release/#step-3-regenerate-the-build-repositories", 
            "text": "To avoid 404 errors when retrieving packages, it's necessary to regenerate the build repositories. Run the following script from a machine with your koji-registered user certificate:  [user@client ~] $   1 -regen-repos  NON-UPCOMING VERSION ( S )", 
            "title": "Step 3: Regenerate the build repositories"
        }, 
        {
            "location": "/release/cut-sw-release/#step-4-create-the-client-tarballs", 
            "text": "Create the client tarballs as root on an EL6 fermicloud machine using the relevant script from git:  [root@client ~] $  git clone https://github.com/opensciencegrid/release-tools.git [root@client ~] $   cd  release-tools [root@client ~] $   1 -client-tarballs  NON-UPCOMING VERSION ( S )   You should get up to 8 tarballs for each version (excluding upcoming), 25-55 megs each and they should all have the version number in the name.", 
            "title": "Step 4: Create the client tarballs"
        }, 
        {
            "location": "/release/cut-sw-release/#step-5-briefly-test-the-client-tarballs", 
            "text": "As an  unprivileged user , extract each tarball into a separate directory. Make sure osg-post-install works. Make sure  osgrun osg-version  works by running the following tests, replacing  NON-UPCOMING VERSION(S)  with the appropriate version numbers:  dotest  ()   { \n     file = $dir / $client - $ver -1. $rhel . $arch .tar.gz\n    mkdir -p  $rhel - $arch \n     pushd   $rhel - $arch \n    tar xzf ../ $file \n     $client /osg/osg-post-install\n     $client /osgrun osg-version\n     popd \n    rm -rf  $rhel - $arch  }  pushd  /tmp for  client in osg-afs-client osg-wn-client ;   do \n     for  ver in  NON-UPCOMING VERSION ( S ) ;   do \n         for  rhel in el6 el7 ;   do \n             arch = x86_64\n             dir = tarballs/ ${ ver %.* } / $arch \n            dotest\n         done \n     done  done  client = osg-wn-client arch = i386 rhel = el6 for  ver in  NON-UPCOMING VERSION ( S ) ;   do \n     dir = tarballs/ ${ ver %.* } / $arch \n    dotest done   If you have time, try some of the binaries, such as grid-proxy-init.   Note  We need to automate this and have it run on the proper architectures and version of RHEL.", 
            "title": "Step 5: Briefly test the client tarballs"
        }, 
        {
            "location": "/release/cut-sw-release/#step-6-update-the-uw-afs-installation-of-the-tarball-client", 
            "text": "The UW keeps an install of the tarball client in  /p/vdt/workspace/tarball-client  on the UW's AFS. To update it, run the following commands:  for  ver in  NON-UPCOMING VERSION ( S ) ;   do \n    /p/vdt/workspace/tarball-client/afs-install-tarball-client  $ver  done", 
            "title": "Step 6: Update the UW AFS installation of the tarball client"
        }, 
        {
            "location": "/release/cut-sw-release/#step-7-wait", 
            "text": "Wait for clearance. The OSG Release Coordinator (in consultation with the Software Team and any testers) need to sign off on the update before it is released. If you are releasing things over two days, this is a good place to stop for the day.", 
            "title": "Step 7: Wait"
        }, 
        {
            "location": "/release/cut-sw-release/#day-2-pushing-the-release", 
            "text": "Note  For the second phase of the release, try to complete it earlier in the day rather than later. The GOC would like to send out the release announcement prior to 3 p.m. Eastern time.", 
            "title": "Day 2: Pushing the Release"
        }, 
        {
            "location": "/release/cut-sw-release/#step-1-push-from-pre-release-to-release", 
            "text": "This script moves the packages into release, clones releases into new version-specific release repos, locks the repos and regenerates them. Afterwards, it produces  *release-note*  files that should be used to update the release note pages. Clone it from the github repo and run the script:  [user@client ~] $   2 -create-release  VERSION ( S )    *.txt  files are also created and it should be verified that they've been moved to /p/vdt/public/html/release-info/ on UW's AFS.  For each release version, use the  *release-note*  files to update the relevant sections of the release note pages.", 
            "title": "Step 1: Push from pre-release to release"
        }, 
        {
            "location": "/release/cut-sw-release/#step-2-upload-the-client-tarballs", 
            "text": "Ask Tim Theisen, Brian Lin, or someone with privileges on the  grid.iu.edu  repo servers to upload the tarballs with the following procedure:", 
            "title": "Step 2: Upload the client tarballs"
        }, 
        {
            "location": "/release/cut-sw-release/#on-a-cs-machine", 
            "text": "for  ver in  NON-UPCOMING VERSION ( S ) ;   do \n     major_ver = ` sed  s/.[0-9]*$//     $ver ` \n     cd  /p/vdt/public/html/tarball-client\n    ssh jump.grid.iu.edu mkdir /tmp/ $ver /\n    scp -p  $major_ver /*/osg-wn-client- $ver *gz jump.grid.iu.edu:/tmp/ $ver / done", 
            "title": "On a CS machine"
        }, 
        {
            "location": "/release/cut-sw-release/#on-jumpgridiuedu", 
            "text": "for  ver in  NON-UPCOMING VERSION ( S ) ;   do \n    scp -pr /tmp/ $ver  repo1:/tmp/\n    scp -pr /tmp/ $ver  repo2:/tmp/\n    rm -rf /tmp/ $ver  done", 
            "title": "On jump.grid.iu.edu"
        }, 
        {
            "location": "/release/cut-sw-release/#on-repo1repo2-as-root", 
            "text": "You can ssh to repo1 and repo2 from jump.grid.iu.edu; you will need to do this procedure on both systems.  sudo su -   for  ver in  NON-UPCOMING VERSION ( S ) ;   do \n     major_ver = ` sed  s/.[0-9]*$//     $ver ` \n    mv /tmp/ $ver  /usr/local/repo/tarball-install/ $major_ver /\n    rm -f /usr/local/repo/tarball-install/ $major_ver /*latest* done \n/root/mk-sims.sh for  ver in  NON-UPCOMING VERSION ( S ) ;   do \n     major_ver = ` sed  s/.[0-9]*$//     $ver ` \n    ls -l /usr/local/repo/tarball-install/ $major_ver /*latest*  # verify the symlinks are correct  done", 
            "title": "On repo1/repo2 (as root)"
        }, 
        {
            "location": "/release/cut-sw-release/#step-3-install-the-tarballs-into-oasis", 
            "text": "Note  You must be an OASIS manager of the  mis  VO to do these steps. Known managers as of 2014-07-22: Mat, Tim C, Tim T, Brian L.    Get the uploader script from Git and run it with  osgrun  from the UW AFS install of the tarball client you made earlier. On a UW CSL machine:  cd  /tmp\ngit clone --depth  1  file:///p/vdt/workspace/git/repo/tarball-client.git for  ver in  NON-UPCOMING VERSION ( S ) ;   do \n    /p/vdt/workspace/tarball-client/current/sys/osgrun /tmp/tarball-client/upload-tarballs-to-oasis  $ver  done   The script will automatically ssh you to oasis-login.opensciencegrid.org and give you instructions to complete the process.", 
            "title": "Step 3: Install the tarballs into OASIS"
        }, 
        {
            "location": "/release/cut-sw-release/#step-4-remove-old-uw-afs-installations-of-the-tarball-client", 
            "text": "To keep space usage down, remove tarball client installations and symlinks under  /p/vdt/workspace/tarball-client  on UW's AFS that are more than 2 months old. The following command will remove them:  [user@client ~] $  find /p/vdt/workspace/tarball-client -maxdepth  1  -mtime +60 -name  3 \\*  -ls -exec rm -rf  {}   \\;", 
            "title": "Step 4: Remove old UW AFS installations of the tarball client"
        }, 
        {
            "location": "/release/cut-sw-release/#step-5-update-the-docker-wn-client", 
            "text": "Update the GitHub repo at  opensciencegrid/docker-osg-wn  using the  update-all  script found in  opensciencegrid/docker-osg-wn-scripts . This requires push access to the  opensciencegrid/docker-osg-wn  repo.  Instructions for using the script:  git clone git@github.com:opensciencegrid/docker-osg-wn-scripts.git  git clone git@github.com:opensciencegrid/docker-osg-wn.git  docker-osg-wn-scripts/update-all docker-osg-wn  cd docker-osg-wn  #  Verify everything looks fine and run the  git push   command  #  that  update-all  should have printed", 
            "title": "Step 5: Update the Docker WN client"
        }, 
        {
            "location": "/release/cut-sw-release/#step-6-announce-the-release", 
            "text": "The following instructions are meant for the release manager (or interim release manager). If you are not the release manager, let the release manager know that they can announce the release.    The release manager writes the release announcement and send it out. Here is a sample, replace  HIGHLIGHTED TEXT  with the appropriate values:   Subject: Announcing OSG Software version  VERSION(S) \n\n We are pleased to announce OSG Software version  VERSION(S) !\n\n This is the new OSG Software distributed via RPMs for:\n\n * Scientific Linux 6 and 7\n * CentOS 6 and 7\n * Red Hat Enterprise Linux 6 and 7\n\n This release affects the  SET OF METAPACKAGES (client, compute element, etc...) . Changes include:\n\n * Major change 1\n * Major change 2\n * Major change 3\n\n Release notes and pointers to more documentation can be found at:\n\n  LINK TO RELEASE NOTES \n\n Need help? Let us know:\n\n https://www.opensciencegrid.org/bin/view/Documentation/Release3/HelpProcedure\n\n We welcome feedback on this release    The release manager emails the announcement to  vdt-discuss@opensciencegrid.org   The release manager asks the GOC to distribute the announcement by  opening a ticket  The release manager closes the tickets marked 'Ready for Release' in the release's JIRA filter using the 'bulk change' function. Uncheck the box that reads \"Send mail for this update\"  The release manager updates the recent/scheduled release tables on the Software/Release  homepage", 
            "title": "Step 6: Announce the release"
        }, 
        {
            "location": "/release/cut-data-release/", 
            "text": "Note\n\n\nIf you are performing a software release, please follow the instructions \nhere\n\n\n\n\nHow to Cut a Data Release\n\n\nThis document details the process for releasing new OSG Data Release version(s). This document does NOT discuss the policy for deciding what goes into a release, which can be found \nhere\n.\n\n\nDue to the length of time that this process takes, it is recommended to do the release over three or more days to allow for errors to be corrected and tests to be run.\n\n\nRequirements\n\n\n\n\nUser certificate registered with OSG's koji with build and release team privileges\n\n\nAn account on UW CS machines (e.g. \nlibrary\n, \ningwe\n) to access UW's AFS\n\n\nrelease-tools\n scripts in your \nPATH\n (\nGitHub\n)\n\n\nosg-build\n scripts in your \nPATH\n (installed via OSG yum repos or \nsource\n)\n\n\n\n\nPick the Version and Revision Numbers\n\n\nThe rest of this document makes references to \nREVISION\n and \nVERSION(S)\n , which refer to the space-delimited list of OSG version(s) and data revision, respectively (e.g. \n3.3.28 3.4.3\n and \n2\n, respectively). If you are unsure about either the version or revision, please consult the release manager.\n\n\nDay 0: Generate Preliminary Release List\n\n\nThe release manager often needs a tentative list of packages to be released. This is done by finding the package differences between osg-testing and the current release. Run \n0-generate-pkg-list\n from a machine that has your koji-registered user certificate:\n\n\n[user@client ~] $\n git clone https://github.com/opensciencegrid/release-tools.git\n\n[user@client ~] $\n \ncd\n release-tools\n\n[user@client ~] $\n \n0\n-generate-pkg-list -d \nREVISION\n \nVERSION\n(\nS\n)\n\n\n\n\n\n\nDay 1: Verify Pre-Release\n\n\nThis section is to be performed 1-2 days before the release (as designated by the release manager) to perform last checks of the release.\n\n\nCompare the list of packages already in pre-release to the final list for the release put together by the OSG Release Coordinator (who should have updated \nrelease-list\n in git). To do this, run the \n1-verify-prerelease\n script from git:\n\n\n[user@client ~] $\n \n1\n-verify-prerelease \nVERSION\n(\nS\n)\n\n\n\n\n\n\nIf there are any discrepancies consult the release manager. You may have to tag packages with the \nosg-koji\n tool.\n\n\nDay 2: Pushing the Release\n\n\nFor the second phase of the release, try to complete it earlier in the day rather than later. The GOC would like to send out the release announcement prior to 3 p.m. Eastern time.\n\n\nStep 1: Push from pre-release to release\n\n\nThis script moves the packages into release, clones releases into new version-specific release repos, locks the repos and regenerates them. Afterwards, it produces \n*release-note*\n files that should be used to update the release note pages. Clone it from the github repo and run the script:\n\n\n[user@client ~] $\n \n2\n-create-release -d \nREVISION\n \nVERSION\n(\nS\n)\n\n\n\n\n\n\n\n\n*.txt\n files are also created and it should be verified that they've been moved to \n/p/vdt/public/html/release-info/\n on UW's AFS.\n\n\nFor each release version, use the \n*release-note*\n files to update the relevant sections of the release note pages.\n\n\n\n\nStep 2: Update the Docker WN client\n\n\nUpdate the GitHub repo at \nopensciencegrid/docker-osg-wn\n using the \nupdate-all\n script found in \nopensciencegrid/docker-osg-wn-scripts\n. This requires push access to the \nopensciencegrid/docker-osg-wn\n repo.\n\n\nInstructions for using the script:\n\n\n[user@client ~] $\n git clone git@github.com:opensciencegrid/docker-osg-wn-scripts.git\n\n[user@client ~] $\n git clone git@github.com:opensciencegrid/docker-osg-wn.git\n\n[user@client ~] $\n docker-osg-wn-scripts/update-all docker-osg-wn\n\n[user@client ~] $\n \ncd\n docker-osg-wn\n\n#\n Verify everything looks fine and run the \ngit push\n \ncommand\n\n\n#\n that \nupdate-all\n should have printed\n\n\n\n\n\nStep 3: Verify the VO Package and/or CA certificates\n\n\nWait for the CA certificates to be propagated to the web server on \nrepo.grid.iu.edu\n. The repository is checked every 10 minutes for update CA certificates. Then, run the following command to update the VO Package and/or CA certificates in the tarball installations and verify that the version of the VO Package and/or CA certificates match the version that was promoted to release.\n\n\n[user@client ~] $\n /p/vdt/workspace/tarball-client/current/amd64_rhel6/osgrun osg-update-data\n\n[user@client ~] $\n /p/vdt/workspace/tarball-client/current/amd64_rhel7/osgrun osg-update-data\n\n\n\n\n\nStep 4: Announce the release\n\n\nThe following instructions are meant for the release manager (or interim release manager). If you are not the release manager, let the release manager know that they can announce the release.\n\n\n\n\n\n\nThe release manager writes the release announcement and send it out. Here is a sample, replace \nHIGHLIGHTED TEXT\n with the appropriate values:\n\n\n Subject: Announcing OSG Software version \nVERSION(S)\n\n\n We are pleased to announce OSG Software version \nVERSION(S)\n!\n\n This is the new OSG Software distributed via RPMs for:\n\n * Scientific Linux 6 and 7\n * CentOS 6 and 7\n * Red Hat Enterprise Linux 6 and 7\n\n This release affects the \nSET OF METAPACKAGES (client, compute element, etc...)\n. Changes include:\n\n * Major change 1\n * Major change 2\n * Major change 3\n\n Release notes and pointers to more documentation can be found at:\n\n \nLINK TO RELEASE NOTES\n\n\n Need help? Let us know:\n\n https://www.opensciencegrid.org/bin/view/Documentation/Release3/HelpProcedure\n\n We welcome feedback on this release\n\n\n\n\n\n\n\n\n\nThe release manager emails the announcement to \nvdt-discuss@opensciencegrid.org\n\n\n\n\nThe release manager asks the GOC to distribute the announcement by \nopening a ticket\n\n\nThe release manager closes the tickets marked 'Ready for Release' in the release's JIRA filter using the 'bulk change' function. Uncheck the box that reads \"Send mail for this update\"\n\n\nThe release manager updates the recent/scheduled release tables on the Software/Release \nhomepage", 
            "title": "How to Cut a Data Release"
        }, 
        {
            "location": "/release/cut-data-release/#how-to-cut-a-data-release", 
            "text": "This document details the process for releasing new OSG Data Release version(s). This document does NOT discuss the policy for deciding what goes into a release, which can be found  here .  Due to the length of time that this process takes, it is recommended to do the release over three or more days to allow for errors to be corrected and tests to be run.", 
            "title": "How to Cut a Data Release"
        }, 
        {
            "location": "/release/cut-data-release/#requirements", 
            "text": "User certificate registered with OSG's koji with build and release team privileges  An account on UW CS machines (e.g.  library ,  ingwe ) to access UW's AFS  release-tools  scripts in your  PATH  ( GitHub )  osg-build  scripts in your  PATH  (installed via OSG yum repos or  source )", 
            "title": "Requirements"
        }, 
        {
            "location": "/release/cut-data-release/#pick-the-version-and-revision-numbers", 
            "text": "The rest of this document makes references to  REVISION  and  VERSION(S)  , which refer to the space-delimited list of OSG version(s) and data revision, respectively (e.g.  3.3.28 3.4.3  and  2 , respectively). If you are unsure about either the version or revision, please consult the release manager.", 
            "title": "Pick the Version and Revision Numbers"
        }, 
        {
            "location": "/release/cut-data-release/#day-0-generate-preliminary-release-list", 
            "text": "The release manager often needs a tentative list of packages to be released. This is done by finding the package differences between osg-testing and the current release. Run  0-generate-pkg-list  from a machine that has your koji-registered user certificate:  [user@client ~] $  git clone https://github.com/opensciencegrid/release-tools.git [user@client ~] $   cd  release-tools [user@client ~] $   0 -generate-pkg-list -d  REVISION   VERSION ( S )", 
            "title": "Day 0: Generate Preliminary Release List"
        }, 
        {
            "location": "/release/cut-data-release/#day-1-verify-pre-release", 
            "text": "This section is to be performed 1-2 days before the release (as designated by the release manager) to perform last checks of the release.  Compare the list of packages already in pre-release to the final list for the release put together by the OSG Release Coordinator (who should have updated  release-list  in git). To do this, run the  1-verify-prerelease  script from git:  [user@client ~] $   1 -verify-prerelease  VERSION ( S )   If there are any discrepancies consult the release manager. You may have to tag packages with the  osg-koji  tool.", 
            "title": "Day 1: Verify Pre-Release"
        }, 
        {
            "location": "/release/cut-data-release/#day-2-pushing-the-release", 
            "text": "For the second phase of the release, try to complete it earlier in the day rather than later. The GOC would like to send out the release announcement prior to 3 p.m. Eastern time.", 
            "title": "Day 2: Pushing the Release"
        }, 
        {
            "location": "/release/cut-data-release/#step-1-push-from-pre-release-to-release", 
            "text": "This script moves the packages into release, clones releases into new version-specific release repos, locks the repos and regenerates them. Afterwards, it produces  *release-note*  files that should be used to update the release note pages. Clone it from the github repo and run the script:  [user@client ~] $   2 -create-release -d  REVISION   VERSION ( S )    *.txt  files are also created and it should be verified that they've been moved to  /p/vdt/public/html/release-info/  on UW's AFS.  For each release version, use the  *release-note*  files to update the relevant sections of the release note pages.", 
            "title": "Step 1: Push from pre-release to release"
        }, 
        {
            "location": "/release/cut-data-release/#step-2-update-the-docker-wn-client", 
            "text": "Update the GitHub repo at  opensciencegrid/docker-osg-wn  using the  update-all  script found in  opensciencegrid/docker-osg-wn-scripts . This requires push access to the  opensciencegrid/docker-osg-wn  repo.  Instructions for using the script:  [user@client ~] $  git clone git@github.com:opensciencegrid/docker-osg-wn-scripts.git [user@client ~] $  git clone git@github.com:opensciencegrid/docker-osg-wn.git [user@client ~] $  docker-osg-wn-scripts/update-all docker-osg-wn [user@client ~] $   cd  docker-osg-wn #  Verify everything looks fine and run the  git push   command  #  that  update-all  should have printed", 
            "title": "Step 2: Update the Docker WN client"
        }, 
        {
            "location": "/release/cut-data-release/#step-3-verify-the-vo-package-andor-ca-certificates", 
            "text": "Wait for the CA certificates to be propagated to the web server on  repo.grid.iu.edu . The repository is checked every 10 minutes for update CA certificates. Then, run the following command to update the VO Package and/or CA certificates in the tarball installations and verify that the version of the VO Package and/or CA certificates match the version that was promoted to release.  [user@client ~] $  /p/vdt/workspace/tarball-client/current/amd64_rhel6/osgrun osg-update-data [user@client ~] $  /p/vdt/workspace/tarball-client/current/amd64_rhel7/osgrun osg-update-data", 
            "title": "Step 3: Verify the VO Package and/or CA certificates"
        }, 
        {
            "location": "/release/cut-data-release/#step-4-announce-the-release", 
            "text": "The following instructions are meant for the release manager (or interim release manager). If you are not the release manager, let the release manager know that they can announce the release.    The release manager writes the release announcement and send it out. Here is a sample, replace  HIGHLIGHTED TEXT  with the appropriate values:   Subject: Announcing OSG Software version  VERSION(S) \n\n We are pleased to announce OSG Software version  VERSION(S) !\n\n This is the new OSG Software distributed via RPMs for:\n\n * Scientific Linux 6 and 7\n * CentOS 6 and 7\n * Red Hat Enterprise Linux 6 and 7\n\n This release affects the  SET OF METAPACKAGES (client, compute element, etc...) . Changes include:\n\n * Major change 1\n * Major change 2\n * Major change 3\n\n Release notes and pointers to more documentation can be found at:\n\n  LINK TO RELEASE NOTES \n\n Need help? Let us know:\n\n https://www.opensciencegrid.org/bin/view/Documentation/Release3/HelpProcedure\n\n We welcome feedback on this release    The release manager emails the announcement to  vdt-discuss@opensciencegrid.org   The release manager asks the GOC to distribute the announcement by  opening a ticket  The release manager closes the tickets marked 'Ready for Release' in the release's JIRA filter using the 'bulk change' function. Uncheck the box that reads \"Send mail for this update\"  The release manager updates the recent/scheduled release tables on the Software/Release  homepage", 
            "title": "Step 4: Announce the release"
        }, 
        {
            "location": "/release/release-policy/", 
            "text": "Software Release Policy\n\n\nThis document doesn't talk about technical details of how to do a release. \nThe release process is discussed elsewhere\n.\n\n\nSoftware Repositories\n\n\nThe Software Team maintains five primary software repositories\n\n\n\n\nosg-development\n: This is the \"wild west\", the place where software goes while it is being worked on by the software team.\n\n\nosg-testing\n: This is where software goes when it is ready for wide-spread testing.\n\n\nosg-prerelease\n: This is where software goes just before being released, for final verification.\n\n\nosg-release\n: This is the official, production release of the software stack. This is the main repository for end-users.\n\n\nosg-contrib\n: This is where software goes that is not officially supported by the OSG Software Team, but we provide as a convenience for software our users might find useful.\n\n\n\n\nWe also create a repository per release, called \nosg-release-VERSION\n (such as osg-release-3.0.4). This is intended mostly for testing purposes, though users may occasionally find it useful.\n\n\nOccasionally there may be other repositories for specific short-term purposes.\n\n\nVersion Numbers\n\n\nThere is a single version number that is used to summarize the contents of the \nosg-release\n repository. Having a single version number is very useful for a variety of reasons, including:\n\n\n\n\nEvery time changes are made to the \nosg-release\n repository, we update the version number and write release notes.\n\n\nWe have a shorthand for referring to the state of the repository; we can talk about specific releases.\n\n\n\n\nHowever, there are important caveats about the version number:\n\n\n\n\nEven if a user says they have installed Version X, it may not be an accurate reflection of what they have installed: they may have chosen to update some of their software from a previous version. To truly understand what they have installed, the entire set of RPMs installed on their computer must be considered.\n\n\nThe version number is only meaningful in the \nosg-release\n repository, though for technical reasons it's present (as an RPM) in other repositories.\n\n\n\n\nThe version number is communicated in two ways:\n\n\n\n\nEvery time a new release is made, the version number is updated. All release notes and communication to users about this release uses the new version number.\n\n\nThere is an \nosg-version\n RPM that reports the version of the release. Major metapackages (osg-ce, osg-client, etc...) depend on this RPM. The RPM itself has the version number in it. It also provide a program that reports the version, and a text file that contains the version number.\n\n\n\n\nThe version number will be of the form X.Y.Z. As of this writing, version numbers are 3.0.Z, where Z indicates a minor revision.\n\n\nProgression of repositories\n\n\nThis figure shows the progression of repositories that packages will go through:\n\n\n osg-development -\n osg-testing -\n osg-prerelease -\n osg-release\n                  \\\n                   -\n osg-contrib\n\n\n\n\n\nRelease policies\n\n\nAdding packages to osg-development\n\n\nNew packages will only be added to \nosg-development\n with the permission of the OSG Software Manager. Updates can be done at any time without permission, but developers should be careful if their updates might be significant, particularly if an update might cause series compatibility issues. In cases where there is uncertainty, discuss it with the Software Manager.\n\n\nMoving packages to osg-testing\n\n\nA package may be moved from \nosg-development\n to \nosg-testing\n when the individual maintainer of that package decides that it is ready for widespread testing and when the package is eventually intended for a production release, and when approved by the OSG Software Manager. Approval is needed because this is when we first make packages available to people outside of the OSG Software Team.\n\n\nMoving packages to osg-prerelease; Readying the release\n\n\nWhen we are ready to make a production release, we first move the correct subset of packages from \nosg-testing\n into \nosg-prerelease\n. This should be done after checking with the OSG Release Manager to verify that it's okay to release the software. The intention of \nosg-prerelease\n is to do a final verification that we have the correct set of packages for release and that they really work together. This is important because the \nosg-testing\n repository might contain a mix of packages that are ready for release with packages that are not ready for release. When moving packages to \nosg-prerelease\n, the team member doing the release will:\n\n\n\n\nUpdate the osg-version RPM to reflect the new version. Push this RPM through \nosg-development\n, \nosg-testing\n, and into \nosg-prerelease\n.\n\n\nFind the correct set of packages to push from \nosg-testing\n into \nosg-prerelease\n.\n\n\nAt a minimum, run the automated test suite on the contents of \nosg-prerelease\n. In cases were more extensive testing is needed, or the test suite doesn't sufficiently cover the testing needs, do specific ad-hoc testing. (If appropriate, consider proposing extensions to the automated test suite.)\n\n\n\n\nWe expect that in most cases, this process of updating and testing the \nosg-prelease\n repository will be less than one day. If there are urgent security updates to release, this process may be shortened.\n\n\nNote that, except in exceptional circumstances, we release software on Tuesdays. Therefore the osg-prerelease cache is probably updated and readied on a Monday (or perhaps late the previous week).\n\n\nMoving packages to osg-release\n\n\nNote that, except in exceptional circumstances, we release software on Tuesdays, so this process will only happen on Tuesdays.\n\n\nWhen the \nosg-prerelease\n repository has been updated and verified, all of the changed software can be moved into the \nosg-release\n repository. As part of this move, two important tasks must be done:\n\n\n\n\nRecord the complete set of packages in the new release repository.\n\n\nUpdate the \nRelease Notes\n. Note that each release has a separate page to describe the release, and it's linked from the main page. The individual page lists the changes at a high level (i.e. Updated package X to version Y) and the complete set of RPMs that changed.\n\n\nCreate a ticket on ticket.grid.iu.edu with a release announcement. Operations will distribute it to the right places.\n\n\n\n\nIn addition, we will make another Koji tag/yum repository called \nosg-release-VERSION\n. All of the latest packages in osg-release will be tagged to be in this repository, and the tag will be locked. This will give us a reproducible way to install any given OSG Software release.\n\n\nMoving packages to osg-release-VERSION\n\n\nWhen we make a specific release, we copy the osg-release repository to a versioned osg-release-VERSION repository. This allows us to do testing with specific versions and in rare cases allows users to use a specific release.\n\n\nMoving packages to osg-contrib\n\n\nThe \nosg-contrib\n repository is loosely regulated. In most cases, the team member in charge of the package can decide when a package is updated in \nosg-contrib\n. Contrib packages should be tested in \nosg-development\n first.\n\n\nTiming of releases\n\n\nNormally, releases happen on Tuesdays.\n\n\nCode freezes happen two business days in advance of the release (normally Friday). Specifically: RPM updates intended to be included in the next release (that is, pushed to the osg-release yum repo) must be in the osg-testing yum repo by noon Central Time two business days in advance of the release. This will allow time for final testing, discussions, reverts, etc.\n\n\nWe will make exceptions for urgent situations; consult with the release manager when needed.\n\n\nCA Certificates and VO Client packages\n\n\nPackages that contain only data are not part of the usual release cycle.\nCurrently, these are the CA certificate packages and the VO Client packages.\nUpdates to these packages come from the Security Team and Operations Team, respectively.\nThey still move through the usual process for release, and the Software and Release Managers decide when these packages should be promoted to the next repository level.\nHowever, the actual releases of these packages do not increment the version number of the software stack.\n\n\nThe release process for data packages is discussed here.", 
            "title": "Release Policy"
        }, 
        {
            "location": "/release/release-policy/#software-release-policy", 
            "text": "This document doesn't talk about technical details of how to do a release.  The release process is discussed elsewhere .", 
            "title": "Software Release Policy"
        }, 
        {
            "location": "/release/release-policy/#software-repositories", 
            "text": "The Software Team maintains five primary software repositories   osg-development : This is the \"wild west\", the place where software goes while it is being worked on by the software team.  osg-testing : This is where software goes when it is ready for wide-spread testing.  osg-prerelease : This is where software goes just before being released, for final verification.  osg-release : This is the official, production release of the software stack. This is the main repository for end-users.  osg-contrib : This is where software goes that is not officially supported by the OSG Software Team, but we provide as a convenience for software our users might find useful.   We also create a repository per release, called  osg-release-VERSION  (such as osg-release-3.0.4). This is intended mostly for testing purposes, though users may occasionally find it useful.  Occasionally there may be other repositories for specific short-term purposes.", 
            "title": "Software Repositories"
        }, 
        {
            "location": "/release/release-policy/#version-numbers", 
            "text": "There is a single version number that is used to summarize the contents of the  osg-release  repository. Having a single version number is very useful for a variety of reasons, including:   Every time changes are made to the  osg-release  repository, we update the version number and write release notes.  We have a shorthand for referring to the state of the repository; we can talk about specific releases.   However, there are important caveats about the version number:   Even if a user says they have installed Version X, it may not be an accurate reflection of what they have installed: they may have chosen to update some of their software from a previous version. To truly understand what they have installed, the entire set of RPMs installed on their computer must be considered.  The version number is only meaningful in the  osg-release  repository, though for technical reasons it's present (as an RPM) in other repositories.   The version number is communicated in two ways:   Every time a new release is made, the version number is updated. All release notes and communication to users about this release uses the new version number.  There is an  osg-version  RPM that reports the version of the release. Major metapackages (osg-ce, osg-client, etc...) depend on this RPM. The RPM itself has the version number in it. It also provide a program that reports the version, and a text file that contains the version number.   The version number will be of the form X.Y.Z. As of this writing, version numbers are 3.0.Z, where Z indicates a minor revision.", 
            "title": "Version Numbers"
        }, 
        {
            "location": "/release/release-policy/#progression-of-repositories", 
            "text": "This figure shows the progression of repositories that packages will go through:   osg-development -  osg-testing -  osg-prerelease -  osg-release\n                  \\\n                   -  osg-contrib", 
            "title": "Progression of repositories"
        }, 
        {
            "location": "/release/release-policy/#release-policies", 
            "text": "", 
            "title": "Release policies"
        }, 
        {
            "location": "/release/release-policy/#adding-packages-to-osg-development", 
            "text": "New packages will only be added to  osg-development  with the permission of the OSG Software Manager. Updates can be done at any time without permission, but developers should be careful if their updates might be significant, particularly if an update might cause series compatibility issues. In cases where there is uncertainty, discuss it with the Software Manager.", 
            "title": "Adding packages to osg-development"
        }, 
        {
            "location": "/release/release-policy/#moving-packages-to-osg-testing", 
            "text": "A package may be moved from  osg-development  to  osg-testing  when the individual maintainer of that package decides that it is ready for widespread testing and when the package is eventually intended for a production release, and when approved by the OSG Software Manager. Approval is needed because this is when we first make packages available to people outside of the OSG Software Team.", 
            "title": "Moving packages to osg-testing"
        }, 
        {
            "location": "/release/release-policy/#moving-packages-to-osg-prerelease-readying-the-release", 
            "text": "When we are ready to make a production release, we first move the correct subset of packages from  osg-testing  into  osg-prerelease . This should be done after checking with the OSG Release Manager to verify that it's okay to release the software. The intention of  osg-prerelease  is to do a final verification that we have the correct set of packages for release and that they really work together. This is important because the  osg-testing  repository might contain a mix of packages that are ready for release with packages that are not ready for release. When moving packages to  osg-prerelease , the team member doing the release will:   Update the osg-version RPM to reflect the new version. Push this RPM through  osg-development ,  osg-testing , and into  osg-prerelease .  Find the correct set of packages to push from  osg-testing  into  osg-prerelease .  At a minimum, run the automated test suite on the contents of  osg-prerelease . In cases were more extensive testing is needed, or the test suite doesn't sufficiently cover the testing needs, do specific ad-hoc testing. (If appropriate, consider proposing extensions to the automated test suite.)   We expect that in most cases, this process of updating and testing the  osg-prelease  repository will be less than one day. If there are urgent security updates to release, this process may be shortened.  Note that, except in exceptional circumstances, we release software on Tuesdays. Therefore the osg-prerelease cache is probably updated and readied on a Monday (or perhaps late the previous week).", 
            "title": "Moving packages to osg-prerelease; Readying the release"
        }, 
        {
            "location": "/release/release-policy/#moving-packages-to-osg-release", 
            "text": "Note that, except in exceptional circumstances, we release software on Tuesdays, so this process will only happen on Tuesdays.  When the  osg-prerelease  repository has been updated and verified, all of the changed software can be moved into the  osg-release  repository. As part of this move, two important tasks must be done:   Record the complete set of packages in the new release repository.  Update the  Release Notes . Note that each release has a separate page to describe the release, and it's linked from the main page. The individual page lists the changes at a high level (i.e. Updated package X to version Y) and the complete set of RPMs that changed.  Create a ticket on ticket.grid.iu.edu with a release announcement. Operations will distribute it to the right places.   In addition, we will make another Koji tag/yum repository called  osg-release-VERSION . All of the latest packages in osg-release will be tagged to be in this repository, and the tag will be locked. This will give us a reproducible way to install any given OSG Software release.", 
            "title": "Moving packages to osg-release"
        }, 
        {
            "location": "/release/release-policy/#moving-packages-to-osg-release-version", 
            "text": "When we make a specific release, we copy the osg-release repository to a versioned osg-release-VERSION repository. This allows us to do testing with specific versions and in rare cases allows users to use a specific release.", 
            "title": "Moving packages to osg-release-VERSION"
        }, 
        {
            "location": "/release/release-policy/#moving-packages-to-osg-contrib", 
            "text": "The  osg-contrib  repository is loosely regulated. In most cases, the team member in charge of the package can decide when a package is updated in  osg-contrib . Contrib packages should be tested in  osg-development  first.", 
            "title": "Moving packages to osg-contrib"
        }, 
        {
            "location": "/release/release-policy/#timing-of-releases", 
            "text": "Normally, releases happen on Tuesdays.  Code freezes happen two business days in advance of the release (normally Friday). Specifically: RPM updates intended to be included in the next release (that is, pushed to the osg-release yum repo) must be in the osg-testing yum repo by noon Central Time two business days in advance of the release. This will allow time for final testing, discussions, reverts, etc.  We will make exceptions for urgent situations; consult with the release manager when needed.", 
            "title": "Timing of releases"
        }, 
        {
            "location": "/release/release-policy/#ca-certificates-and-vo-client-packages", 
            "text": "Packages that contain only data are not part of the usual release cycle.\nCurrently, these are the CA certificate packages and the VO Client packages.\nUpdates to these packages come from the Security Team and Operations Team, respectively.\nThey still move through the usual process for release, and the Software and Release Managers decide when these packages should be promoted to the next repository level.\nHowever, the actual releases of these packages do not increment the version number of the software stack.  The release process for data packages is discussed here.", 
            "title": "CA Certificates and VO Client packages"
        }, 
        {
            "location": "/release/release-schedule/", 
            "text": "OSG Software Release Schedule\n\n\nTo help with predictability and align with the GOC update schedule, we have adopted a fixed release schedule. OSG Software will be released on the second Tuesday of the month. Urgent releases could happen on the fourth Tuesday of the month.\n\n\nRelease Cycle Phases\n\n\nVarious \"Freeze\" dates will happen in the weeks leading up to the release. They are as follows:\n\n\n\n\nDevelopment Freeze (end of the day on the Monday two weeks before release):\n\n\nThe remaining testing is planned for this release.\n\n\nPackages are considered for inclusion in the release on this date.\n\n\nSome additional work on the packages may be done later in the day.\n\n\nAll packages to be tested, need to be promoted to the testing repository by the end of the day.\n\n\nWe will schedule testing for all packages at this date.\n\n\nIf a package is promoted after this date, it may not get tested in time for the release.\n\n\n\n\n\n\nPackage Freeze (end of the day on the Monday one week before release):\n\n\nThe intended release set is determined.\n\n\nPackages that have been tested will be promoted to the prerelease repository as they are ready.\n\n\nNew packages that have short predictable testing cycle will be accepted for release.\n\n\n\n\n\n\nPrerelease Freeze (2 PM Central on the Friday before release):\n\n\nLast minute decision of which packages are included.\n\n\nPackages will be run through the automated tests just prior to release.\n\n\nTarball clients will be created just prior to release.\n\n\n\n\n\n\nRelease (second or fourth Tuesday of the month):\n\n\nThe software packages are placed in the release repositories.\n\n\nThe release is announced.\n\n\n\n\n\n\n\n\nIf a freeze date falls on a holiday, it will be moved to the next business day. Exceptions for freezes may be made at the discretion of the Release Manager.\n\n\nExample Schedule\n\n\nExample schedule for the September 2017 release:\n\n\n\n\n\n\n\n\nPhase\n\n\nDate\n\n\n\n\n\n\n\n\n\n\nDevelopment Freeze\n\n\nMonday, 8/28 at 5 PM\n\n\n\n\n\n\nPackage Freeze\n\n\nMonday, 9/5 at 5 PM\n\n\n\n\n\n\nPrerelease Freeze\n\n\nFriday, 9/9 at 2 PM\n\n\n\n\n\n\nRelease\n\n\nTuesday, 9/12", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/release/release-schedule/#osg-software-release-schedule", 
            "text": "To help with predictability and align with the GOC update schedule, we have adopted a fixed release schedule. OSG Software will be released on the second Tuesday of the month. Urgent releases could happen on the fourth Tuesday of the month.", 
            "title": "OSG Software Release Schedule"
        }, 
        {
            "location": "/release/release-schedule/#release-cycle-phases", 
            "text": "Various \"Freeze\" dates will happen in the weeks leading up to the release. They are as follows:   Development Freeze (end of the day on the Monday two weeks before release):  The remaining testing is planned for this release.  Packages are considered for inclusion in the release on this date.  Some additional work on the packages may be done later in the day.  All packages to be tested, need to be promoted to the testing repository by the end of the day.  We will schedule testing for all packages at this date.  If a package is promoted after this date, it may not get tested in time for the release.    Package Freeze (end of the day on the Monday one week before release):  The intended release set is determined.  Packages that have been tested will be promoted to the prerelease repository as they are ready.  New packages that have short predictable testing cycle will be accepted for release.    Prerelease Freeze (2 PM Central on the Friday before release):  Last minute decision of which packages are included.  Packages will be run through the automated tests just prior to release.  Tarball clients will be created just prior to release.    Release (second or fourth Tuesday of the month):  The software packages are placed in the release repositories.  The release is announced.     If a freeze date falls on a holiday, it will be moved to the next business day. Exceptions for freezes may be made at the discretion of the Release Manager.", 
            "title": "Release Cycle Phases"
        }, 
        {
            "location": "/release/release-schedule/#example-schedule", 
            "text": "Example schedule for the September 2017 release:     Phase  Date      Development Freeze  Monday, 8/28 at 5 PM    Package Freeze  Monday, 9/5 at 5 PM    Prerelease Freeze  Friday, 9/9 at 2 PM    Release  Tuesday, 9/12", 
            "title": "Example Schedule"
        }, 
        {
            "location": "/release/new-release-series/", 
            "text": "How to Prepare a New Release Series\n\n\n\n\nWarning\n\n\nThis document was specifically written for OSG 3.4 and will need to be\nadapted to work for 3.5, 3.6, etc.\n\n\nThe information was taken from\n\nSOFTWARE-2622\n\nand\n\nMatyas's notes for the OSG 3.4 release\n.\n\n\n\n\nDo first, anytime before the month of the release\n\n\n\n\n\n\nAdd 3.4 koji tags and targets\n\n\n\n\nModify this script as appropriate and run: \nhttps://github.com/opensciencegrid/osg-next-tools/blob/osg34/koji/create-new-koji-osg34-tags-etc\n\n\nAt first, upcoming-build should continue to inherit from 3.3-devel\n\n\nCreate osg-3.4-elX-bootstrap and have the build tag inherit from it\n\n\n\n\n\n\n\n\nAdd Koji package signing\n\n\n\n\nEdit /etc/koji-hub/plugins/sign.conf; copy and modify one of the other osg entries\n\n\nEnsure the permissions are 0600 apache:apache\n\n\nSave the result with \netckeeper commit\n\n\n\n\n\n\n\n\nUpdate \nosg-build\n to use the new koji tags and targets (not by default of course)\n\n\n\n\nSee the \nGit commits\n on opensciencegrid/osg-build for SOFTWARE-2693 for details on how to do this\n\n\nYou will be using this version of \nosg-build\n for some tasks, even if it hasn't been released\n\n\n\n\n\n\n\n\nDo afterward, anytime before the month of the release\n\n\n\n\n\n\nCreate a blank \nosg-3.4\n SVN branch and add \nbuildsys-macros\n package\n\n\n\n\n\n\nsvn copy \nbuildsys-macros\n from trunk and hand-edit it to hardcode all the new values\n\n\nFor EL 6:\n\n\n\n\nEdit the spec file and set \ndver\n to 6\n\n\nRun the following commands (adjust the NVR as necessary):\n$\n osg-build rpmbuild --el6\n\n$\n osg-koji import _build_results/buildsys-macros-*.el6.src.rpm\n\n$\n osg-koji import _build_results/buildsys-macros-*.el6.noarch.rpm\n\n$\n osg-koji tag-pkg osg-3.4-el6-development buildsys-macros-7-5.osg34.el6\n\n\n\n\n\n\n\n\n\nThen do the same for EL 7:\n\n\n\n\nEdit the spec file and set \ndver\n to 7\n\n\nRun the following commands (adjust the NVR as necessary):\n$\n osg-build rpmbuild --el7\n\n$\n osg-koji import _build_results/buildsys-macros-*.el7.src.rpm\n\n$\n osg-koji import _build_results/buildsys-macros-*.el7.noarch.rpm\n\n$\n osg-koji tag-pkg osg-3.4-el7-development buildsys-macros-7-5.osg34.el7\n\n\n\n\n\n\n\n\n\n\n\n\n\nDo a 'real' build of \nbuildsys-macros\n\n\n\n\n\n\nBump the revision in the \nbuildsys-macros\n spec file and edit the \n%changelog\n.\n    \nAgain, you will need a version of osg-build with 3.4 support.\n\n\n\n\n\n\nSet \ndver\n to 6. Commit\n\n\n$\n osg-build koji --repo\n=\n3\n.4 --el6\n\n\n\n\n\n\n\n\n\nSet \ndver\n to 7. Commit\n\n\n$\n osg-build koji --repo\n=\n3\n.4 --el7\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUpdate \ntarball-client\n scripts\n\n\n\n\nbundles.ini\n\n\npatches/\n\n\nupload-tarballs-to-oasis\n (for 3.4, \nforeach_dver_arch\n will need to be updated because we're dropping i386 support)\n\n\n\n\n\n\n\n\nPopulate the \nbootstrap\n tags\n\n\nNeed to have them inherit from the 3.3 development tags, but only packages, not builds (hence the \n--noconfig\n; yes, the name is weird)\n\n\n$\n \nfor\n el in el6 el7\n;\n \ndo\n \n\\\n\n    \nfor\n repo in \n3\n.3 upcoming\n;\n \ndo\n \n\\\n\n        osg-koji add-tag-inheritance --noconfig --priority\n=\n2\n \n\\\n\n            osg-3.4-\n$el\n-bootstrap osg-\n$repo\n-\n$el\n-development\n;\n \n\\\n\n    \ndone\n;\n \n\\\n\n\ndone\n\n\n\n\n\n\n\n\n\n\nGet the actual NVRs to tag\n\n\n\n\nI put Brian's spreadsheet into Excel and used its filtering feature to separate out:\n\n\nthe packages going into 3.4.0\n\n\nthe el6 versus el7 packages\n\n\n\n\n\n\nand copied the column containing the NVRs into Vim, and did a search\nreplace of the dver to the appropriate version\n\n\nsaved two text files (pkgtotag-el6.txt and pkgtotag-el7.txt)\n\n\n\n\nTagging:\n\n\n$\n \nfor\n el in el6 el7\n;\n \ndo\n \n\\\n\n    xargs -a pkgtotag-\n$el\n osg-koji tag-pkg osg-3.4-\n$el\n-bootstrap\n;\n \n\\\n\n\ndone\n\n\n\n\n\n\n(btw, xargs -a doesn't work on a Mac)\n\n\n\n\n\n\n\n\n\n\nDo on the month of the 3.4.0 release\n\n\n\n\nPopulate SVN branch and tags (as in fill it with the packages we're going to release for 3.4)\n\n\nMass rebuild\n\n\nDon't forget to update the \nempty\n and \ncontrib\n tags with the appropriate packages;\n    \nremove the \nempty*\n packages from the development tags after they've been tagged into the \nempty\n tags\n\n\n\n\n\n\nUpdate mash (coordinate with steige)\n\n\nOn repo-itb\n\n\nOn repo1/repo2\n\n\n\n\n\n\nUpdate documentation at\n    \nhttps://opensciencegrid.github.io/technology/software/development-process/\n\n\nUpdate osg-test / vmu-test-runs\n\n\nThey're only going to test from minefield (and eventurally testing) until the release\n\n\n\n\n\n\n\n\nDo immediately after the 3.4.0 release\n\n\n\n\nUpdate tag inheritance on the \nupcoming-build\n tags to inherit from \n3.4-devel\n instead of \n3.3-devel\n\n\nDrop the \nosg-3.4-elX-bootstrap\n koji tags\n\n\n\n\nDo sometime after the 3.4.0 release\n\n\n\n\nDo these three at the same time:\n\n\nMove the SVN \ntrunk\n to \nbranches/osg-3.3\n and move \nbranches/osg-3.4\n to \ntrunk\n\n\nUpdate the koji \nosg-elX\n build targets to build from and to 3.4 instead of 3.3\n\n\nNotify the software list of this change\n\n\n\n\n\n\nUpdate osg-test / vmu-test-runs again to add release and release -\n testing tests\n\n\n\n\nUpdate the docker-osg-wn-client scripts to build from 3.4 (need direct push access)\n\n\n\n\nUpdate the constants in the \ngenbranches\n script in the \ndocker-osg-wn-scripts\n repo\n\n\n\n\nUpdate the branches in \ndocker-osg-wn-client\n; a script like this ought to work:\n\n\ngit clone git@github.com:opensciencegrid/docker-osg-wn-scripts.git\ngit clone git@github.com:opensciencegrid/docker-osg-wn.git\n\ncd\n docker-osg-wn-scripts\n./genbranches\n\ncd\n ../docker-osg-wn\n\nfor\n bpath in ../docker-osg-wn-scripts/branches/*\n;\n \ndo\n\n    \nb\n=\n${\nbpath\n##*/\n}\n\n    git checkout -b \n$b\n master \n \n\\\n\n        mv \n$bpath\n Dockerfile.in \n \n\\\n\n        git add Dockerfile.in \n \n\\\n\n        git commit -m \nAdd branch \n$b\n\n\ndone\n\n\n\n\n\n\nand then run a similar script to update the existing branches\nCheck the results before pushing, and then run \ngit push --all\n\n\n\n\n\n\nUpdate the arrays in \nupdate-all\n and \nosg-wn-nightly-build\n in \ndocker-osg-wn-scripts\n\n\n\n\n\n\n\n\n\n\nUpdate the default promotion route aliases in \nosg-promote\n\n\n\n\n\n\nUpdate documentation again to reflect that 3.4 is now the \nmain\n branch and 3.3 is the \nmaintenance\n branch:\n    \nhttps://opensciencegrid.github.io/technology/software/development-process/", 
            "title": "New Release Series"
        }, 
        {
            "location": "/release/new-release-series/#how-to-prepare-a-new-release-series", 
            "text": "Warning  This document was specifically written for OSG 3.4 and will need to be\nadapted to work for 3.5, 3.6, etc.  The information was taken from SOFTWARE-2622 \nand Matyas's notes for the OSG 3.4 release .", 
            "title": "How to Prepare a New Release Series"
        }, 
        {
            "location": "/release/new-release-series/#do-first-anytime-before-the-month-of-the-release", 
            "text": "Add 3.4 koji tags and targets   Modify this script as appropriate and run:  https://github.com/opensciencegrid/osg-next-tools/blob/osg34/koji/create-new-koji-osg34-tags-etc  At first, upcoming-build should continue to inherit from 3.3-devel  Create osg-3.4-elX-bootstrap and have the build tag inherit from it     Add Koji package signing   Edit /etc/koji-hub/plugins/sign.conf; copy and modify one of the other osg entries  Ensure the permissions are 0600 apache:apache  Save the result with  etckeeper commit     Update  osg-build  to use the new koji tags and targets (not by default of course)   See the  Git commits  on opensciencegrid/osg-build for SOFTWARE-2693 for details on how to do this  You will be using this version of  osg-build  for some tasks, even if it hasn't been released", 
            "title": "Do first, anytime before the month of the release"
        }, 
        {
            "location": "/release/new-release-series/#do-afterward-anytime-before-the-month-of-the-release", 
            "text": "Create a blank  osg-3.4  SVN branch and add  buildsys-macros  package    svn copy  buildsys-macros  from trunk and hand-edit it to hardcode all the new values  For EL 6:   Edit the spec file and set  dver  to 6  Run the following commands (adjust the NVR as necessary): $  osg-build rpmbuild --el6 $  osg-koji import _build_results/buildsys-macros-*.el6.src.rpm $  osg-koji import _build_results/buildsys-macros-*.el6.noarch.rpm $  osg-koji tag-pkg osg-3.4-el6-development buildsys-macros-7-5.osg34.el6    Then do the same for EL 7:   Edit the spec file and set  dver  to 7  Run the following commands (adjust the NVR as necessary): $  osg-build rpmbuild --el7 $  osg-koji import _build_results/buildsys-macros-*.el7.src.rpm $  osg-koji import _build_results/buildsys-macros-*.el7.noarch.rpm $  osg-koji tag-pkg osg-3.4-el7-development buildsys-macros-7-5.osg34.el7      Do a 'real' build of  buildsys-macros    Bump the revision in the  buildsys-macros  spec file and edit the  %changelog .\n     Again, you will need a version of osg-build with 3.4 support.    Set  dver  to 6. Commit  $  osg-build koji --repo = 3 .4 --el6    Set  dver  to 7. Commit  $  osg-build koji --repo = 3 .4 --el7        Update  tarball-client  scripts   bundles.ini  patches/  upload-tarballs-to-oasis  (for 3.4,  foreach_dver_arch  will need to be updated because we're dropping i386 support)     Populate the  bootstrap  tags  Need to have them inherit from the 3.3 development tags, but only packages, not builds (hence the  --noconfig ; yes, the name is weird)  $   for  el in el6 el7 ;   do   \\ \n     for  repo in  3 .3 upcoming ;   do   \\ \n        osg-koji add-tag-inheritance --noconfig --priority = 2   \\ \n            osg-3.4- $el -bootstrap osg- $repo - $el -development ;   \\ \n     done ;   \\  done     Get the actual NVRs to tag   I put Brian's spreadsheet into Excel and used its filtering feature to separate out:  the packages going into 3.4.0  the el6 versus el7 packages    and copied the column containing the NVRs into Vim, and did a search replace of the dver to the appropriate version  saved two text files (pkgtotag-el6.txt and pkgtotag-el7.txt)   Tagging:  $   for  el in el6 el7 ;   do   \\ \n    xargs -a pkgtotag- $el  osg-koji tag-pkg osg-3.4- $el -bootstrap ;   \\  done   (btw, xargs -a doesn't work on a Mac)", 
            "title": "Do afterward, anytime before the month of the release"
        }, 
        {
            "location": "/release/new-release-series/#do-on-the-month-of-the-340-release", 
            "text": "Populate SVN branch and tags (as in fill it with the packages we're going to release for 3.4)  Mass rebuild  Don't forget to update the  empty  and  contrib  tags with the appropriate packages;\n     remove the  empty*  packages from the development tags after they've been tagged into the  empty  tags    Update mash (coordinate with steige)  On repo-itb  On repo1/repo2    Update documentation at\n     https://opensciencegrid.github.io/technology/software/development-process/  Update osg-test / vmu-test-runs  They're only going to test from minefield (and eventurally testing) until the release", 
            "title": "Do on the month of the 3.4.0 release"
        }, 
        {
            "location": "/release/new-release-series/#do-immediately-after-the-340-release", 
            "text": "Update tag inheritance on the  upcoming-build  tags to inherit from  3.4-devel  instead of  3.3-devel  Drop the  osg-3.4-elX-bootstrap  koji tags", 
            "title": "Do immediately after the 3.4.0 release"
        }, 
        {
            "location": "/release/new-release-series/#do-sometime-after-the-340-release", 
            "text": "Do these three at the same time:  Move the SVN  trunk  to  branches/osg-3.3  and move  branches/osg-3.4  to  trunk  Update the koji  osg-elX  build targets to build from and to 3.4 instead of 3.3  Notify the software list of this change    Update osg-test / vmu-test-runs again to add release and release -  testing tests   Update the docker-osg-wn-client scripts to build from 3.4 (need direct push access)   Update the constants in the  genbranches  script in the  docker-osg-wn-scripts  repo   Update the branches in  docker-osg-wn-client ; a script like this ought to work:  git clone git@github.com:opensciencegrid/docker-osg-wn-scripts.git\ngit clone git@github.com:opensciencegrid/docker-osg-wn.git cd  docker-osg-wn-scripts\n./genbranches cd  ../docker-osg-wn for  bpath in ../docker-osg-wn-scripts/branches/* ;   do \n     b = ${ bpath ##*/ } \n    git checkout -b  $b  master    \\ \n        mv  $bpath  Dockerfile.in    \\ \n        git add Dockerfile.in    \\ \n        git commit -m  Add branch  $b  done   and then run a similar script to update the existing branches\nCheck the results before pushing, and then run  git push --all    Update the arrays in  update-all  and  osg-wn-nightly-build  in  docker-osg-wn-scripts      Update the default promotion route aliases in  osg-promote    Update documentation again to reflect that 3.4 is now the  main  branch and 3.3 is the  maintenance  branch:\n     https://opensciencegrid.github.io/technology/software/development-process/", 
            "title": "Do sometime after the 3.4.0 release"
        }, 
        {
            "location": "/release/old-release-removal/", 
            "text": "Old Release Series Removal Plan\n\n\nIn order to reduce clutter and disk usage on our repositories and build system,\nwe will remove older OSG Software release series.  This will result in packages\nfrom those series becoming unavailable, so we will remove a release series when\nits packages are no longer needed.\n\n\nWe will remove a release series when the \nfollowing\n series is completely out\nof support.  For example, OSG 3.1 will be removed when OSG 3.2 is out of\nsupport, and OSG 3.2 will be removed when OSG 3.3 is out of support.\n\n\nTasks\n\n\nRemoving a release series requires work from both Operations and Software \n\nRelease.  The first step is to create a JIRA ticket in the SOFTWARE project to\ntrack the work.\n\n\nOperations tasks should be completed before Software \n Release tasks.\n\n\nOperations\n\n\nThese tasks should be completed in order.\n\n\n\n\n\n\nTwo weeks in advance, notify sites (including mirror sites) that the\n    release series is going away.\n\n\n\n\n\n\nRemove the series from the mash configs on the repo.grid.iu.edu machines.\n\n\n\n\n\n\nRemove the appropriate mirrorlist directories from \n/usr/local/mirror/osg\n.\n\n\n\n\n\n\nRemove the appropriate repo directories from \n/usr/local/repo/osg\n.\n\n\n\n\n\n\nWait for mash to run and verify that the repos are no longer getting\n    updated:\n\n\n\n\nLook at the mash logs in \n/var/log/repo\n.\n\n\nVerify that mash did not recreate the repo directory under\n   \n/usr/local/repo/osg\n corresponding to the old release series.\n\n\n\n\n\n\n\n\nSoftware \n Release\n\n\nThese tasks can be completed in any order.\n\n\n\n\n\n\nTag and remove the SVN branch corresponding to the release series.\n\n\n\n\n\n\nEdit \nvm-test-runs\n and remove any \"long tail\" tests that reference the\n  series.\n\n\n\n\n\n\nEdit \ntarball-client\n:\n\n\n\n\n\n\nRemove bundles from \nbundles.ini\n.\n\n\n\n\nRemove patch and other files that were used only by those bundles.\n\n\n\n\nTest that the current bundles didn't get broken by your changes.\n\n\n\n\n\n\nEdit \nosg-build\n:\n\n\n\n\n\n\nRemove the promotion routes from \npromoter.ini\n.\n\n\n\n\nRemove references in \nconstants.py\n.\n\n\n\n\nTest your changes; also run the unit tests.\n\n\n\n\n\n\nRemove things from Koji:\n\n\n\n\n\n\nAll targets referencing the series.\n\n\n\n\n\n\nAll tags referencing the series.\n\n\n\n\n\n\nRemove references to the series from \nopensciencegrid/docker-osg-wn-scripts\n\n  on GitHub, including the \ngenbranches\n and \nupdate-all\n scripts.\n\n\n\n\n\n\nRemove branches from \nopensciencegrid/docker-osg-wn\n on GitHub.\n\n\n\n\n\n\nMove files in \n/p/vdt/public/html/release-info\n to its \nattic\n subdirectory.\n\n\n\n\n\n\nUndoing\n\n\nIf we really need RPMs from a removed release series, we can look at the text\nfiles in \n/p/vdt/public/html/release-info/attic\n to determine the exact NVRs we\nneed, and download them from Koji.", 
            "title": "Old Release Series Removal"
        }, 
        {
            "location": "/release/old-release-removal/#old-release-series-removal-plan", 
            "text": "In order to reduce clutter and disk usage on our repositories and build system,\nwe will remove older OSG Software release series.  This will result in packages\nfrom those series becoming unavailable, so we will remove a release series when\nits packages are no longer needed.  We will remove a release series when the  following  series is completely out\nof support.  For example, OSG 3.1 will be removed when OSG 3.2 is out of\nsupport, and OSG 3.2 will be removed when OSG 3.3 is out of support.", 
            "title": "Old Release Series Removal Plan"
        }, 
        {
            "location": "/release/old-release-removal/#tasks", 
            "text": "Removing a release series requires work from both Operations and Software  \nRelease.  The first step is to create a JIRA ticket in the SOFTWARE project to\ntrack the work.  Operations tasks should be completed before Software   Release tasks.", 
            "title": "Tasks"
        }, 
        {
            "location": "/release/old-release-removal/#operations", 
            "text": "These tasks should be completed in order.    Two weeks in advance, notify sites (including mirror sites) that the\n    release series is going away.    Remove the series from the mash configs on the repo.grid.iu.edu machines.    Remove the appropriate mirrorlist directories from  /usr/local/mirror/osg .    Remove the appropriate repo directories from  /usr/local/repo/osg .    Wait for mash to run and verify that the repos are no longer getting\n    updated:   Look at the mash logs in  /var/log/repo .  Verify that mash did not recreate the repo directory under\n    /usr/local/repo/osg  corresponding to the old release series.", 
            "title": "Operations"
        }, 
        {
            "location": "/release/old-release-removal/#software-release", 
            "text": "These tasks can be completed in any order.    Tag and remove the SVN branch corresponding to the release series.    Edit  vm-test-runs  and remove any \"long tail\" tests that reference the\n  series.    Edit  tarball-client :    Remove bundles from  bundles.ini .   Remove patch and other files that were used only by those bundles.   Test that the current bundles didn't get broken by your changes.    Edit  osg-build :    Remove the promotion routes from  promoter.ini .   Remove references in  constants.py .   Test your changes; also run the unit tests.    Remove things from Koji:    All targets referencing the series.    All tags referencing the series.    Remove references to the series from  opensciencegrid/docker-osg-wn-scripts \n  on GitHub, including the  genbranches  and  update-all  scripts.    Remove branches from  opensciencegrid/docker-osg-wn  on GitHub.    Move files in  /p/vdt/public/html/release-info  to its  attic  subdirectory.", 
            "title": "Software &amp; Release"
        }, 
        {
            "location": "/release/old-release-removal/#undoing", 
            "text": "If we really need RPMs from a removed release series, we can look at the text\nfiles in  /p/vdt/public/html/release-info/attic  to determine the exact NVRs we\nneed, and download them from Koji.", 
            "title": "Undoing"
        }, 
        {
            "location": "/release/condor-itb-testing/", 
            "text": "Testing HTCondor Prereleases on the Madison ITB Site\n\n\nThis document contains a basic recipe for testing an HTCondor prerelease build on the Madison ITB site.\n\n\nPrerequisites\n\n\nThe following items are known prerequisites to using this recipe.  If you are not running the Ansible commands from\nosghost, there are almost certainly other prerequisites that are not listed below.  And even using osghost for Ansible\nand itb-submit for the submissions, there may be other prerequisites missing.  Please improve this document by adding\nother prerequisites as they are identified!\n\n\n\n\nA checkout of the osgitb directory from our local git instance (not GitHub)\n\n\nYour X.509 DN in the \nosgitb/unmanaged/htcondor/condor_mapfile\n file and (via Ansible) on \nitb-ce1\n and \nitb-ce2\n\n\n\n\nGather Information\n\n\nTechnically skippable, this section is about checking on the state of the ITB machines before making changes.  The plan\nis to keep the ITB machines generally up-to-date independently, so those steps are not listed here.  And honestly, the\nsteps below are just some ideas; do whatever makes sense for the given update.\n\n\nThe commands can be run as-is from within the \nosgitb\n directory from git.\n\n\n\n\n\n\nCheck OS versions for all current ITB hosts:\n\n\nansible current -i inventory -f 20 -o -m command -a \ncat /etc/redhat-release\n\n\n\n\n\n\n\n\n\n\nCheck HTCondor versions for all HTCondor hosts:\n\n\nansible condor -i inventory -f 20 -o -m command -a \nrpm -q condor\n\n\n\n\n\n\n\n\n\n\nObtain the NVR of the HTCondor prerelease build from OSG to test.  Do this by talking to Tim\nT. and checking\n   Koji.  The expectation is that the HTCondor prerelease build will be in the development repository (or\n   upcoming-development).\n\n\n\n\n\n\nInstall HTCondor Prerelease\n\n\n\n\n\n\nShut down HTCondor and HTCondor-CE on prerelease machines:\n\n\nansible condordev -i inventory -bK -f 20 -m service -a \nname=condor-ce state=stopped\n -l \nitb-ce*\n\n\nansible condordev -i inventory -bK -f 20 -m service -a \nname=condor state=stopped\n\n\n\n\n\n\n\n\n\n\nInstall new version of HTCondor on prerelease machines:\n\n\nansible condordev -i inventory -bK -f 10 -m command -a \nyum --enablerepo=osg-development --assumeyes update condor\n\n\n\n\n\n\nor, if you need to install an NVR that is \u201cearlier\u201d (in the RPM sense) than what is currently installed:\n\n\nansible condordev -i inventory -bK -f 10 -m command -a \nyum --enablerepo=osg-development --assumeyes downgrade condor condor-classads condor-python condor-procd blahp\n\n\n\n\n\n\n\n\n\n\nVerify installation of correct RPM version:\n\n\nansible condor -i inventory -f 20 -o -m command -a \nrpm -q condor\n\n\n\n\n\n\n\n\n\n\nRestart HTCondor and HTCondor-CE on prerelease machines:\n\n\nansible condordev -i inventory -bK -f 20 -m service -a \nname=condor state=started\n\n\nansible condordev -i inventory -bK -f 20 -m service -a \nname=condor-ce state=started\n -l \nitb-ce*\n\n\n\n\n\n\n\n\n\n\nRun Tests\n\n\nFor the first two test workflows, use your personal space on \nitb-submit\n.  Copy or checkout the \nosgitb/htcondor-tests\n\ndirectory to get the test directories.\n\n\nSubmitting jobs directly\n\n\n\n\n\n\nChange into the \n1-direct-jobs\n subdirectory\n\n\n\n\n\n\nIf there are old result files in the directory, remove them:\n\n\nmake distclean\n\n\n\n\n\n\n\n\n\n\nSubmit the test workflow\n\n\ncondor_submit_dag test.dag\n\n\n\n\n\n\n\n\n\n\nMonitor the jobs until they are complete or stuck\n\n\nIn the initial test runs, the entire workflow ran in a few minutes.  If the DAG or jobs exit immediately, go on\nhold, or otherwise fail, then you have some troubleshooting to do!  Keep trying steps 2 and 3 until you get a clean\nrun (or one or more HTCondor bug tickets).\n\n\n\n\n\n\nCheck the final output file:\n\n\ncat count-by-hostnames.txt\n\n\n\n\n\n\nYou should see a reasonable distribution of jobs by hostname, keeping in mind the different number of cores per\nmachine and the fact that HTCondor can and will reuse claims to process many jobs on a single host.  Especially\nwatch out for a case in which no jobs run on the newly updated hosts (at the time of writing: \nitb-data[456]\n).\n\n\n\n\n\n\n(Optional) Clean up, using the \nmake clean\n or \nmake distclean\n commands.  Use the \nclean\n target to remove\n   intermediate result and log files generated by a workflow run but preserve the final output file; use the \ndistclean\n\n   target to remove all workflow-generated files (plus Emacs backup files).\n\n\n\n\n\n\nSubmitting jobs using HTCondor-C\n\n\nIf direct submissions fail, there is probably no point to doing this step.\n\n\n\n\n\n\nChange into the \n2-htcondor-c-jobs\n subdirectory\n\n\n\n\n\n\nIf there are old result files in the directory, remove them:\n\n\nmake distclean\n\n\n\n\n\n\n\n\n\n\nGet a proxy for your X.509 credentials\n\n\nvoms-proxy-init\n\n\n\n\n\n\n\n\n\n\nSubmit the test workflow\n\n\ncondor_submit_dag test.dag\n\n\n\n\n\n\n\n\n\n\nMonitor the jobs until they are complete or stuck\n\n\nIn the initial test runs, the entire workflow ran in 10 minutes or less; generally, this test takes longer than the\ndirect submission test, because of the layers of indirection.  Also, status updates from the CEs back to the submit\nhost are infrequent.  For direct information about the CEs, log in to \nitb-ce1\n and \nitb-ce2\n to check status; don\u2019t\nforget to check both \ncondor_ce_q\n and \ncondor_q\n on the CEs, probably in that order.\n\n\nIf the DAG or jobs exit immediately, go on hold, or otherwise fail, then you have some troubleshooting to do!  Keep\ntrying steps 2 and 3 until you get a clean run (or one or more HTCondor bug tickets).\n\n\n\n\n\n\nCheck the final output file:\n\n\ncat count-by-hostnames.txt\n\n\n\n\n\n\nAgain, look for a reasonable distribution of jobs by hostname.\n\n\n\n\n\n\n(Optional) Clean up, using the \nmake clean\n or \nmake distclean\n commands.\n\n\n\n\n\n\nSubmitting jobs from a GlideinWMS VO Frontend\n\n\nFor this workflow, use your personal space on \nglidein3.chtc.wisc.edu\n.  Copy or checkout the \nosgitb/htcondor-tests\n\ndirectory to get the test directories.  Again, if previous steps fail, do not bother with this step.\n\n\n\n\n\n\nChange into the \n3-frontend-jobs\n subdirectory\n\n\n\n\n\n\nIf there are old result files in the directory, remove them:\n\n\nmake distclean\n\n\n\n\n\n\n\n\n\n\nSubmit the test workflow\n\n\ncondor_submit_dag test.dag\n\n\n\n\n\n\n\n\n\n\nMonitor the jobs until they are complete or stuck\n\n\nThis workflow could take much longer than the first two, maybe an hour or so.  Also, unless there are active\nglideins, it will take 10 minutes or longer for the first glideins to appear and start matching jobs.  Thus it is\nhelpful to monitor \ncondor_q -totals\n until all of the jobs are submitted (there should be 2001), then switch to\nmonitoring \ncondor_status\n until glideins start appearing.  After the first jobs start running and finishing, it is\nprobably safe to ignore the rest of the run.  If the jobs do not appear in the local queue, if glideins do not\nappear, or if jobs do not start running on the glideins, it is time to start troubleshooting.\n\n\n\n\n\n\nCheck the final output file:\n\n\ncat count-by-hostnames.txt\n\n\n\n\n\n\nThe distribution of jobs per execute node may be more skewed than in the first two workflows, due to the way in\nwhich pilots ramp up over time and how HTCondor allocates jobs to slots.\n\n\n\n\n\n\n(Optional) Clean up, using the \nmake clean\n or \nmake distclean\n commands.", 
            "title": "HTCondor Prerelease Testing"
        }, 
        {
            "location": "/release/condor-itb-testing/#testing-htcondor-prereleases-on-the-madison-itb-site", 
            "text": "This document contains a basic recipe for testing an HTCondor prerelease build on the Madison ITB site.", 
            "title": "Testing HTCondor Prereleases on the Madison ITB Site"
        }, 
        {
            "location": "/release/condor-itb-testing/#prerequisites", 
            "text": "The following items are known prerequisites to using this recipe.  If you are not running the Ansible commands from\nosghost, there are almost certainly other prerequisites that are not listed below.  And even using osghost for Ansible\nand itb-submit for the submissions, there may be other prerequisites missing.  Please improve this document by adding\nother prerequisites as they are identified!   A checkout of the osgitb directory from our local git instance (not GitHub)  Your X.509 DN in the  osgitb/unmanaged/htcondor/condor_mapfile  file and (via Ansible) on  itb-ce1  and  itb-ce2", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/release/condor-itb-testing/#gather-information", 
            "text": "Technically skippable, this section is about checking on the state of the ITB machines before making changes.  The plan\nis to keep the ITB machines generally up-to-date independently, so those steps are not listed here.  And honestly, the\nsteps below are just some ideas; do whatever makes sense for the given update.  The commands can be run as-is from within the  osgitb  directory from git.    Check OS versions for all current ITB hosts:  ansible current -i inventory -f 20 -o -m command -a  cat /etc/redhat-release     Check HTCondor versions for all HTCondor hosts:  ansible condor -i inventory -f 20 -o -m command -a  rpm -q condor     Obtain the NVR of the HTCondor prerelease build from OSG to test.  Do this by talking to Tim T. and checking\n   Koji.  The expectation is that the HTCondor prerelease build will be in the development repository (or\n   upcoming-development).", 
            "title": "Gather Information"
        }, 
        {
            "location": "/release/condor-itb-testing/#install-htcondor-prerelease", 
            "text": "Shut down HTCondor and HTCondor-CE on prerelease machines:  ansible condordev -i inventory -bK -f 20 -m service -a  name=condor-ce state=stopped  -l  itb-ce*  ansible condordev -i inventory -bK -f 20 -m service -a  name=condor state=stopped     Install new version of HTCondor on prerelease machines:  ansible condordev -i inventory -bK -f 10 -m command -a  yum --enablerepo=osg-development --assumeyes update condor   or, if you need to install an NVR that is \u201cearlier\u201d (in the RPM sense) than what is currently installed:  ansible condordev -i inventory -bK -f 10 -m command -a  yum --enablerepo=osg-development --assumeyes downgrade condor condor-classads condor-python condor-procd blahp     Verify installation of correct RPM version:  ansible condor -i inventory -f 20 -o -m command -a  rpm -q condor     Restart HTCondor and HTCondor-CE on prerelease machines:  ansible condordev -i inventory -bK -f 20 -m service -a  name=condor state=started  ansible condordev -i inventory -bK -f 20 -m service -a  name=condor-ce state=started  -l  itb-ce*", 
            "title": "Install HTCondor Prerelease"
        }, 
        {
            "location": "/release/condor-itb-testing/#run-tests", 
            "text": "For the first two test workflows, use your personal space on  itb-submit .  Copy or checkout the  osgitb/htcondor-tests \ndirectory to get the test directories.", 
            "title": "Run Tests"
        }, 
        {
            "location": "/release/condor-itb-testing/#submitting-jobs-directly", 
            "text": "Change into the  1-direct-jobs  subdirectory    If there are old result files in the directory, remove them:  make distclean     Submit the test workflow  condor_submit_dag test.dag     Monitor the jobs until they are complete or stuck  In the initial test runs, the entire workflow ran in a few minutes.  If the DAG or jobs exit immediately, go on\nhold, or otherwise fail, then you have some troubleshooting to do!  Keep trying steps 2 and 3 until you get a clean\nrun (or one or more HTCondor bug tickets).    Check the final output file:  cat count-by-hostnames.txt   You should see a reasonable distribution of jobs by hostname, keeping in mind the different number of cores per\nmachine and the fact that HTCondor can and will reuse claims to process many jobs on a single host.  Especially\nwatch out for a case in which no jobs run on the newly updated hosts (at the time of writing:  itb-data[456] ).    (Optional) Clean up, using the  make clean  or  make distclean  commands.  Use the  clean  target to remove\n   intermediate result and log files generated by a workflow run but preserve the final output file; use the  distclean \n   target to remove all workflow-generated files (plus Emacs backup files).", 
            "title": "Submitting jobs directly"
        }, 
        {
            "location": "/release/condor-itb-testing/#submitting-jobs-using-htcondor-c", 
            "text": "If direct submissions fail, there is probably no point to doing this step.    Change into the  2-htcondor-c-jobs  subdirectory    If there are old result files in the directory, remove them:  make distclean     Get a proxy for your X.509 credentials  voms-proxy-init     Submit the test workflow  condor_submit_dag test.dag     Monitor the jobs until they are complete or stuck  In the initial test runs, the entire workflow ran in 10 minutes or less; generally, this test takes longer than the\ndirect submission test, because of the layers of indirection.  Also, status updates from the CEs back to the submit\nhost are infrequent.  For direct information about the CEs, log in to  itb-ce1  and  itb-ce2  to check status; don\u2019t\nforget to check both  condor_ce_q  and  condor_q  on the CEs, probably in that order.  If the DAG or jobs exit immediately, go on hold, or otherwise fail, then you have some troubleshooting to do!  Keep\ntrying steps 2 and 3 until you get a clean run (or one or more HTCondor bug tickets).    Check the final output file:  cat count-by-hostnames.txt   Again, look for a reasonable distribution of jobs by hostname.    (Optional) Clean up, using the  make clean  or  make distclean  commands.", 
            "title": "Submitting jobs using HTCondor-C"
        }, 
        {
            "location": "/release/condor-itb-testing/#submitting-jobs-from-a-glideinwms-vo-frontend", 
            "text": "For this workflow, use your personal space on  glidein3.chtc.wisc.edu .  Copy or checkout the  osgitb/htcondor-tests \ndirectory to get the test directories.  Again, if previous steps fail, do not bother with this step.    Change into the  3-frontend-jobs  subdirectory    If there are old result files in the directory, remove them:  make distclean     Submit the test workflow  condor_submit_dag test.dag     Monitor the jobs until they are complete or stuck  This workflow could take much longer than the first two, maybe an hour or so.  Also, unless there are active\nglideins, it will take 10 minutes or longer for the first glideins to appear and start matching jobs.  Thus it is\nhelpful to monitor  condor_q -totals  until all of the jobs are submitted (there should be 2001), then switch to\nmonitoring  condor_status  until glideins start appearing.  After the first jobs start running and finishing, it is\nprobably safe to ignore the rest of the run.  If the jobs do not appear in the local queue, if glideins do not\nappear, or if jobs do not start running on the glideins, it is time to start troubleshooting.    Check the final output file:  cat count-by-hostnames.txt   The distribution of jobs per execute node may be more skewed than in the first two workflows, due to the way in\nwhich pilots ramp up over time and how HTCondor allocates jobs to slots.    (Optional) Clean up, using the  make clean  or  make distclean  commands.", 
            "title": "Submitting jobs from a GlideinWMS VO Frontend"
        }, 
        {
            "location": "/release/testing-vo-package/", 
            "text": "Testing VO Package releases\n\n\nThis document contains a basic recipe for testing a VO Package release\n\n\nPrerequisites\n\n\nTesting the VO package requires a few components:\n   * X.509 certificate with membership to at least one VO\n   * System with working GUMS installation\n   * System with OSG installation (voms-proxy-init and edg-mkgridmap)\n\n\nTesting \nvoms-proxy-init\n\n\nLogin in the system that has voms-proxy-init installed.  Make sure that you have the correct \nvo-client rpms installed and that your X.509 certificate is in your home directory.  For each\nVO that you have membership in, run the following \nvoms-proxy-init -voms [VO]\n where \n[VO]\n is \nthe appropriate VO (e.g. osg, cms, etc.).  You should be able to generate a voms-proxy for that\nVO without errors.\n\n\nTesting \nedg-mkgridmap\n\n\nLog on to a system with \nedg-mkgridmap\n installed.  Make sure you have the correct vo-client rpms \ninstalled (vo-client-edgmkgridmap).  Run \nedg-mkgridmap\n and check the log output for errors.\n\nThere will be some errors so compare your errors with the errors on previous vo-package tickets to\nmake sure no new errors have appeared.\n\n\nTesting GUMS\n\n\nLog on to a system with a working GUMS install.  Make sure that you have the correct vo-client \nrpms (osg-gums-config) installed.  \n\n\n\n\nMake a backup of \n/etc/gums/gums.config\n \n\n\nCopy the mysql database information from \n/etc/gums/gums.config\n to \n/etc/gums/gums.config.template\n\n\nCopy \n/etc/gums/gums.config.template\n to \n/etc/gums/gums.config\n\n\nStart the \ntomcat6\n service \n\n\nGo to the GUMS interface (e.g. https://my.host:8443/gums)\n\n\nGo to the Update VO members page and click on the \nupdate VO members\n button\n\n\nOnce completed, there will probably be some errors.\n\n\nCompare errors to errors on prior vo package update tickets and make sure no new errors have occurred.", 
            "title": "VO Package Testing"
        }, 
        {
            "location": "/release/testing-vo-package/#testing-vo-package-releases", 
            "text": "This document contains a basic recipe for testing a VO Package release", 
            "title": "Testing VO Package releases"
        }, 
        {
            "location": "/release/testing-vo-package/#prerequisites", 
            "text": "Testing the VO package requires a few components:\n   * X.509 certificate with membership to at least one VO\n   * System with working GUMS installation\n   * System with OSG installation (voms-proxy-init and edg-mkgridmap)", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/release/testing-vo-package/#testing-voms-proxy-init", 
            "text": "Login in the system that has voms-proxy-init installed.  Make sure that you have the correct \nvo-client rpms installed and that your X.509 certificate is in your home directory.  For each\nVO that you have membership in, run the following  voms-proxy-init -voms [VO]  where  [VO]  is \nthe appropriate VO (e.g. osg, cms, etc.).  You should be able to generate a voms-proxy for that\nVO without errors.", 
            "title": "Testing voms-proxy-init"
        }, 
        {
            "location": "/release/testing-vo-package/#testing-edg-mkgridmap", 
            "text": "Log on to a system with  edg-mkgridmap  installed.  Make sure you have the correct vo-client rpms \ninstalled (vo-client-edgmkgridmap).  Run  edg-mkgridmap  and check the log output for errors. \nThere will be some errors so compare your errors with the errors on previous vo-package tickets to\nmake sure no new errors have appeared.", 
            "title": "Testing edg-mkgridmap"
        }, 
        {
            "location": "/release/testing-vo-package/#testing-gums", 
            "text": "Log on to a system with a working GUMS install.  Make sure that you have the correct vo-client \nrpms (osg-gums-config) installed.     Make a backup of  /etc/gums/gums.config    Copy the mysql database information from  /etc/gums/gums.config  to  /etc/gums/gums.config.template  Copy  /etc/gums/gums.config.template  to  /etc/gums/gums.config  Start the  tomcat6  service   Go to the GUMS interface (e.g. https://my.host:8443/gums)  Go to the Update VO members page and click on the  update VO members  button  Once completed, there will probably be some errors.  Compare errors to errors on prior vo package update tickets and make sure no new errors have occurred.", 
            "title": "Testing GUMS"
        }, 
        {
            "location": "/policy/gums-retire/", 
            "text": "GUMS Retirement\n\n\nThis document provides an overview of the planned retirement of support for GUMS in the OSG Software Stack.\n\n\nIntroduction\n\n\nGUMS (Grid User Management System) is an authentication system used by OSG resource providers to map grid credentials to\nlocal UNIX accounts. It provides OSG site adminstrators with a centrally managed service that can handle requests from\nmultiple hosts that require authentication e.g., HTCondor-CE, GridFTP, and XRootD servers. In discussion with the OSG\ncommunity, we have found that sites use the following GUMS features:\n\n\n\n\nMapping based on VOMS attributes\n\n\nHost-based mappings\n\n\nBanning users/VOs\n\n\nSupporting pool accounts\n\n\n\n\nGUMS is a large Java web application that is more complex than necessary for the subset of features used in the\nOSG. Additionally, upstream support has tailed off and as a result, the maintenance burden has largely fallen on the OSG\nSoftware team.\n\n\nOSG's plans to retire GUMS has two major components:\n\n\n\n\nFind a suitable replacement for GUMS\n\n\nProvide documentation, tooling, and support to aid in the transition from GUMS to the intended solution\n\n\n\n\nSite Transition Plans\n\n\nWe have released a configuration of the LCMAPS authorization framework that performs distributed verification of VOMS\nextensions. This configuration, referred to as the LCMAPS VOMS plugin, supports VOMS attribute based mappings as well as\nuser and VO banning. Host-based mappings are not supported however, the simplicity of the plugin's installation and\nthe distributed verification of VOMS extensions makes this feature unnecessary.\n\n\nPool accounts are not supported by the plugin but this feature will be addressed in an upcoming transition-specific\ndocument. The intended solution will revolve around mapping local user accounts via user grid mapfile and we will work\nwith any site for which this solution does not work.\n\n\nLCMAPS VOMS plugin installation and configuration documentation can be\nfound \nhere\n.\n\n\nTimeline\n\n\n\n\nApril 2017 (completed): \nlcmaps-plugins-voms\n shipped and supported by OSG.\n\n\nMay 2017 (completed): \nosg-configure\n and documentation necessary for using \nlcmaps-plugins-voms\n is shipped.\n\n\nJune 2017 (completed): OSG 3.4.0 is released without VOMS-Admin, \nedg-mkgridmap\n, or GUMS.\n\n\nJuly 2017: OSG 3.4 CEs can be configured with 3.3 GUMS hosts\n  (\nSOFTWARE-2482\n)\n\n\nSeptember 2017?: Complete transition for sites using default GUMS configurations\n\n\nDecember 2017?: Complete transition for sites not using pool accounts\n\n\nMay 2018: Support is dropped for OSG 3.3 series; no further support for GUMS is provided.", 
            "title": "GUMS Retirement"
        }, 
        {
            "location": "/policy/gums-retire/#gums-retirement", 
            "text": "This document provides an overview of the planned retirement of support for GUMS in the OSG Software Stack.", 
            "title": "GUMS Retirement"
        }, 
        {
            "location": "/policy/gums-retire/#introduction", 
            "text": "GUMS (Grid User Management System) is an authentication system used by OSG resource providers to map grid credentials to\nlocal UNIX accounts. It provides OSG site adminstrators with a centrally managed service that can handle requests from\nmultiple hosts that require authentication e.g., HTCondor-CE, GridFTP, and XRootD servers. In discussion with the OSG\ncommunity, we have found that sites use the following GUMS features:   Mapping based on VOMS attributes  Host-based mappings  Banning users/VOs  Supporting pool accounts   GUMS is a large Java web application that is more complex than necessary for the subset of features used in the\nOSG. Additionally, upstream support has tailed off and as a result, the maintenance burden has largely fallen on the OSG\nSoftware team.  OSG's plans to retire GUMS has two major components:   Find a suitable replacement for GUMS  Provide documentation, tooling, and support to aid in the transition from GUMS to the intended solution", 
            "title": "Introduction"
        }, 
        {
            "location": "/policy/gums-retire/#site-transition-plans", 
            "text": "We have released a configuration of the LCMAPS authorization framework that performs distributed verification of VOMS\nextensions. This configuration, referred to as the LCMAPS VOMS plugin, supports VOMS attribute based mappings as well as\nuser and VO banning. Host-based mappings are not supported however, the simplicity of the plugin's installation and\nthe distributed verification of VOMS extensions makes this feature unnecessary.  Pool accounts are not supported by the plugin but this feature will be addressed in an upcoming transition-specific\ndocument. The intended solution will revolve around mapping local user accounts via user grid mapfile and we will work\nwith any site for which this solution does not work.  LCMAPS VOMS plugin installation and configuration documentation can be\nfound  here .", 
            "title": "Site Transition Plans"
        }, 
        {
            "location": "/policy/gums-retire/#timeline", 
            "text": "April 2017 (completed):  lcmaps-plugins-voms  shipped and supported by OSG.  May 2017 (completed):  osg-configure  and documentation necessary for using  lcmaps-plugins-voms  is shipped.  June 2017 (completed): OSG 3.4.0 is released without VOMS-Admin,  edg-mkgridmap , or GUMS.  July 2017: OSG 3.4 CEs can be configured with 3.3 GUMS hosts\n  ( SOFTWARE-2482 )  September 2017?: Complete transition for sites using default GUMS configurations  December 2017?: Complete transition for sites not using pool accounts  May 2018: Support is dropped for OSG 3.3 series; no further support for GUMS is provided.", 
            "title": "Timeline"
        }, 
        {
            "location": "/policy/bestman2-retire/", 
            "text": "BeStMan2 Retirement\n\n\nThis document provides an overview of the planned retirement of support for BeStMan in the OSG Software Stack.\n\n\nIntroduction\n\n\nBeStMan2 is a standalone implementation of a subset of the Storage Resource Manager v2 (SRMv2) protocol.  SRM was meant to be a high-level management protocol for site storage resources, allowing administrators to manage storage offerings using the abstraction of \"storage tokens.\"  Additionally, SRM can be used to mediate transfer protocol selection.\n\n\nOSG currently supports BeStMan2 in \"gateway mode\" -- in this mode, SRM is only used for metadata operations (listing directory contents), listing total space used, and load-balancing GridFTP servers.  This functionality is redundant to what can be accomplished with GridFTP alone.\n\n\nBeStMan2 has not received upstream support for approximately five years; the existing code base (about 150,000 lines of Java - similar in size to Globus GridFTP) and its extensive set of dependencies (such as JGlobus) are now quite outdated and would require significant investment to modernize.  OSG has worked at length with our stakeholders to replace SRM-specific use cases with other equivalents.  We believe none of our stakeholders require sites to have an SRM endpoint: this document describes the site transition plan.\n\n\nSite Transition Plans\n\n\nWe have released \ndocumentation\n\nfor a configuration of GridFTP that takes advantage of Linux Virtual Server (LVS) for load balancing between multiple\nGridFTP endpoints.\n\n\nSites should work with their supported VOs (typically, CMS or ATLAS) to identify any VO-specific usage and replacement plans for BeStMan2.\n\n\nTimeline\n\n\n\n\nMarch 2017 (completed):\n  Release \nload balanced GridFTP\n\n  documentation\n\n\nJune 2017 (completed): OSG 3.4.0 is released without BeStMan\n\n\nDecember 2018: Security-only support for OSG 3.3 series and BeStMan is provided\n\n\nMay 2018: Support is dropped for OSG 3.3 series; no further support for BeStMan is provided.", 
            "title": "BeStMan2 Retirement"
        }, 
        {
            "location": "/policy/bestman2-retire/#bestman2-retirement", 
            "text": "This document provides an overview of the planned retirement of support for BeStMan in the OSG Software Stack.", 
            "title": "BeStMan2 Retirement"
        }, 
        {
            "location": "/policy/bestman2-retire/#introduction", 
            "text": "BeStMan2 is a standalone implementation of a subset of the Storage Resource Manager v2 (SRMv2) protocol.  SRM was meant to be a high-level management protocol for site storage resources, allowing administrators to manage storage offerings using the abstraction of \"storage tokens.\"  Additionally, SRM can be used to mediate transfer protocol selection.  OSG currently supports BeStMan2 in \"gateway mode\" -- in this mode, SRM is only used for metadata operations (listing directory contents), listing total space used, and load-balancing GridFTP servers.  This functionality is redundant to what can be accomplished with GridFTP alone.  BeStMan2 has not received upstream support for approximately five years; the existing code base (about 150,000 lines of Java - similar in size to Globus GridFTP) and its extensive set of dependencies (such as JGlobus) are now quite outdated and would require significant investment to modernize.  OSG has worked at length with our stakeholders to replace SRM-specific use cases with other equivalents.  We believe none of our stakeholders require sites to have an SRM endpoint: this document describes the site transition plan.", 
            "title": "Introduction"
        }, 
        {
            "location": "/policy/bestman2-retire/#site-transition-plans", 
            "text": "We have released  documentation \nfor a configuration of GridFTP that takes advantage of Linux Virtual Server (LVS) for load balancing between multiple\nGridFTP endpoints.  Sites should work with their supported VOs (typically, CMS or ATLAS) to identify any VO-specific usage and replacement plans for BeStMan2.", 
            "title": "Site Transition Plans"
        }, 
        {
            "location": "/policy/bestman2-retire/#timeline", 
            "text": "March 2017 (completed):\n  Release  load balanced GridFTP \n  documentation  June 2017 (completed): OSG 3.4.0 is released without BeStMan  December 2018: Security-only support for OSG 3.3 series and BeStMan is provided  May 2018: Support is dropped for OSG 3.3 series; no further support for BeStMan is provided.", 
            "title": "Timeline"
        }, 
        {
            "location": "/policy/voms-admin-retire/", 
            "text": "VOMS-Admin Retirement\n\n\nIntroduction\n\n\nThis document provides an overview of the planned retirement of support for VOMS-Admin\nin the OSG Software Stack.\n\n\nSupport for the VOMS infrastructure has three major components:\n\n\n\n\nVOMS-Admin\n: A web interface for maintaining the list of authorized users in\n    a VO and their various authorizations (group membership, roles, attributes, etc).\n\n\nVOMS-Server\n: A TCP service which signs a cryptographic extension on an X509\n    proxy certificate asserting the authorizations available to the authenticated user.\n\n\nVOMS Client\n: Software for extracting and validating the signed VOMS extension from\n    an X509 proxy.  The validation is meant to be distributed: the VOMS client does not\n    need to contact the VOMS-Admin server.  However, OSG has historically used software\n    such as GUMS or \nedg-mkgridmap\n to cache a list of authorizations from the VOMS-Admin\n    interface, creating a dependency between VOMS client and VOMS-Admin.\n\n\n\n\nVOMS-Admin is a large, complex Java web application.  Over the last\nfew years, upstream support has tailed off - particularly as OSG has been unable\nto update to VOMS-Admin version 3.  As a result, the maintenance burden has largely\nfallen on the OSG Software team.\n\n\nGiven that VOMS-Admin is deeply tied to X509 security infrastructure - and is\nmaintenance-only from OSG Software - there is no path forward to eliminate the use\nof X509 certificates in the web browser, a high-priority goal\n\n\nIn discussions with the OSG community, we have found very few VOs utilize VOMS-Admin\nto manage their VO users.  Instead, the majority use VOMS-Admin to whitelist a pilot\ncertificate: this can be done without a VOMS-Admin endpoint.\n\n\nOSG's plans to retire VOMS-Admin has three major components:\n\n\n\n\n(Sites) Enable distributed validation of VOMS extensions in the VOMS client.\n\n\n(VOs) Migrate VOs that use VOMS only for pilot certificates to direct signing\n  of VOMS proxies.\n\n\n(VOs) Migrate remaining VOs to a central \ncomanage\n instance for managing user\n  authorizations; maintain a plugin to enable direct callouts from VOMS-Server\n  to \ncomanage\n for authorization lookups.\n\n\n\n\nSite Transition Plans\n\n\nWe will release a configuration of the LCMAPS authorization framework that performs\ndistributed verification of VOMS extensions; this verification eludes the need to\ncontact the VOMS-Admin interface for a list of authorizations.\n\n\nIn 2015/2016, LCMAPS and GUMS were upgraded so GUMS skips the VOMS-Admin lookup when\nLCMAPS asserts the validation was performed.  Hence, when GUMS sites update clients to the\nlatest (April 2017) LCMAPS and HTCondor-CE releases, the callout to VOMS-Admin is no longer\nneeded. \nNote\n: In parallel to the VOMS-Admin transition, OSG Software plans to \nretire GUMS\n.\nThere is no need to complete one transition before the other.\n\n\nSites using \nedg-mkgridmap\n will need to use its replacement, \nlcmaps-plugins-voms\n (this\nprocess is documented \nhere\n).\n\n\nVO Transition Plans\n\n\nBased on one-to-one discussions, we believe the majority of VOs only use VOMS-Admin to maintain\na list of authorized pilots.  For these VOs, we will help convert invocations of \nvoms-proxy-init\n:\n\n\nvoms-proxy-init -voms hcc:/hcc/Role=pilot\n\n\n\n\n\nto an equivalent call to \nvoms-proxy-fake\n:\n\n\nvoms-proxy-fake -hostcert /etc/grid-security/voms/vomscert.pem \\\n                -hostkey /etc/grid-security/voms/vomskey.pem \\\n                -fqan /hcc/Role=pilot \\\n                -voms hcc -uri hcc-voms.unl.edu:15000\n\n\n\n\n\nThe latter command would typically be run on the VO's glideinWMS frontend host, requiring the service certificate\ncurrently on the VOMS-Admin server to be kept on the frontend host.  The frontend's account may also need access\nto the certificate.\n\n\nWe plan to transition more complex VOs - those using VOMS-Admin to track membership in a VO - to \ncomanage\n.  It is\nnot clear there are any such VOs that need support from OSG.  If there are, a hosted version of \ncomanage\n is expected\nto be available in summer 2017 from the CILogon 2.0 project.  If you feel your VO is affected, please contact the\nOSG and we will build a custom timeline.  If there are no such VOs, we will not need to adopt \ncomanage\n for this\nuse case (other uses of \ncomanage\n are expected to proceed regardless).\n\n\nTimeline\n\n\n\n\nApril 2017 (completed): \nlcmaps-plugins-voms\n shipped and supported by OSG.\n\n\nMay 2017 (completed): \nosg-configure\n and documentation necessary for using \nlcmaps-plugins-voms\n is shipped.\n\n\nJune 2017 (completed): OSG 3.4.0 is released without VOMS-Admin, \nedg-mkgridmap\n, or GUMS.  Sites begin transition\n  to validating VOMS extensions.\n\n\nSummer 2017 (completed): As necessary, VOs are given access to a hosted \ncomanage\n instance.\n\n\nWinter 2017: First VOs begin to retire VOMS-Admin.\n\n\nMay 2018: Support is dropped for OSG 3.3 series; no further support for VOMS-Admin or GUMS is provided.", 
            "title": "VOMS Admin Retirement"
        }, 
        {
            "location": "/policy/voms-admin-retire/#voms-admin-retirement", 
            "text": "", 
            "title": "VOMS-Admin Retirement"
        }, 
        {
            "location": "/policy/voms-admin-retire/#introduction", 
            "text": "This document provides an overview of the planned retirement of support for VOMS-Admin\nin the OSG Software Stack.  Support for the VOMS infrastructure has three major components:   VOMS-Admin : A web interface for maintaining the list of authorized users in\n    a VO and their various authorizations (group membership, roles, attributes, etc).  VOMS-Server : A TCP service which signs a cryptographic extension on an X509\n    proxy certificate asserting the authorizations available to the authenticated user.  VOMS Client : Software for extracting and validating the signed VOMS extension from\n    an X509 proxy.  The validation is meant to be distributed: the VOMS client does not\n    need to contact the VOMS-Admin server.  However, OSG has historically used software\n    such as GUMS or  edg-mkgridmap  to cache a list of authorizations from the VOMS-Admin\n    interface, creating a dependency between VOMS client and VOMS-Admin.   VOMS-Admin is a large, complex Java web application.  Over the last\nfew years, upstream support has tailed off - particularly as OSG has been unable\nto update to VOMS-Admin version 3.  As a result, the maintenance burden has largely\nfallen on the OSG Software team.  Given that VOMS-Admin is deeply tied to X509 security infrastructure - and is\nmaintenance-only from OSG Software - there is no path forward to eliminate the use\nof X509 certificates in the web browser, a high-priority goal  In discussions with the OSG community, we have found very few VOs utilize VOMS-Admin\nto manage their VO users.  Instead, the majority use VOMS-Admin to whitelist a pilot\ncertificate: this can be done without a VOMS-Admin endpoint.  OSG's plans to retire VOMS-Admin has three major components:   (Sites) Enable distributed validation of VOMS extensions in the VOMS client.  (VOs) Migrate VOs that use VOMS only for pilot certificates to direct signing\n  of VOMS proxies.  (VOs) Migrate remaining VOs to a central  comanage  instance for managing user\n  authorizations; maintain a plugin to enable direct callouts from VOMS-Server\n  to  comanage  for authorization lookups.", 
            "title": "Introduction"
        }, 
        {
            "location": "/policy/voms-admin-retire/#site-transition-plans", 
            "text": "We will release a configuration of the LCMAPS authorization framework that performs\ndistributed verification of VOMS extensions; this verification eludes the need to\ncontact the VOMS-Admin interface for a list of authorizations.  In 2015/2016, LCMAPS and GUMS were upgraded so GUMS skips the VOMS-Admin lookup when\nLCMAPS asserts the validation was performed.  Hence, when GUMS sites update clients to the\nlatest (April 2017) LCMAPS and HTCondor-CE releases, the callout to VOMS-Admin is no longer\nneeded.  Note : In parallel to the VOMS-Admin transition, OSG Software plans to  retire GUMS .\nThere is no need to complete one transition before the other.  Sites using  edg-mkgridmap  will need to use its replacement,  lcmaps-plugins-voms  (this\nprocess is documented  here ).", 
            "title": "Site Transition Plans"
        }, 
        {
            "location": "/policy/voms-admin-retire/#vo-transition-plans", 
            "text": "Based on one-to-one discussions, we believe the majority of VOs only use VOMS-Admin to maintain\na list of authorized pilots.  For these VOs, we will help convert invocations of  voms-proxy-init :  voms-proxy-init -voms hcc:/hcc/Role=pilot  to an equivalent call to  voms-proxy-fake :  voms-proxy-fake -hostcert /etc/grid-security/voms/vomscert.pem \\\n                -hostkey /etc/grid-security/voms/vomskey.pem \\\n                -fqan /hcc/Role=pilot \\\n                -voms hcc -uri hcc-voms.unl.edu:15000  The latter command would typically be run on the VO's glideinWMS frontend host, requiring the service certificate\ncurrently on the VOMS-Admin server to be kept on the frontend host.  The frontend's account may also need access\nto the certificate.  We plan to transition more complex VOs - those using VOMS-Admin to track membership in a VO - to  comanage .  It is\nnot clear there are any such VOs that need support from OSG.  If there are, a hosted version of  comanage  is expected\nto be available in summer 2017 from the CILogon 2.0 project.  If you feel your VO is affected, please contact the\nOSG and we will build a custom timeline.  If there are no such VOs, we will not need to adopt  comanage  for this\nuse case (other uses of  comanage  are expected to proceed regardless).", 
            "title": "VO Transition Plans"
        }, 
        {
            "location": "/policy/voms-admin-retire/#timeline", 
            "text": "April 2017 (completed):  lcmaps-plugins-voms  shipped and supported by OSG.  May 2017 (completed):  osg-configure  and documentation necessary for using  lcmaps-plugins-voms  is shipped.  June 2017 (completed): OSG 3.4.0 is released without VOMS-Admin,  edg-mkgridmap , or GUMS.  Sites begin transition\n  to validating VOMS extensions.  Summer 2017 (completed): As necessary, VOs are given access to a hosted  comanage  instance.  Winter 2017: First VOs begin to retire VOMS-Admin.  May 2018: Support is dropped for OSG 3.3 series; no further support for VOMS-Admin or GUMS is provided.", 
            "title": "Timeline"
        }, 
        {
            "location": "/policy/release-series/", 
            "text": "OSG Software Release Series Support Policy\n\n\nThis document describes the OSG policy for managing its software releases. Use this policy to help plan when to perform major OSG software updates at your site.\n\n\nOSG software releases are organized into \nrelease series\n, with the intent that software updates within a series do not take long to perform, cause significant downtime, or break dependent software. New series can be more disruptive, allowing OSG to add, substantially change, and remove software components. Releases are assigned versions, such as \"OSG 3.2.1\", where the first two numbers designate the release series and the third number increments within the series. Changes to the first number are infrequent and indicate sweeping changes to the way in which OSG software is distributed.\n\n\nOSG supports at most two concurrent release series, \ncurrent\n and \nprevious\n, where the goal is to begin a new release series about every 18 months. Once a new series starts, OSG will support the previous series for at least 12 months and will announce its end-of-life date at least 6 months in advance. During the first 6 months of a series, OSG will endeavor to apply backward-compatible changes to the previous series as well; afterward, OSG will apply only critical bug and security fixes. When support ends for a release series, it means that OSG no longer updates the software, fixes issues, or troubleshoots installations for releases within the series. The plan is to maintain interoperability between supported series, but there is no guarantee that unsupported series will continue to function in OSG.\n\n\nOSG Operations will handle deviations from this policy, in consultation with OSG Technology and stakeholders.\n\n\nLife-cycle Dates\n\n\n\n\n\n\n\n\nRelease Series\n\n\nInitial Release\n\n\nEnd of Regular Support\n\n\nEnd of Critical Bug/Security Support\n\n\n\n\n\n\n\n\n\n\n3.4\n\n\nJune 2017\n\n\nNot set\n\n\nNot set\n\n\n\n\n\n\n3.3\n\n\nAugust 2015\n\n\nDecember 2017\n\n\nMay 2018\n\n\n\n\n\n\n3.2\n\n\nNovember 2013\n\n\nFebruary 2016\n\n\nAugust 2016\n\n\n\n\n\n\n3.1\n\n\nApril 2012\n\n\nOctober 2014\n\n\nApril 2015", 
            "title": "Release Series Support"
        }, 
        {
            "location": "/policy/release-series/#osg-software-release-series-support-policy", 
            "text": "This document describes the OSG policy for managing its software releases. Use this policy to help plan when to perform major OSG software updates at your site.  OSG software releases are organized into  release series , with the intent that software updates within a series do not take long to perform, cause significant downtime, or break dependent software. New series can be more disruptive, allowing OSG to add, substantially change, and remove software components. Releases are assigned versions, such as \"OSG 3.2.1\", where the first two numbers designate the release series and the third number increments within the series. Changes to the first number are infrequent and indicate sweeping changes to the way in which OSG software is distributed.  OSG supports at most two concurrent release series,  current  and  previous , where the goal is to begin a new release series about every 18 months. Once a new series starts, OSG will support the previous series for at least 12 months and will announce its end-of-life date at least 6 months in advance. During the first 6 months of a series, OSG will endeavor to apply backward-compatible changes to the previous series as well; afterward, OSG will apply only critical bug and security fixes. When support ends for a release series, it means that OSG no longer updates the software, fixes issues, or troubleshoots installations for releases within the series. The plan is to maintain interoperability between supported series, but there is no guarantee that unsupported series will continue to function in OSG.  OSG Operations will handle deviations from this policy, in consultation with OSG Technology and stakeholders.", 
            "title": "OSG Software Release Series Support Policy"
        }, 
        {
            "location": "/policy/release-series/#life-cycle-dates", 
            "text": "Release Series  Initial Release  End of Regular Support  End of Critical Bug/Security Support      3.4  June 2017  Not set  Not set    3.3  August 2015  December 2017  May 2018    3.2  November 2013  February 2016  August 2016    3.1  April 2012  October 2014  April 2015", 
            "title": "Life-cycle Dates"
        }, 
        {
            "location": "/policy/globus-toolkit/", 
            "text": "OSG Support of the Globus Toolkit\n\n\n6 June 2017\n\n\nMany in the OSG community have heard the news about the \nend of support for the open-source Globus Toolkit\n.\n\n\nWhat does this imply for the OSG Software stack?  Not much: OSG support for the Globus Toolkit (e.g., GridFTP and GSI) will continue for as long as stakeholders need it. Period.\n\n\nNote the OSG Software team provides a support guarantee for all the software in its stack. When a software component reaches end-of-life, the OSG assists its stakeholders in managing the transition to new software to replace or extend those capabilities. This assistance comes in many forms, such as finding an equivalent replacement, adapting code to avoid the dependency, or helping research and develop a transition to new technology.  During such transition periods, OSG takes on traditional maintenance duties (i.e., patching, bug fixes and support) of the end-of-life software.  The OSG is committed to keep the software secure until its stakeholders have successfully transitioned to new software. \n\n\nThis model has been successfully demonstrated throughout the lifetime of OSG, including for example the five year transition period for the BestMan storage resource manager. The Globus Toolkit will not be an exception.  Indeed, OSG has accumulated more than a decade of experience with this software and has often provided patches back to Globus. \n\n\nOver the next weeks and months, we will be in contact with our stakeholder VOs, sites, and software providers to discuss their requirements and timelines with regard to GridFTP and GSI.  \n\n\nPlease reach out to \ngoc@opensciencegrid.org\n with your questions, comments, and concerns.\n\n\nA copy of this statement can be found at \nhttps://opensciencegrid.github.io/technology/policy/globus-toolkit\n.", 
            "title": "Globus Toolkit Support"
        }, 
        {
            "location": "/policy/globus-toolkit/#osg-support-of-the-globus-toolkit", 
            "text": "6 June 2017  Many in the OSG community have heard the news about the  end of support for the open-source Globus Toolkit .  What does this imply for the OSG Software stack?  Not much: OSG support for the Globus Toolkit (e.g., GridFTP and GSI) will continue for as long as stakeholders need it. Period.  Note the OSG Software team provides a support guarantee for all the software in its stack. When a software component reaches end-of-life, the OSG assists its stakeholders in managing the transition to new software to replace or extend those capabilities. This assistance comes in many forms, such as finding an equivalent replacement, adapting code to avoid the dependency, or helping research and develop a transition to new technology.  During such transition periods, OSG takes on traditional maintenance duties (i.e., patching, bug fixes and support) of the end-of-life software.  The OSG is committed to keep the software secure until its stakeholders have successfully transitioned to new software.   This model has been successfully demonstrated throughout the lifetime of OSG, including for example the five year transition period for the BestMan storage resource manager. The Globus Toolkit will not be an exception.  Indeed, OSG has accumulated more than a decade of experience with this software and has often provided patches back to Globus.   Over the next weeks and months, we will be in contact with our stakeholder VOs, sites, and software providers to discuss their requirements and timelines with regard to GridFTP and GSI.    Please reach out to  goc@opensciencegrid.org  with your questions, comments, and concerns.  A copy of this statement can be found at  https://opensciencegrid.github.io/technology/policy/globus-toolkit .", 
            "title": "OSG Support of the Globus Toolkit"
        }, 
        {
            "location": "/infrastructure/koji-restore-recipe/", 
            "text": "How to Restore Koji\n\n\nThis document contains recipes on how to restore the Koji services and the database they require. It is divided into two sections: one for the database (to be done if something happens to \ndb-01\n), and one for the server hosting the koji services (to be done if something happens to \nkoji.chtc\n).\n\n\nIn case both the database and the hub need to be restored, the database should be restored first.\n\n\nBackground information\n\n\nBackups of \nkoji.chtc.wisc.edu\n and \ndb-01.batlab.org\n are on \nhost-3.chtc.wisc.edu\n in \n/export/backup/\nDATE\n. That machine is in WID. (Same room as \nkoji.chtc\n itself, which is why we have offsite backups). It's a homebrew rsync-based backup system. (Not our home -- Nate told me it was written for Midwest Tier 2.) They go back up to a week, with a monthly snapshot for a year.\n\n\nSetting up your environment\n\n\nFor all of these steps, you will need a root shell on \nhost-3.chtc.wisc.edu\n and have the following environment variables defined:\n\n\nNEWDB=\nFQDN OF NEW DATABASE SERVER\n\n\nNEWKOJI=\nFQDN OF NEW KOJI HOST\n\n\nDATE=\nYYYY-MM-DD DATE OF MOST RECENT GOOD BACKUP\n\n\nDBBACKUP=/export/backup/$DATE/db-01.batlab.org\n\n\nKOJIBACKUP=/export/backup/$DATE/koji.chtc.wisc.edu\n\n\nRSYNC=\nrsync --archive --hard-links --verbose\n\n\n\n\n\n\nRestoring the database\n\n\nThe entire filesystem of \ndb-01\n is backed up -- this includes all of \n/var/lib/pgsql\n, including the database as-is. In theory, this means that we could just rsync all the files to a blank hard drive, boot up, and we'd have a \ndb-01\n again. However, the Postgres manual warns against restoring the database from a filesystem backup that was made while the database was live, and we do not shut down the database before backups.\n\n\nWe might be able to restore every other part of the filesystem besides the database, which would speed up the overall restoration process, but only the fresh install was tested.\n\n\nThe new database server is called \nnewdb\n in these instructions.\n\n\nRestoring Services\n\n\nPrerequisites for \nnewdb\n: an EL 6+ host with an SSH server set up and accessible (as root) from \nhost-3.chtc.wisc.edu\n\n\n#\n# On newdb:\n\n\n#\n# Install postgres, get a blank DB up and create the user that koji\n\n\n#\n# will be using.\n\n\n[root@newdb]#\n yum install -y postgresql-server\n\n[root@newdb]#\n service postgresql initdb\n\n[root@newdb]#\n useradd -r -m koji\n\n#\n# Make a directory we\nll put the restored files into.\n\n\n[root@newdb]#\n mkdir -p /root/dbrestore\n\n\n#\n# On host-3:\n\n\n[you@host-3]$\n sudo \n$RSYNC\n \n$DBBACKUP\n/homefs/  \n$NEWDB\n:/root/dbrestore/home\n\n[you@host-3]$\n \nfor\n dir in root etc var\n;\n \ndo\n \n\\\n\n   sudo \n$RSYNC\n \n$DBBACKUP\n/rootfs/\n$dir\n/ \n\\\n\n      \n$NEWDB\n:/root/dbrestore/\n$dir\n \n\\\n\n   \ndone\n\n\n\n\n\n\nContinue on to the next section\n\n\nRestoring Database Contents\n\n\nAssumes you have restored the /var directory from backup into \n/root/dbrestore/var\n.\n\n\n\n\n\n\nRestore the postgres config files so the koji-hub daemon can log in:\n\n\n#\n# On newdb:\n\n\n[root@newdb]#\n service postgresql stop\n\n[root@newdb]#\n cp -a /root/dbrestore/var/lib/pgsql/data/\n{\n*.conf,postmaster.opts\n}\n \n\\\n\n    /var/lib/pgsql/data/\n\n\n\n\n\n\n\n\n\nEdit \n/var/lib/pgsql/data/pg_hba.conf\n. There are lines like:\n\n\n# Koji-hub IPv4:\nhost koji koji 128.104.100.41/32 md5\n\n\n\n\n\nChange the IP address to the public IP address of the host that will serve as the new hub.\n\n\n\n\n\n\nRestore the actual database:\n\n\n#\n# On newdb:\n\n\n[root@newdb]#\n chown -R postgres:postgres /var/lib/pgsql/*\n\n[root@newdb]#\n service postgresql start\n\n[root@newdb]#\n gunzip -c /root/dbrestore/var/lib/pgsql-backup/postgres-db-01.sql.gz \n|\n\n\n    psql -U postgres postgres\n\n\n\n\n\n\n\n\n\n\nValidation\n\n\nDo the following tests to make sure the database is ready to use:\n\n\n\n\n\n\nTest that the contents got properly restored:\n\n\n[root@newdb]#\n psql -U koji koji koji\n\n\n\n\n\n\n\n\nkoji=\n \nselect\n \n*\n \nfrom\n \nusers\n;\n\n\nkoji=\n \nselect\n \n*\n \nfrom\n \nbuild\n \norder\n \nby\n \nid\n \ndesc\n \nlimit\n \n10\n;\n\n\n\n\n\n\n\n\n\n\nTest logging in as the koji user:\n\n\n[root@newdb]#\n psql -U koji -h newdb koji\n\n\n\n\n\n(you must use the FQDN of \nnewdb\n, not \nlocalhost\n).\nBe sure you get prompted for a password, and the password from \n/etc/koji-hub/hub.conf\n works.\n\n\n\n\n\n\nContinue to \"Restoring Koji\" if needed, otherwise skip to \"Starting Services and Validation\"\n\n\n\n\n\n\nRestoring Koji\n\n\nBoth the root filesystem of \nkoji.chtc\n and \n/mnt/koji\n are backed up. The root filesystem backups are in the \nrootfs\n subdirectory of \n/export/backup/$DATE/koji.chtc.wisc.edu\n and the backups of \n/mnt/koji\n are in the \nkojifs\n subdirectory.\n\n\nThe following instructions show how to restore the critical components of Koji onto a new machine.\n\n\nIn the instructions, the new host will be named \nnewkoji\n.\n\n\nInstalling the OS\n\n\nPrerequisites for \nnewkoji\n: an EL 6 host with an SSH server set up and accessible (as root) from \nhost-3.chtc.wisc.edu\n \n\n(This recipe was tested for EL 6, on the same machine as \nnewdb\n).\n\n\n\n\n\n\nInstall EPEL and OSG repos:\n\n\n[root@newkoji]#\n rpm -Uvh \n\\\n\n    https://dl.fedoraproject.org/pub/epel/epel-release-latest-6.noarch.rpm \n\\\n\n    https://repo.grid.iu.edu/osg/3.4/osg-3.4-el6-release-latest.rpm\n\n[root@newkoji]#\n yum install -y yum-plugin-priorities\n\n\n\n\n\n\n\n\n\nEdit \n/etc/yum.repos.d/osg-*development.repo\n:\n\n\n\n\nEnable the development repo\n\n\nAdd \nincludepkg=koji*\n to the definition for the development repo\n\n\n\n\n\n\n\n\nGo through the other repo files and make sure that EPEL and OS priorities are worse than 98.\n    This means absent or numerically greater.\n    Especially look at \ncobbler-config.repo\n if it exists.\n\n\n\n\n\n\nInstall the koji packages and dependencies, making sure the koji packages themselves come from osg:\n\n\n[root@newkoji]#\n yum install koji koji-builder koji-hub koji-plugin-sign \n\\\n\n    koji-theme-fedora koji-utils koji-web mod_ssl postgresql\n\n\n\n\n\n\n\n\n\nMount \n/mnt/koji\n if necessary\n\n\n\n\n\n\nRestore the contents of the koji filesystem. On \nhost-3\n:\n\n\n#\n# At a minimum, you must restore the /mnt/koji/packages directory\n\n\n[you@host-3]$\n sudo \n$RSYNC\n \n$KOJIBACKUP\n/kojifs/packages/ \n$NEWKOJI\n:/mnt/koji/packages\n\n#\n# The other directories are optional, though it saves a lot of time to restore /mnt/koji/repos\n\n\n[you@host-3]$\n sudo \n$RSYNC\n \n$KOJIBACKUP\n/kojifs/repos/ \n$NEWKOJI\n:/mnt/koji/repos\n\n[you@host-3]$\n sudo \n$RSYNC\n \n$KOJIBACKUP\n/kojifs/work/ \n$NEWKOJI\n:/mnt/koji/work\n\n[you@host-3]$\n sudo \n$RSYNC\n \n$KOJIBACKUP\n/kojifs/scratch/ \n$NEWKOJI\n:/mnt/koji/scratch\n\n#\n# Any dirs you did not restore should be created.\n\n\n\n\n\n\n\n\n\n\nFix permissions if needed. On \nnewkoji\n:\n\n\n[root@newkoji]#\n chown -R apache:apache /mnt/koji/\n{\npackages,repos,work,scratch\n}\n\n\n[root@newkoji]#\n chmod \n0755\n /mnt/koji/\n{\npackages,repos,work,scratch\n}\n\n\n\n\n\n\n\n\n\n\nContinue on to the next section\n\n\n\n\n\n\nRestoring Configuration\n\n\nOn \nnewkoji\n, define the shell function \ndirclone\n, listed below:\n\n\ndirclone \n()\n \n{\n\n   \nsrcdir\n=\n$(\ndirname \n$1\n)\n/\n$(\nbasename \n$1\n)\n\n   \ndestdir\n=\n$(\ndirname \n$2\n)\n/\n$(\nbasename \n$2\n)\n\n   mkdir -p \n$(\ndirname \n$2\n)\n\n   rsync --archive --delete-after --acls --xattrs \n\\\n\n         --partial --partial-dir\n=\n.rsync-partial \n\\\n\n         \n$srcdir\n/\n \n$destdir\n\n\n}\n\n\n\n\n\n\n\n\n\n\nOn \nnewkoji\n:\n\n\n[root@newkoji]#\n mkdir -p /root/hubrestore\n\n\n\n\n\n\n\n\n\nOn \nhost-3\n:\n\n\n[you@host-3]$\n sudo \n$RSYNC\n \n$KOJIBACKUP\n/rootfs/\n{\nroot,home,etc\n}\n \n$NEWKOJI\n:/root/hubrestore/\n\n[you@host-3]$\n sudo \n$RSYNC\n \n$KOJIBACKUP\n/varfs/ \n$NEWKOJI\n:/root/hubrestore/var/\n\n\n\n\n\n\n\n\n\nOn \nnewkoji\n, install some utils we will need later:\n\n\n[root@newkoji]#\n yum install -y dos2unix vim-enhanced\n\n\n\n\n\n(vim-enhanced is used for vimdiff)\n\n\n\n\n\n\nOn \nnewkoji\n:\n\n\n#\n# Restore some of the directories in /etc:\n\n\n[root@newkoji]#\n \nwhile\n \nread\n subtree\n;\n \ndo\n\n\n    dirclone /root/hubrestore/etc/$subtree /etc/$subtree\n\n\ndone \n___END___\n\n\nhttpd\n\n\nkojid\n\n\nkoji-hub\n\n\nkojira\n\n\nkoji-sign-plugin\n\n\nkojiweb\n\n\nmock\n\n\npki/koji\n\n\npki/tls/certs\n\n\npki/tls/private\n\n\n___END___\n\n\n#\n# Restore some of the files:\n\n\n[root@newkoji]#\n cp -a /root/hubrestore/etc/koji.conf /etc/\n\n[root@newkoji]#\n cp -a /root/hubrestore/etc/sysconfig/\n{\nhttpd,kojid,kojira\n}\n /etc/sysconfig/\n\n\n\n\n\n\n\n\n\nRestore users and home directories\n\n\n\n\n\n\nIf \nnewkoji\n is on a separate host from \nnewdb\n, then just simply copy over the files:\n\n\n[root@newkoji]#\n dirclone /root/hubrestore/home /home\n\n[root@newkoji]#\n cp -a /root/hubrestore/etc/\n{\npasswd,shadow,group,gshadow\n}\n /etc\n\n\n\n\n\n\n\n\n\nIf \nnewkoji\n is on the same host as \nnewdb\n, then you will have to be more careful:\n\n\n#\n# Skip home directories for the special users\n\n\n[root@newkoji]#\n \nfor\n dir in /root/hubrestore/home/*\n;\n \ndo\n\n\n    bndir=$(basename \n$dir\n)\n\n\n    if [[ $bndir = koji \n $bndir = postgres ]]; then\n\n\n        dirclone \n$dir\n /home/\n$bndir\n\n\n    fi\n\n\ndone\n\n\n#\n# Now merge the passwd, group, shadow, and gshadow files in /etc.\n\n\n#\n# Make sure that your editor does not create backup files\n\n\n#\n# (\nset nobackup\n in vim), and that shadow and gshadow are owned by\n\n\n#\n# root and have 0400 permissions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEnsure a 'koji' user exists\n\n\n\n\n\n\nFix dirs in \n/var\n:\n\n\n[root@newkoji]#\n rm -rf /var/lib/mock/*\n\n[root@newkoji]#\n chown root:mock /var/lib/mock\n\n[root@newkoji]#\n chmod \n2775\n /var/lib/mock\n\n\n\n\n\n\n\n\n\nRestore \n/var/www/html\n and \n/var/spool/cron\n \n\n    (TODO) \n/var\n should have been backed up, but in case it isn't, the following files need to exist in \n/var/www/html\n:\n\n\n\n\nA symlink \nmnt -\n /mnt\n\n\nA robots.txt with contents\nUser-agent: *\nDisallow: /\n\n\n\n\n\n\n\n\n\n\n\n\n\nFixing Names\n\n\nThis section should be done if \nnewdb\n or \nnewkoji\n do not have the same as the previous db server and hub (i.e. \ndb-01.batlab.org\n and \nkoji.chtc.wisc.edu\n). This section should be completed on \nnewkoji\n.\n\n\nFixing config files if \nnewdb\n was renamed\n\n\nThe only change that's needed if \nnewdb\n was renamed is to \n/etc/koji-hub/hub.conf\n. Edit that file and change the DBHost line to point to the new hostname. After editing, make sure \nhub.conf\n is owned by \nroot:apache\n and chmodded 0640.\n\n\nInstalling new cert/key pairs for \nnewkoji\n\n\nYou will need two cert/key pairs: one for the host, and one for the kojira service. Run \ndos2unix\n on all cert and key files before using them. Define the shell function \nmakepem\n, listed below. \nmakepem\n combines a public and private keypair to make a .pem file that the koji services use.\n\n\nUsage: \nmakepem \nCERTFILE\n \nKEYFILE\n \nOUTPUT_FILE\n\n\nmakepem \n()\n \n{\n\n    \ncertfile\n=\n$1\n\n    \nkeyfile\n=\n$2\n\n    \noutputfile\n=\n$3\n\n    \n(\nset\n -e\n    \nkeymodulus\n=\n$(\nopenssl rsa -noout -modulus -in \n$keyfile\n)\n\n    \ncertmodulus\n=\n$(\nopenssl x509 -noout -modulus -in \n$certfile\n)\n\n    \nif\n \n[[\n \n$keymodulus\n \n=\n \n$certmodulus\n \n]]\n \n;\n \nthen\n\n        \necho\n \nkeyfile and certfile do not match\n;\n \nreturn\n \n1\n\n    \nfi\n\n    \nif\n \n[[\n -f \n$outputfile\n \n]]\n \n;\n \nthen\n\n        mv -f \n$outputfile\n{\n,.bak\n}\n\n    \nfi\n\n    \n(\ndos2unix \n \n$certfile\n;\n echo\n;\n dos2unix \n \n$keyfile\n)\n \n \n$outputfile\n\n    chmod \n0600\n \n$outputfile\n\n    \n)\n\n\n}\n\n\n\n\n\n\nPlace cert and key files into the following paths:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhost cert\n\n\n/root/hostcert.pem\n\n\n\n\n\n\nhost key\n\n\n/root/hostkey.pem\n\n\n\n\n\n\nkojira cert\n\n\n/root/kojiracert.pem\n\n\n\n\n\n\nkojira key\n\n\n/root/kojirakey.pem\n\n\n\n\n\n\n\n\nThen use \nmakepem\n to combine the certs and put them in the proper locations.\n\n\n[root@newkoji]#\n makepem /root/hostcert.pem /root/hostkey.pem \n\\\n\n   /etc/pki/tls/private/kojiweb.pem\n\n[root@newkoji]#\n makepem /root/kojiracert.pem /root/kojirakey.pem \n\\\n\n   /etc/pki/tls/private/kojira.pem\n\n[root@newkoji]#\n chown apache:apache /etc/pki/tls/private/kojiweb.pem\n\n[root@newkoji]#\n chown root:root /etc/pki/tls/private/kojira.pem\n\n\n\n\n\nIn addition, copy the host cert and key into the locations HTTPD expects it them.\n\n\n[root@newkoji]#\n cp -a /root/hostcert.pem /etc/pki/tls/certs/hostcert.pem\n\n[root@newkoji]#\n cp -a /root/hostkey.pem /etc/pki/tls/private/hostkey.pem\n\n\n\n\n\nFixing hostname in config files\n\n\nUse sed to replace the hostname in the following config files in /etc:\n\n\n\n\n/etc/kojira/kojira.conf\n\n\n/etc/koji.conf\n\n\n/etc/koji-hub/hub.conf\n\n\n/etc/httpd/conf.d/kojiweb.conf\n\n\n/etc/httpd/conf/httpd.conf\n\n\n/etc/kojid/kojid.conf\n\n\n\n\nYou will need to fix \n/etc/kojid/kojid.conf\n on all builder machines as well (e.g. \nkojibuilder2.chtc.wisc.edu\n).\n\n\nFixing hostname in database\n\n\nYou will need to find and fix entries that contain the hostname in the following tables:\n\n\n\n\nhost\n (should be 1 entry)\n\n\nusers\n (should be 2 entries, one for the host, and one for the kojira user)\n\n\n\n\nFixing hostname elsewhere\n\n\nThese steps are only necessary if you cannot get a DNS Canonical Name (CN) record such that \nkoji.chtc.wisc.edu\n resolves to \nnewkoji\n.\n\n\n\n\nUpdate the repo definitions in the \nosg-release\n package\n\n\nUpdate the mash script(s) at the GOC\n\n\nMail the software team and users that anyone using the \nminefield\n repos will need to update \nosg-release\n\n\nFix all the build machines to point to the new name\n\n\nFix the following files in \nosg-build\n and make a new release\n\n\ndata/osg-koji-home.conf\n\n\ndata/osg-koji-site.conf\n\n\nosgbuild/constants.py\n\n\nosgbuild/kojiinter.py\n\n\n\n\n\n\nMail people that they will need to update \nosg-build\n and rerun \nosg-koji setup\n\n\n\n\nStarting Services and Validation\n\n\nNow you will start up Koji services and verify that they function.\nPrerequisite: previous restore steps have been completed and \npostgresql\n is running on the database host.\n\n\nAll steps will be run on \nnewkoji\n.\n\n\n\n\n\n\nStart the main koji daemon:\n\n\n[root@newkoji]#\n service httpd start\n\n\n\n\n\n\n\n\n\nUse \nps\n to verify that it came up\n\n\n\n\nConnect to the web interface in your browser.\n    Make sure you can use https and you can log in.\n\n\nAs yourself, run the \nkoji\n command-line tool and make a few queries (e.g. list-tags)\n\n\n\n\nStart the koji build daemon:\n\n\n[root@newkoji]#\n service kojid start\n\n\n\n\n\n\n\n\n\nUse \nps\n to verify that it came up\n\n\n\n\n\n\nIf you did not restore the \n/mnt/koji/repos\n directory, you will now need to regenerate the build repos.\nUse \nkoji list-tags\n to get a list of tags and run \nkoji regen-repo\n on all of the ones with \n-build\n in the name.\nThis \nwill\n take several hours.\nYou will also need to regen the \n-development\n repos so that \nminefield\n works again. Keep an eye on the tasks in the web interface to make sure they are getting farmed out to the right hosts.\n\n\n\n\nTry a scratch build\n\n\n\n\nStart \nkojira\n:\n\n\n[root@newkoji]#\n service kojira start\n\n\n\n\n\n\n\n\n\nUse \nps\n to verify that it came up\n\n\n\n\nWait half a minute and use \nps\n to verify that \nkojid\n is still up; the two processes can kick each other off if they are both using the same certificate\n\n\nBump a package if needed and try a real, non-scratch build\n\n\nMake sure that kojira is regenerating the repos\n\n\n\n\nIf you have updated \nosg-release\n and/or \nosg-build\n, rebuild those packages now.", 
            "title": "Koji Restore Recipe"
        }, 
        {
            "location": "/infrastructure/koji-restore-recipe/#how-to-restore-koji", 
            "text": "This document contains recipes on how to restore the Koji services and the database they require. It is divided into two sections: one for the database (to be done if something happens to  db-01 ), and one for the server hosting the koji services (to be done if something happens to  koji.chtc ).  In case both the database and the hub need to be restored, the database should be restored first.", 
            "title": "How to Restore Koji"
        }, 
        {
            "location": "/infrastructure/koji-restore-recipe/#background-information", 
            "text": "Backups of  koji.chtc.wisc.edu  and  db-01.batlab.org  are on  host-3.chtc.wisc.edu  in  /export/backup/ DATE . That machine is in WID. (Same room as  koji.chtc  itself, which is why we have offsite backups). It's a homebrew rsync-based backup system. (Not our home -- Nate told me it was written for Midwest Tier 2.) They go back up to a week, with a monthly snapshot for a year.", 
            "title": "Background information"
        }, 
        {
            "location": "/infrastructure/koji-restore-recipe/#setting-up-your-environment", 
            "text": "For all of these steps, you will need a root shell on  host-3.chtc.wisc.edu  and have the following environment variables defined:  NEWDB= FQDN OF NEW DATABASE SERVER  NEWKOJI= FQDN OF NEW KOJI HOST  DATE= YYYY-MM-DD DATE OF MOST RECENT GOOD BACKUP  DBBACKUP=/export/backup/$DATE/db-01.batlab.org  KOJIBACKUP=/export/backup/$DATE/koji.chtc.wisc.edu  RSYNC= rsync --archive --hard-links --verbose", 
            "title": "Setting up your environment"
        }, 
        {
            "location": "/infrastructure/koji-restore-recipe/#restoring-the-database", 
            "text": "The entire filesystem of  db-01  is backed up -- this includes all of  /var/lib/pgsql , including the database as-is. In theory, this means that we could just rsync all the files to a blank hard drive, boot up, and we'd have a  db-01  again. However, the Postgres manual warns against restoring the database from a filesystem backup that was made while the database was live, and we do not shut down the database before backups.  We might be able to restore every other part of the filesystem besides the database, which would speed up the overall restoration process, but only the fresh install was tested.  The new database server is called  newdb  in these instructions.", 
            "title": "Restoring the database"
        }, 
        {
            "location": "/infrastructure/koji-restore-recipe/#restoring-services", 
            "text": "Prerequisites for  newdb : an EL 6+ host with an SSH server set up and accessible (as root) from  host-3.chtc.wisc.edu  # # On newdb:  # # Install postgres, get a blank DB up and create the user that koji  # # will be using.  [root@newdb]#  yum install -y postgresql-server [root@newdb]#  service postgresql initdb [root@newdb]#  useradd -r -m koji # # Make a directory we ll put the restored files into.  [root@newdb]#  mkdir -p /root/dbrestore # # On host-3:  [you@host-3]$  sudo  $RSYNC   $DBBACKUP /homefs/   $NEWDB :/root/dbrestore/home [you@host-3]$   for  dir in root etc var ;   do   \\ \n   sudo  $RSYNC   $DBBACKUP /rootfs/ $dir /  \\ \n       $NEWDB :/root/dbrestore/ $dir   \\ \n    done   Continue on to the next section", 
            "title": "Restoring Services"
        }, 
        {
            "location": "/infrastructure/koji-restore-recipe/#restoring-database-contents", 
            "text": "Assumes you have restored the /var directory from backup into  /root/dbrestore/var .    Restore the postgres config files so the koji-hub daemon can log in:  # # On newdb:  [root@newdb]#  service postgresql stop [root@newdb]#  cp -a /root/dbrestore/var/lib/pgsql/data/ { *.conf,postmaster.opts }   \\ \n    /var/lib/pgsql/data/    Edit  /var/lib/pgsql/data/pg_hba.conf . There are lines like:  # Koji-hub IPv4:\nhost koji koji 128.104.100.41/32 md5  Change the IP address to the public IP address of the host that will serve as the new hub.    Restore the actual database:  # # On newdb:  [root@newdb]#  chown -R postgres:postgres /var/lib/pgsql/* [root@newdb]#  service postgresql start [root@newdb]#  gunzip -c /root/dbrestore/var/lib/pgsql-backup/postgres-db-01.sql.gz  |      psql -U postgres postgres", 
            "title": "Restoring Database Contents"
        }, 
        {
            "location": "/infrastructure/koji-restore-recipe/#validation", 
            "text": "Do the following tests to make sure the database is ready to use:    Test that the contents got properly restored:  [root@newdb]#  psql -U koji koji koji   koji=   select   *   from   users ;  koji=   select   *   from   build   order   by   id   desc   limit   10 ;     Test logging in as the koji user:  [root@newdb]#  psql -U koji -h newdb koji  (you must use the FQDN of  newdb , not  localhost ).\nBe sure you get prompted for a password, and the password from  /etc/koji-hub/hub.conf  works.    Continue to \"Restoring Koji\" if needed, otherwise skip to \"Starting Services and Validation\"", 
            "title": "Validation"
        }, 
        {
            "location": "/infrastructure/koji-restore-recipe/#restoring-koji", 
            "text": "Both the root filesystem of  koji.chtc  and  /mnt/koji  are backed up. The root filesystem backups are in the  rootfs  subdirectory of  /export/backup/$DATE/koji.chtc.wisc.edu  and the backups of  /mnt/koji  are in the  kojifs  subdirectory.  The following instructions show how to restore the critical components of Koji onto a new machine.  In the instructions, the new host will be named  newkoji .", 
            "title": "Restoring Koji"
        }, 
        {
            "location": "/infrastructure/koji-restore-recipe/#installing-the-os", 
            "text": "Prerequisites for  newkoji : an EL 6 host with an SSH server set up and accessible (as root) from  host-3.chtc.wisc.edu   \n(This recipe was tested for EL 6, on the same machine as  newdb ).    Install EPEL and OSG repos:  [root@newkoji]#  rpm -Uvh  \\ \n    https://dl.fedoraproject.org/pub/epel/epel-release-latest-6.noarch.rpm  \\ \n    https://repo.grid.iu.edu/osg/3.4/osg-3.4-el6-release-latest.rpm [root@newkoji]#  yum install -y yum-plugin-priorities    Edit  /etc/yum.repos.d/osg-*development.repo :   Enable the development repo  Add  includepkg=koji*  to the definition for the development repo     Go through the other repo files and make sure that EPEL and OS priorities are worse than 98.\n    This means absent or numerically greater.\n    Especially look at  cobbler-config.repo  if it exists.    Install the koji packages and dependencies, making sure the koji packages themselves come from osg:  [root@newkoji]#  yum install koji koji-builder koji-hub koji-plugin-sign  \\ \n    koji-theme-fedora koji-utils koji-web mod_ssl postgresql    Mount  /mnt/koji  if necessary    Restore the contents of the koji filesystem. On  host-3 :  # # At a minimum, you must restore the /mnt/koji/packages directory  [you@host-3]$  sudo  $RSYNC   $KOJIBACKUP /kojifs/packages/  $NEWKOJI :/mnt/koji/packages # # The other directories are optional, though it saves a lot of time to restore /mnt/koji/repos  [you@host-3]$  sudo  $RSYNC   $KOJIBACKUP /kojifs/repos/  $NEWKOJI :/mnt/koji/repos [you@host-3]$  sudo  $RSYNC   $KOJIBACKUP /kojifs/work/  $NEWKOJI :/mnt/koji/work [you@host-3]$  sudo  $RSYNC   $KOJIBACKUP /kojifs/scratch/  $NEWKOJI :/mnt/koji/scratch # # Any dirs you did not restore should be created.     Fix permissions if needed. On  newkoji :  [root@newkoji]#  chown -R apache:apache /mnt/koji/ { packages,repos,work,scratch }  [root@newkoji]#  chmod  0755  /mnt/koji/ { packages,repos,work,scratch }     Continue on to the next section", 
            "title": "Installing the OS"
        }, 
        {
            "location": "/infrastructure/koji-restore-recipe/#restoring-configuration", 
            "text": "On  newkoji , define the shell function  dirclone , listed below:  dirclone  ()   { \n    srcdir = $( dirname  $1 ) / $( basename  $1 ) \n    destdir = $( dirname  $2 ) / $( basename  $2 ) \n   mkdir -p  $( dirname  $2 ) \n   rsync --archive --delete-after --acls --xattrs  \\ \n         --partial --partial-dir = .rsync-partial  \\ \n          $srcdir /   $destdir  }     On  newkoji :  [root@newkoji]#  mkdir -p /root/hubrestore    On  host-3 :  [you@host-3]$  sudo  $RSYNC   $KOJIBACKUP /rootfs/ { root,home,etc }   $NEWKOJI :/root/hubrestore/ [you@host-3]$  sudo  $RSYNC   $KOJIBACKUP /varfs/  $NEWKOJI :/root/hubrestore/var/    On  newkoji , install some utils we will need later:  [root@newkoji]#  yum install -y dos2unix vim-enhanced  (vim-enhanced is used for vimdiff)    On  newkoji :  # # Restore some of the directories in /etc:  [root@newkoji]#   while   read  subtree ;   do      dirclone /root/hubrestore/etc/$subtree /etc/$subtree  done  ___END___  httpd  kojid  koji-hub  kojira  koji-sign-plugin  kojiweb  mock  pki/koji  pki/tls/certs  pki/tls/private  ___END___  # # Restore some of the files:  [root@newkoji]#  cp -a /root/hubrestore/etc/koji.conf /etc/ [root@newkoji]#  cp -a /root/hubrestore/etc/sysconfig/ { httpd,kojid,kojira }  /etc/sysconfig/    Restore users and home directories    If  newkoji  is on a separate host from  newdb , then just simply copy over the files:  [root@newkoji]#  dirclone /root/hubrestore/home /home [root@newkoji]#  cp -a /root/hubrestore/etc/ { passwd,shadow,group,gshadow }  /etc    If  newkoji  is on the same host as  newdb , then you will have to be more careful:  # # Skip home directories for the special users  [root@newkoji]#   for  dir in /root/hubrestore/home/* ;   do      bndir=$(basename  $dir )      if [[ $bndir = koji   $bndir = postgres ]]; then          dirclone  $dir  /home/ $bndir      fi  done  # # Now merge the passwd, group, shadow, and gshadow files in /etc.  # # Make sure that your editor does not create backup files  # # ( set nobackup  in vim), and that shadow and gshadow are owned by  # # root and have 0400 permissions.       Ensure a 'koji' user exists    Fix dirs in  /var :  [root@newkoji]#  rm -rf /var/lib/mock/* [root@newkoji]#  chown root:mock /var/lib/mock [root@newkoji]#  chmod  2775  /var/lib/mock    Restore  /var/www/html  and  /var/spool/cron   \n    (TODO)  /var  should have been backed up, but in case it isn't, the following files need to exist in  /var/www/html :   A symlink  mnt -  /mnt  A robots.txt with contents User-agent: *\nDisallow: /", 
            "title": "Restoring Configuration"
        }, 
        {
            "location": "/infrastructure/koji-restore-recipe/#fixing-names", 
            "text": "This section should be done if  newdb  or  newkoji  do not have the same as the previous db server and hub (i.e.  db-01.batlab.org  and  koji.chtc.wisc.edu ). This section should be completed on  newkoji .", 
            "title": "Fixing Names"
        }, 
        {
            "location": "/infrastructure/koji-restore-recipe/#fixing-config-files-if-newdb-was-renamed", 
            "text": "The only change that's needed if  newdb  was renamed is to  /etc/koji-hub/hub.conf . Edit that file and change the DBHost line to point to the new hostname. After editing, make sure  hub.conf  is owned by  root:apache  and chmodded 0640.", 
            "title": "Fixing config files if newdb was renamed"
        }, 
        {
            "location": "/infrastructure/koji-restore-recipe/#installing-new-certkey-pairs-for-newkoji", 
            "text": "You will need two cert/key pairs: one for the host, and one for the kojira service. Run  dos2unix  on all cert and key files before using them. Define the shell function  makepem , listed below.  makepem  combines a public and private keypair to make a .pem file that the koji services use.  Usage:  makepem  CERTFILE   KEYFILE   OUTPUT_FILE  makepem  ()   { \n     certfile = $1 \n     keyfile = $2 \n     outputfile = $3 \n     ( set  -e\n     keymodulus = $( openssl rsa -noout -modulus -in  $keyfile ) \n     certmodulus = $( openssl x509 -noout -modulus -in  $certfile ) \n     if   [[   $keymodulus   =   $certmodulus   ]]   ;   then \n         echo   keyfile and certfile do not match ;   return   1 \n     fi \n     if   [[  -f  $outputfile   ]]   ;   then \n        mv -f  $outputfile { ,.bak } \n     fi \n     ( dos2unix    $certfile ;  echo ;  dos2unix    $keyfile )     $outputfile \n    chmod  0600   $outputfile \n     )  }   Place cert and key files into the following paths:           host cert  /root/hostcert.pem    host key  /root/hostkey.pem    kojira cert  /root/kojiracert.pem    kojira key  /root/kojirakey.pem     Then use  makepem  to combine the certs and put them in the proper locations.  [root@newkoji]#  makepem /root/hostcert.pem /root/hostkey.pem  \\ \n   /etc/pki/tls/private/kojiweb.pem [root@newkoji]#  makepem /root/kojiracert.pem /root/kojirakey.pem  \\ \n   /etc/pki/tls/private/kojira.pem [root@newkoji]#  chown apache:apache /etc/pki/tls/private/kojiweb.pem [root@newkoji]#  chown root:root /etc/pki/tls/private/kojira.pem  In addition, copy the host cert and key into the locations HTTPD expects it them.  [root@newkoji]#  cp -a /root/hostcert.pem /etc/pki/tls/certs/hostcert.pem [root@newkoji]#  cp -a /root/hostkey.pem /etc/pki/tls/private/hostkey.pem", 
            "title": "Installing new cert/key pairs for newkoji"
        }, 
        {
            "location": "/infrastructure/koji-restore-recipe/#fixing-hostname-in-config-files", 
            "text": "Use sed to replace the hostname in the following config files in /etc:   /etc/kojira/kojira.conf  /etc/koji.conf  /etc/koji-hub/hub.conf  /etc/httpd/conf.d/kojiweb.conf  /etc/httpd/conf/httpd.conf  /etc/kojid/kojid.conf   You will need to fix  /etc/kojid/kojid.conf  on all builder machines as well (e.g.  kojibuilder2.chtc.wisc.edu ).", 
            "title": "Fixing hostname in config files"
        }, 
        {
            "location": "/infrastructure/koji-restore-recipe/#fixing-hostname-in-database", 
            "text": "You will need to find and fix entries that contain the hostname in the following tables:   host  (should be 1 entry)  users  (should be 2 entries, one for the host, and one for the kojira user)", 
            "title": "Fixing hostname in database"
        }, 
        {
            "location": "/infrastructure/koji-restore-recipe/#fixing-hostname-elsewhere", 
            "text": "These steps are only necessary if you cannot get a DNS Canonical Name (CN) record such that  koji.chtc.wisc.edu  resolves to  newkoji .   Update the repo definitions in the  osg-release  package  Update the mash script(s) at the GOC  Mail the software team and users that anyone using the  minefield  repos will need to update  osg-release  Fix all the build machines to point to the new name  Fix the following files in  osg-build  and make a new release  data/osg-koji-home.conf  data/osg-koji-site.conf  osgbuild/constants.py  osgbuild/kojiinter.py    Mail people that they will need to update  osg-build  and rerun  osg-koji setup", 
            "title": "Fixing hostname elsewhere"
        }, 
        {
            "location": "/infrastructure/koji-restore-recipe/#starting-services-and-validation", 
            "text": "Now you will start up Koji services and verify that they function.\nPrerequisite: previous restore steps have been completed and  postgresql  is running on the database host.  All steps will be run on  newkoji .    Start the main koji daemon:  [root@newkoji]#  service httpd start    Use  ps  to verify that it came up   Connect to the web interface in your browser.\n    Make sure you can use https and you can log in.  As yourself, run the  koji  command-line tool and make a few queries (e.g. list-tags)   Start the koji build daemon:  [root@newkoji]#  service kojid start    Use  ps  to verify that it came up    If you did not restore the  /mnt/koji/repos  directory, you will now need to regenerate the build repos.\nUse  koji list-tags  to get a list of tags and run  koji regen-repo  on all of the ones with  -build  in the name.\nThis  will  take several hours.\nYou will also need to regen the  -development  repos so that  minefield  works again. Keep an eye on the tasks in the web interface to make sure they are getting farmed out to the right hosts.   Try a scratch build   Start  kojira :  [root@newkoji]#  service kojira start    Use  ps  to verify that it came up   Wait half a minute and use  ps  to verify that  kojid  is still up; the two processes can kick each other off if they are both using the same certificate  Bump a package if needed and try a real, non-scratch build  Make sure that kojira is regenerating the repos   If you have updated  osg-release  and/or  osg-build , rebuild those packages now.", 
            "title": "Starting Services and Validation"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/", 
            "text": "Notes on Koji-Hub Setup\n\n\nCurrent Koji documentation\n may be of use.\n\n\nTags\n\n\nBase tags\n\n\ndist-el[567]-build\n\n\nTag list\n\n\nThese tags contains 3 external repos each, hosted locally under \nhttp://mirror.batlab.org/pub/linux\n:\n\n\n\n\ndist-el[567]-epel\n: A mirror of the EPEL 5/6/7 repositories\n\n\ndist-el[567]-centos*-os\n: A mirror of the base CentOS repositories\n\n\ndist-el[567]-centos*-updates*\n: A mirror of the CentOS Updates repositories\n\n\n\n\nWe don't put any packages in them (except for ones required for building, like \nbuildsys-macros\n and \nfetch-sources\n), and generally don't build from them directly, but use tag inheritance.\n\n\nosg-el[567]\n\n\nTag list\n\n\nThese tags contains all the \npackage names\n that we put into OSG; \nkoji add-pkg\n adds to them. \nosg-build\n does this automatically. The tags do not actually contain any builds (i.e. packages with version-release). All the other \nosg-*\n tags inherit from these (either directly or indirectly). The purpose of this is to make promoting builds easier, since it keeps you from having to run \nadd-pkg\n when you promote.\n\n\nkojira-fake\n\n\nTag\n\n\nThis tag (and targets building to it) were created because \nkojira\n does not automatically regenerate a repo unless it's the source of another repo. Without this, the osg-development tags (for example) wouldn't get regenerated automatically after a build.\n\n\nMain OSG tags\n\n\nosg-3.[123]-el[567]-build\n\n\nTag list\n\n\nThese are used to initialize the buildroot of most packages we make. They inherit from their respective dist-build and osg-development tags. The EL5 and EL6 tags also contain the \njpackage[56]-bin\n external repos under \nhttp://mirror.batlab.org/pub/jpackage/\n since we use those for some builds.\n\n\nNote\n that the JPackage external repos must have a better priority than the OS and EPEL external repos to avoid build problems for Java packages.\n\n\nosg-upcoming-el[567]-build\n\n\nTag list\n\n\nThese tags are special in that they also need to inherit from the latest mainline osg-build repo (that is, if trunk is 3.3, then \nosg-upcoming-el6-build\n should inherit from \nosg-3.3-el6-build\n).\n\n\nosg-*-el[567]-development\n\n\nTag list\n\n\nThese contain the builds in the \nosg-minefield\n repos. The \nosg-development\n repos hosted by the GOC take packages from this, so \nosg-development\n is pretty much \nosg-minefield\n after a 1-hour delay. They inherit from osg-testing (and occasionally from the more specialized branches like el5-gt52-experimental, though that is now discouraged). Builds that are made using the \nosg-el[567]\n targets (default if you're using \nosg-build\n) get their buildroots from the newest osg-build tags and put their results in the newest osg-development tags.\n\n\nosg-*-el[567]-testing\n\n\nTag list\n\n\nThese contain the builds in the \nosg-testing\n repos. They inherit from the respective \nosg-release\n tags.\n\n\nosg-*-el[567]-prerelease\n\n\nTag list\n\n\nThese are a staging are for packages that we are \ncertain\n will be released in the next release. They are otherwise empty. These are used for testing and for building the tarball clients.\n\n\nosg-*-el[567]-release\n\n\nTag list\n\n\nThese contain the builds in the \nosg-release\n repos. They should be locked except for when moving packages from the \nosg-prerelease\n repos to the \nosg-release\n repos. They inherit from \nosg-el[567]\n.\n\n\nosg-3.[123]-el[567]-release-build\n\n\nTag list\n\n\nThese inherit the \ndist-*-build\n tags and the \nosg-*-release\n tags, putting a base OS along with OSG packages in a single repo, without the need for yum priorities. It is used, along with \nosg-*-prerelease\n, for building the tarball client. Note that there are no \nrelease-build\n repos for upcoming.\n\n\nosg-3.[123]-el[567]-contrib\n\n\nTag list\n\n\nThese contain the builds in the \nosg-contrib\n repos. Note that there are no \nosg-upcoming-contrib\n repos.\n\n\nSpecialized tags\n\n\nThese tags are generally made for long projects which may be in an unstable state and should not interfere with the main development of OSG packages. An example is a full-scale Globus update, where many packages have to be built, using each other as dependencies, and the whole system is not considered usable until all the updates are done. They should generally be removed after the work is done.\n\n\nel[67]-globus and el[67]-globus-build\n\n\nThese tags were made by Matyas Selmeci for mass Globus updates.\n\n\nCollaborator Tags\n\n\nhcc-*\n\n\nFor use by the Holland Computing Center at UNL.\n\n\nBuild Targets\n\n\nA koji \ntarget\n pairs a build tag (which contains packages needed to build software) and a destination tag (which the software will be tagged into once it is built).\n\n\nosg-el[567]\n\n\nThese build from the osg-*-el[567]-build tags for the current release series into the osg-*-el[567]-development tags and are the primary targets used for building OSG software.\n\n\nosg-3.[123]-el[567]\n\n\nThese build from the osg-*-el[567]-build tags into the osg-*-el[567]-development tags and are used for building to releases other than the current one.\n\n\nosg-upcoming-el[567]\n\n\nThese build from the osg-upcoming-el[567]-build tags into the osg-upcoming-el[567]-development tags and are used for building to upcoming.\n\n\ndist-el[567]-build\n\n\nThese build from the \ndist-el*-build\n tag directly into the \ndist-el*-build\n tag. It is used for making builds that should be in every buildroot.\n\n\nkojira-fake-*\n\n\nThese fool kojira into regenerating their build tags as repos we can yum install from. Without this, the osg-development tags (for example) wouldn't get regenerated and osg-minefield wouldn't work.\n\n\nhcc-el[567], panda-el6\n\n\nThese were made for builds made for our collaborators to build into their tags.\n\n\nSigning plugin\n\n\nThe signing plugin is used to sign packages right after they are built. We give it a GPG signing key and corresponding passphrase. It is configured per build tag. The current default is to use the OSG key to sign if a configuration is not specified. This is because it's very difficult to sign packages after the fact, so it's better to erroneously sign some of them with the wrong key than to not sign them.\n\n\nIt is therefore important that whenever a new build tag is created, a corresponding config section for the signing plugin is added, too.\n\n\nThis comes from the package \nkoji-plugin-sign\n and has configs in \n/etc/koji-sign-plugin\n and \n/etc/koji-hub/plugins\n. There is a script called \nfix-permissions\n in both directories that will make sure the plugin can read the config.\n\n\nTweaks\n\n\nThese are local config changes we needed to make to get certain features to work.\n\n\nUsing proxy certs\n\n\n/etc/sysconfig/httpd\n needed to be changed to include the following lines:\n\n\nOPENSSL_ALLOW_PROXY=1\nOPENSSL_ALLOW_PROXY_CERTS=1\n\nexport OPENSSL_ALLOW_PROXY\nexport OPENSSL_ALLOW_PROXY_CERTS\n\n\n\n\n\nThe user must use RFC proxies and must have a version of the koji client of 1.6.0-6.osg or newer.\n\n\nProcedures\n\n\nUser cert switch\n\n\nThe following steps need to be taken for someone replacing their user cert:\n\n\n\n\nAn admin should log in to the database and update the \nusers\n table with their new CN.\n\n\nThe user should import their new cert into their browser and rerun \nosg-koji setup\n to fix their \nclient.crt\n.\n\n\nIf the user gets a Python stack trace when connecting to \nkoji.chtc.wisc.edu\n via their browser, they should clear their cookies.\n\n\n\n\nAdding CAs for user authentication\n\n\nSince our Koji instance uses certs for auth, we specify which CAs we trust for signing user certs. The CA certs for user auth are concatenated together in the file \n/etc/pki/tls/certs/allowed-cas-for-users.crt\n. A comment line before the \nBEGIN CERTIFICATE\n line is used to name the file the cert comes from. We take the certs from the \nosg-ca-certs\n repository.\n\n\nFor example, when I added the CERN CAs to the bundle, I installed osg-ca-certs onto a Fermicloud VM, copied \n/etc/grid-security/certificates/CERN-TCA.pem\n (which signed user certs) and \nCERN-Root.pem\n (which signed \nCERN-TCA.pem\n) to \nkoji.chtc.wisc.edu\n, catted them to the end of the \nallowed-cas-for-users.crt\n file, edited the file to add comments before the certs, and restarted \nhttpd\n.\n\n\nA few tidbits of knowledge for administrators of our Koji server:\n\n\n\n\nThree services need to be running for koji-hub to be functioning: kojira, kojid, and the Apache web server. To restart these:\n\n\nservice kojid restart\n\n\nservice kojira restart\n\n\nservice httpd restart\n\n\n\n\n\n\nLogfiles can be found here:\n\n\n/var/log/kojid.log\n\n\n/var/log/kojira.log\n\n\n/var/log/messages\n\n\n\n\n\n\nkojid is configured to stop starting new tasks if it has less than 8GB free.\n\n\nFailed build roots are kept for 4 hours, and each build root is about 1GB currently. Hence, if too many tasks fail, the kojid might stop accepting new tasks for 4 hours.\n\n\nManually clean these out.\n\n\n\n\n\n\n\n\nKoji Permissions\n\n\nTo add a new user to Koji for someone with a given DN, first extract the CN. For example, Alain has the DN \n/DC=org/DC=doegrids/OU=People/CN=Alain Roy 424511\n, and the CN is just \nAlain Roy 424511\n. The commands below use just the CN.\n\n\n[you@client ~]$\n osg-koji add-user \nCN\n\n\n[you@client ~]$\n osg-koji grant-permission build \nCN\n\n\n[you@client ~]$\n osg-koji grant-permission repo \nCN\n\n\n\n\n\n\nIf you want to see the set of possible permissions:\n\n\n[you@client ~]$\n koji list-permissions \n\nEnter PEM pass phrase: \n\n\nadmin\n\n\nbuild\n\n\nrepo\n\n\nlivecd\n\n\nmaven-import\n\n\nwin-import\n\n\nwin-admin\n\n\nappliance\n\n\n\n\n\n\nIf you want to see someone's permissions:\n\n\n[you@client ~]$\n koji list-permissions --user \nAlain Roy 424511\n\n\nEnter PEM pass phrase: \n\n\nadmin\n\n\n\n\n\n\nIf you want to see your own permissions:\n\n\n[you@client ~]$\n koji list-permissions --mine\n\nEnter PEM pass phrase: \n\n\nadmin\n\n\n\n\n\n\nTo see the list of users, go to the \nKoji web site\n.\n\n\nRenewing host and service certs\n\n\nCerts and keys are stored in \n/p/condor/home/certificates/...\n\n\nTo obtain renewed certificates, use \nthe OSG PKI commandline clients\n or the web interface at \nhttps://oim.grid.iu.edu/oim/certificaterequesthost\n.\n\n\nThe following cert files are necessary:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhostcert.pem\n\n\nkoji.chtc host cert (\nCN=koji.chtc.wisc.edu\n)\n\n\n\n\n\n\nhostkey.pem\n\n\nkoji.chtc host key\n\n\n\n\n\n\nkojiracert.pem\n\n\nkoji.chtc/kojira service cert (\nCN=koji.chtc.wisc.edu/kojira\n)\n\n\n\n\n\n\nkojirakey.pem\n\n\nkoji.chtc/kojira service key\n\n\n\n\n\n\nkojiweb.pem\n\n\nConcatenation of \nhostcert.pem\n and \nhostkey.pem\n\n\n\n\n\n\nkojira.pem\n\n\nConcatenation of \nkojiracert.pem\n and \nkojirakey.pem\n\n\n\n\n\n\n\n\nTo create \nkojiweb.pem\n and \nkojira.pem\n from their respective cert/key files, do:\n\n\n[root@koji ~]#\n \n(\ndos2unix \n hostcert.pem\n;\n echo\n;\n dos2unix \n hostkey.pem\n)\n \n kojiweb.pem\n\n[root@koji ~]#\n chmod \n0600\n kojiweb.pem\n\n[root@koji ~]#\n \n(\ndos2unix \n kojiracert.pem\n;\n echo\n;\n dos2unix \n kojirakey.pem\n)\n \n kojira.pem\n\n[root@koji ~]#\n chmod \n0600\n kojira.pem\n\n\n\n\n\nAs \nroot\n in \n/etc\n on \nkoji.chtc.wisc.edu\n:\n\n\n\n\nRun \ngit status\n.\n\n\nRun \netckeeper commit\n to commit any uncommited changes (including unversioned files).\n\n\n\n\nPut the files onto \nkoji.chtc.wisc.edu\n as follows:\n\n\n\n\n\n\n\n\nFile\n\n\nLocation\n\n\nchown\n\n\nchmod\n\n\n\n\n\n\n\n\n\n\nhostcert.pem\n\n\n/etc/pki/tls/certs/hostcert.pem\n\n\nroot:root\n\n\n0644\n\n\n\n\n\n\nhostkey.pem\n\n\n/etc/pki/tls/private/hostkey.pem\n\n\nroot:root\n\n\n0600\n\n\n\n\n\n\nkojiweb.pem\n\n\n/etc/pki/tls/private/kojiweb.pem\n\n\napache:apache\n\n\n0600\n\n\n\n\n\n\nkojira.pem\n\n\n/etc/pki/tls/private/kojira.pem\n\n\nroot:root\n\n\n0600\n\n\n\n\n\n\n\n\n\n\nShut down \nkojira\n and \nkojid\n.\n\n\nRestart \nhttpd\n.\n\n\nLog in via your own cert to the web interface to verify that it is working.\n\n\nRun \nosg-koji list-permissions --mine\n to verify command-line access is working.\n\n\nRun \netckeeper commit\n to commit your changes in \n/etc\n.\n\n\nStart up \nkojira\n and \nkojid\n.", 
            "title": "Koji-Hub Setup"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#notes-on-koji-hub-setup", 
            "text": "Current Koji documentation  may be of use.", 
            "title": "Notes on Koji-Hub Setup"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#tags", 
            "text": "", 
            "title": "Tags"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#base-tags", 
            "text": "", 
            "title": "Base tags"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#dist-el9156793-build", 
            "text": "Tag list  These tags contains 3 external repos each, hosted locally under  http://mirror.batlab.org/pub/linux :   dist-el[567]-epel : A mirror of the EPEL 5/6/7 repositories  dist-el[567]-centos*-os : A mirror of the base CentOS repositories  dist-el[567]-centos*-updates* : A mirror of the CentOS Updates repositories   We don't put any packages in them (except for ones required for building, like  buildsys-macros  and  fetch-sources ), and generally don't build from them directly, but use tag inheritance.", 
            "title": "dist-el[567]-build"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#osg-el9156793", 
            "text": "Tag list  These tags contains all the  package names  that we put into OSG;  koji add-pkg  adds to them.  osg-build  does this automatically. The tags do not actually contain any builds (i.e. packages with version-release). All the other  osg-*  tags inherit from these (either directly or indirectly). The purpose of this is to make promoting builds easier, since it keeps you from having to run  add-pkg  when you promote.", 
            "title": "osg-el[567]"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#kojira-fake", 
            "text": "Tag  This tag (and targets building to it) were created because  kojira  does not automatically regenerate a repo unless it's the source of another repo. Without this, the osg-development tags (for example) wouldn't get regenerated automatically after a build.", 
            "title": "kojira-fake"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#main-osg-tags", 
            "text": "", 
            "title": "Main OSG tags"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#osg-39112393-el9156793-build", 
            "text": "Tag list  These are used to initialize the buildroot of most packages we make. They inherit from their respective dist-build and osg-development tags. The EL5 and EL6 tags also contain the  jpackage[56]-bin  external repos under  http://mirror.batlab.org/pub/jpackage/  since we use those for some builds.  Note  that the JPackage external repos must have a better priority than the OS and EPEL external repos to avoid build problems for Java packages.", 
            "title": "osg-3.[123]-el[567]-build"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#osg-upcoming-el9156793-build", 
            "text": "Tag list  These tags are special in that they also need to inherit from the latest mainline osg-build repo (that is, if trunk is 3.3, then  osg-upcoming-el6-build  should inherit from  osg-3.3-el6-build ).", 
            "title": "osg-upcoming-el[567]-build"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#osg-42-el9156793-development", 
            "text": "Tag list  These contain the builds in the  osg-minefield  repos. The  osg-development  repos hosted by the GOC take packages from this, so  osg-development  is pretty much  osg-minefield  after a 1-hour delay. They inherit from osg-testing (and occasionally from the more specialized branches like el5-gt52-experimental, though that is now discouraged). Builds that are made using the  osg-el[567]  targets (default if you're using  osg-build ) get their buildroots from the newest osg-build tags and put their results in the newest osg-development tags.", 
            "title": "osg-*-el[567]-development"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#osg-42-el9156793-testing", 
            "text": "Tag list  These contain the builds in the  osg-testing  repos. They inherit from the respective  osg-release  tags.", 
            "title": "osg-*-el[567]-testing"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#osg-42-el9156793-prerelease", 
            "text": "Tag list  These are a staging are for packages that we are  certain  will be released in the next release. They are otherwise empty. These are used for testing and for building the tarball clients.", 
            "title": "osg-*-el[567]-prerelease"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#osg-42-el9156793-release", 
            "text": "Tag list  These contain the builds in the  osg-release  repos. They should be locked except for when moving packages from the  osg-prerelease  repos to the  osg-release  repos. They inherit from  osg-el[567] .", 
            "title": "osg-*-el[567]-release"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#osg-39112393-el9156793-release-build", 
            "text": "Tag list  These inherit the  dist-*-build  tags and the  osg-*-release  tags, putting a base OS along with OSG packages in a single repo, without the need for yum priorities. It is used, along with  osg-*-prerelease , for building the tarball client. Note that there are no  release-build  repos for upcoming.", 
            "title": "osg-3.[123]-el[567]-release-build"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#osg-39112393-el9156793-contrib", 
            "text": "Tag list  These contain the builds in the  osg-contrib  repos. Note that there are no  osg-upcoming-contrib  repos.", 
            "title": "osg-3.[123]-el[567]-contrib"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#specialized-tags", 
            "text": "These tags are generally made for long projects which may be in an unstable state and should not interfere with the main development of OSG packages. An example is a full-scale Globus update, where many packages have to be built, using each other as dependencies, and the whole system is not considered usable until all the updates are done. They should generally be removed after the work is done.", 
            "title": "Specialized tags"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#el916793-globus-and-el916793-globus-build", 
            "text": "These tags were made by Matyas Selmeci for mass Globus updates.", 
            "title": "el[67]-globus and el[67]-globus-build"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#collaborator-tags", 
            "text": "", 
            "title": "Collaborator Tags"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#hcc-42", 
            "text": "For use by the Holland Computing Center at UNL.", 
            "title": "hcc-*"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#build-targets", 
            "text": "A koji  target  pairs a build tag (which contains packages needed to build software) and a destination tag (which the software will be tagged into once it is built).", 
            "title": "Build Targets"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#osg-el9156793_1", 
            "text": "These build from the osg-*-el[567]-build tags for the current release series into the osg-*-el[567]-development tags and are the primary targets used for building OSG software.", 
            "title": "osg-el[567]"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#osg-39112393-el9156793", 
            "text": "These build from the osg-*-el[567]-build tags into the osg-*-el[567]-development tags and are used for building to releases other than the current one.", 
            "title": "osg-3.[123]-el[567]"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#osg-upcoming-el9156793", 
            "text": "These build from the osg-upcoming-el[567]-build tags into the osg-upcoming-el[567]-development tags and are used for building to upcoming.", 
            "title": "osg-upcoming-el[567]"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#dist-el9156793-build_1", 
            "text": "These build from the  dist-el*-build  tag directly into the  dist-el*-build  tag. It is used for making builds that should be in every buildroot.", 
            "title": "dist-el[567]-build"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#kojira-fake-42", 
            "text": "These fool kojira into regenerating their build tags as repos we can yum install from. Without this, the osg-development tags (for example) wouldn't get regenerated and osg-minefield wouldn't work.", 
            "title": "kojira-fake-*"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#hcc-el9156793-panda-el6", 
            "text": "These were made for builds made for our collaborators to build into their tags.", 
            "title": "hcc-el[567], panda-el6"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#signing-plugin", 
            "text": "The signing plugin is used to sign packages right after they are built. We give it a GPG signing key and corresponding passphrase. It is configured per build tag. The current default is to use the OSG key to sign if a configuration is not specified. This is because it's very difficult to sign packages after the fact, so it's better to erroneously sign some of them with the wrong key than to not sign them.  It is therefore important that whenever a new build tag is created, a corresponding config section for the signing plugin is added, too.  This comes from the package  koji-plugin-sign  and has configs in  /etc/koji-sign-plugin  and  /etc/koji-hub/plugins . There is a script called  fix-permissions  in both directories that will make sure the plugin can read the config.", 
            "title": "Signing plugin"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#tweaks", 
            "text": "These are local config changes we needed to make to get certain features to work.", 
            "title": "Tweaks"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#using-proxy-certs", 
            "text": "/etc/sysconfig/httpd  needed to be changed to include the following lines:  OPENSSL_ALLOW_PROXY=1\nOPENSSL_ALLOW_PROXY_CERTS=1\n\nexport OPENSSL_ALLOW_PROXY\nexport OPENSSL_ALLOW_PROXY_CERTS  The user must use RFC proxies and must have a version of the koji client of 1.6.0-6.osg or newer.", 
            "title": "Using proxy certs"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#procedures", 
            "text": "", 
            "title": "Procedures"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#user-cert-switch", 
            "text": "The following steps need to be taken for someone replacing their user cert:   An admin should log in to the database and update the  users  table with their new CN.  The user should import their new cert into their browser and rerun  osg-koji setup  to fix their  client.crt .  If the user gets a Python stack trace when connecting to  koji.chtc.wisc.edu  via their browser, they should clear their cookies.", 
            "title": "User cert switch"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#adding-cas-for-user-authentication", 
            "text": "Since our Koji instance uses certs for auth, we specify which CAs we trust for signing user certs. The CA certs for user auth are concatenated together in the file  /etc/pki/tls/certs/allowed-cas-for-users.crt . A comment line before the  BEGIN CERTIFICATE  line is used to name the file the cert comes from. We take the certs from the  osg-ca-certs  repository.  For example, when I added the CERN CAs to the bundle, I installed osg-ca-certs onto a Fermicloud VM, copied  /etc/grid-security/certificates/CERN-TCA.pem  (which signed user certs) and  CERN-Root.pem  (which signed  CERN-TCA.pem ) to  koji.chtc.wisc.edu , catted them to the end of the  allowed-cas-for-users.crt  file, edited the file to add comments before the certs, and restarted  httpd .  A few tidbits of knowledge for administrators of our Koji server:   Three services need to be running for koji-hub to be functioning: kojira, kojid, and the Apache web server. To restart these:  service kojid restart  service kojira restart  service httpd restart    Logfiles can be found here:  /var/log/kojid.log  /var/log/kojira.log  /var/log/messages    kojid is configured to stop starting new tasks if it has less than 8GB free.  Failed build roots are kept for 4 hours, and each build root is about 1GB currently. Hence, if too many tasks fail, the kojid might stop accepting new tasks for 4 hours.  Manually clean these out.", 
            "title": "Adding CAs for user authentication"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#koji-permissions", 
            "text": "To add a new user to Koji for someone with a given DN, first extract the CN. For example, Alain has the DN  /DC=org/DC=doegrids/OU=People/CN=Alain Roy 424511 , and the CN is just  Alain Roy 424511 . The commands below use just the CN.  [you@client ~]$  osg-koji add-user  CN  [you@client ~]$  osg-koji grant-permission build  CN  [you@client ~]$  osg-koji grant-permission repo  CN   If you want to see the set of possible permissions:  [you@client ~]$  koji list-permissions  Enter PEM pass phrase:   admin  build  repo  livecd  maven-import  win-import  win-admin  appliance   If you want to see someone's permissions:  [you@client ~]$  koji list-permissions --user  Alain Roy 424511  Enter PEM pass phrase:   admin   If you want to see your own permissions:  [you@client ~]$  koji list-permissions --mine Enter PEM pass phrase:   admin   To see the list of users, go to the  Koji web site .", 
            "title": "Koji Permissions"
        }, 
        {
            "location": "/infrastructure/koji-hub-setup/#renewing-host-and-service-certs", 
            "text": "Certs and keys are stored in  /p/condor/home/certificates/...  To obtain renewed certificates, use  the OSG PKI commandline clients  or the web interface at  https://oim.grid.iu.edu/oim/certificaterequesthost .  The following cert files are necessary:           hostcert.pem  koji.chtc host cert ( CN=koji.chtc.wisc.edu )    hostkey.pem  koji.chtc host key    kojiracert.pem  koji.chtc/kojira service cert ( CN=koji.chtc.wisc.edu/kojira )    kojirakey.pem  koji.chtc/kojira service key    kojiweb.pem  Concatenation of  hostcert.pem  and  hostkey.pem    kojira.pem  Concatenation of  kojiracert.pem  and  kojirakey.pem     To create  kojiweb.pem  and  kojira.pem  from their respective cert/key files, do:  [root@koji ~]#   ( dos2unix   hostcert.pem ;  echo ;  dos2unix   hostkey.pem )    kojiweb.pem [root@koji ~]#  chmod  0600  kojiweb.pem [root@koji ~]#   ( dos2unix   kojiracert.pem ;  echo ;  dos2unix   kojirakey.pem )    kojira.pem [root@koji ~]#  chmod  0600  kojira.pem  As  root  in  /etc  on  koji.chtc.wisc.edu :   Run  git status .  Run  etckeeper commit  to commit any uncommited changes (including unversioned files).   Put the files onto  koji.chtc.wisc.edu  as follows:     File  Location  chown  chmod      hostcert.pem  /etc/pki/tls/certs/hostcert.pem  root:root  0644    hostkey.pem  /etc/pki/tls/private/hostkey.pem  root:root  0600    kojiweb.pem  /etc/pki/tls/private/kojiweb.pem  apache:apache  0600    kojira.pem  /etc/pki/tls/private/kojira.pem  root:root  0600      Shut down  kojira  and  kojid .  Restart  httpd .  Log in via your own cert to the web interface to verify that it is working.  Run  osg-koji list-permissions --mine  to verify command-line access is working.  Run  etckeeper commit  to commit your changes in  /etc .  Start up  kojira  and  kojid .", 
            "title": "Renewing host and service certs"
        }, 
        {
            "location": "/infrastructure/koji-inf-overview/", 
            "text": "Koji Infrastructure Overview\n\n\nIn Madison\n\n\nIn WID\n\n\n\n\nkoji.chtc.wisc.edu\n is the main Koji server. It runs:\n\n\nkoji-hub\n (httpd/mod_python service) -- controls everything else\n\n\nkoji-web\n (httpd/mod_python service) -- provides the web interface to koji-hub\n\n\nkojid\n (standalone daemon) -- builds packages\n\n\nkojira\n (standalone daemon) -- creates tasks to regen repos automatically\n\n\nrsyncd\n (via xinetd)\n\n\npuppetd\n (cron job)\n\n\n(also stores RPMs in its \n/mnt/koji\n directory)\n\n\n\n\n\n\ndb-01.batlab.org\n is the database server. It runs:\n\n\npostgres\n (standalone daemon)\n\n\nrsyncd\n (via xinetd)\n\n\npuppetd\n (cron job)\n\n\n(others)\n\n\n\n\n\n\nhost-3.chtc.wisc.edu\n is a backup server. It runs:\n\n\nrsync\n (cron job)\n\n\npuppetd\n (cron job)\n\n\n(others)\n\n\n\n\n\n\nwid-service-1.chtc.wisc.edu\n is a Puppet Master. It runs:\n\n\npuppetmaster\n (standalone daemon)\n\n\n(others)\n\n\n\n\n\n\n\n\nIn CS (3370A)\n\n\n\n\nosghost.chtc.wisc.edu\n is a VM host. It runs:\n\n\nkojibuilder2.chtc.wisc.edu\n and \nkojibuilder3.chtc.wisc.edu\n are builder VMs. Each runs:\n\n\nkojid\n (standalone daemon) -- builds packages\n\n\npuppetd\n (cron job)\n\n\n\n\n\n\n\n\n\n\n\n\nIn Indiana\n\n\n\n\nrepo1.grid.iu.edu\n, \nrepo2.grid.iu.edu\n and \nrepo-itb.grid.iu.edu\n are repo hosts. Each runs:\n\n\nmash\n (cron job) -- pulls RPMs from a \nkoji-hub\n\n\n(others)\n\n\n\n\n\n\nrepo.grid.iu.edu\n is a DNS alias pointing to either \nrepo1\n or \nrepo2\n\n\n\n\nLines of Communication\n\n\n\n\nkoji-web\n provides a web interface to \nkoji-hub\n\n\nkoji-hub\n sends jobs to all \nkojid\n daemons and gets back the results\n\n\nkojira\n sends requests to \nkoji-hub\n to create tasks\n\n\nkoji-hub\n talks directly to \npostgres\n for all metadata\n\n\nkoji-hub\n writes to and reads from \n/mnt/koji\n\n\nmash\n pulls RPMs from \n/mnt/koji\n and communicates with \nkoji-hub\n to get tag information\n\n\npuppetd\n on all Madison machines pulls Puppet configuration from \npuppetmaster\n on \nwid-service-1\n\n\nrsync\n on \nhost-3.chtc.wisc.edu\n pulls files from \nrsyncd\n on \nkoji.chtc.wisc.edu\n and \ndb-01.batlab.org\n\n\n\n\nManagement\n\n\nManaged by the GOC:\n\n\n\n\nrepo1.grid.iu.edu\n\n\nrepo2.grid.iu.edu\n\n\nrepo.grid.iu.edu\n\n\nrepo-itb.grid.iu.edu\n\n\n\n\nFully managed by CHTC Infrastructure:\n\n\n\n\ndb-01.batlab.org\n\n\nhost-3.chtc.wisc.edu\n\n\nwid-service-1.chtc.wisc.edu\n\n\n\n\nManagement split between CHTC infrastructure and OSG-Software:\n\n\n\n\nkoji.chtc.wisc.edu\n\n\nkojibuilder2.chtc.wisc.edu\n\n\nkojibuilder3.chtc.wisc.edu\n\n\nosghost.chtc.wisc.edu\n\n\n\n\n(In general, CHTC-inf takes care of accounts, firewalls, other basic config, and OSG takes care of the Koji services)", 
            "title": "Koji Infrastructure Overview"
        }, 
        {
            "location": "/infrastructure/koji-inf-overview/#koji-infrastructure-overview", 
            "text": "", 
            "title": "Koji Infrastructure Overview"
        }, 
        {
            "location": "/infrastructure/koji-inf-overview/#in-madison", 
            "text": "", 
            "title": "In Madison"
        }, 
        {
            "location": "/infrastructure/koji-inf-overview/#in-wid", 
            "text": "koji.chtc.wisc.edu  is the main Koji server. It runs:  koji-hub  (httpd/mod_python service) -- controls everything else  koji-web  (httpd/mod_python service) -- provides the web interface to koji-hub  kojid  (standalone daemon) -- builds packages  kojira  (standalone daemon) -- creates tasks to regen repos automatically  rsyncd  (via xinetd)  puppetd  (cron job)  (also stores RPMs in its  /mnt/koji  directory)    db-01.batlab.org  is the database server. It runs:  postgres  (standalone daemon)  rsyncd  (via xinetd)  puppetd  (cron job)  (others)    host-3.chtc.wisc.edu  is a backup server. It runs:  rsync  (cron job)  puppetd  (cron job)  (others)    wid-service-1.chtc.wisc.edu  is a Puppet Master. It runs:  puppetmaster  (standalone daemon)  (others)", 
            "title": "In WID"
        }, 
        {
            "location": "/infrastructure/koji-inf-overview/#in-cs-3370a", 
            "text": "osghost.chtc.wisc.edu  is a VM host. It runs:  kojibuilder2.chtc.wisc.edu  and  kojibuilder3.chtc.wisc.edu  are builder VMs. Each runs:  kojid  (standalone daemon) -- builds packages  puppetd  (cron job)", 
            "title": "In CS (3370A)"
        }, 
        {
            "location": "/infrastructure/koji-inf-overview/#in-indiana", 
            "text": "repo1.grid.iu.edu ,  repo2.grid.iu.edu  and  repo-itb.grid.iu.edu  are repo hosts. Each runs:  mash  (cron job) -- pulls RPMs from a  koji-hub  (others)    repo.grid.iu.edu  is a DNS alias pointing to either  repo1  or  repo2", 
            "title": "In Indiana"
        }, 
        {
            "location": "/infrastructure/koji-inf-overview/#lines-of-communication", 
            "text": "koji-web  provides a web interface to  koji-hub  koji-hub  sends jobs to all  kojid  daemons and gets back the results  kojira  sends requests to  koji-hub  to create tasks  koji-hub  talks directly to  postgres  for all metadata  koji-hub  writes to and reads from  /mnt/koji  mash  pulls RPMs from  /mnt/koji  and communicates with  koji-hub  to get tag information  puppetd  on all Madison machines pulls Puppet configuration from  puppetmaster  on  wid-service-1  rsync  on  host-3.chtc.wisc.edu  pulls files from  rsyncd  on  koji.chtc.wisc.edu  and  db-01.batlab.org", 
            "title": "Lines of Communication"
        }, 
        {
            "location": "/infrastructure/koji-inf-overview/#management", 
            "text": "Managed by the GOC:   repo1.grid.iu.edu  repo2.grid.iu.edu  repo.grid.iu.edu  repo-itb.grid.iu.edu   Fully managed by CHTC Infrastructure:   db-01.batlab.org  host-3.chtc.wisc.edu  wid-service-1.chtc.wisc.edu   Management split between CHTC infrastructure and OSG-Software:   koji.chtc.wisc.edu  kojibuilder2.chtc.wisc.edu  kojibuilder3.chtc.wisc.edu  osghost.chtc.wisc.edu   (In general, CHTC-inf takes care of accounts, firewalls, other basic config, and OSG takes care of the Koji services)", 
            "title": "Management"
        }, 
        {
            "location": "/infrastructure/koji-policy-writing/", 
            "text": "Koji permissions and policy\n\n\nThese are some notes I wrote on Koji ACLs/policy after doing some source diving in the Koji code.\nThe version of Koji was 1.6.0.\nI later found some documentation at \nhttps://docs.pagure.org/koji/defining_hub_policies/\n. Go read it, that page has better examples.\n\n\nDefault policies (defined in hub/kojixmlrpc.py)\n\n\nbuild_from_srpm =\n       has_perm admin :: allow\n       all :: deny\nbuild_from_repo_id =\n       has_perm admin :: allow\n       all :: deny\nchannel =\n       has req_channel :: req\n       is_child_task :: parent\n       all :: use default\npackage_list =\n       has_perm admin :: allow\n       all :: deny\nvm =\n       has_perm admin win-admin :: allow\n       all :: deny\n\n\n\n\n\nIf \nMissingPolicyOk\n is true (default), then policies that do not exist default to \"allow\".\n\n\nPolicy syntax\n\n\nPolicies are set in the \n[policy]\n section of \n/etc/koji-hub/hub.conf\n\n\nEach policy definition starts with\n\n\npolicy_name =\n\n\n\n\n\nThe rest of the definition is indented. The lines in the policy definition have the following format:\n\n\nSimple tests:\n\n\ntest [params] [\n test [params] ...] :: action-if-true\ntest [params] [\n test [params] ...] !! action-if-false\n\n\n\n\n\nComplex tests:\n\n\ntest [params [\n ...]] :: {\n   test [params [\n ...]] :: action\n   test [params [\n ...]] :: {\n      ...\n      }\n}\n\n\n\n\n\nThe following generic tests are defined in \nkoji/policy.py\n:\n\n\n\n\n\n\ntrue\n / \nall\n \n\n    always true\n\n\n\n\n\n\nfalse\n / \nnone\n \n\n    always false\n\n\n\n\n\n\nhas FIELD\n \n\n    true if policy data contains a field called FIELD\n\n\n\n\n\n\nbool FIELD\n \n\n    true if FIELD is true\n\n\n\n\n\n\nmatch FIELD PATTERN1 [PATTERN2 ...]\n \n\n    true if FIELD matches any of the patterns (globs)\n\n\n\n\n\n\ncompare FIELD OP NUMBER\n \n\n    compare FIELD against a number. OP can be \n, \n, \n=, \n=, =, !=\n\n\n\n\n\n\nthe following koji-specific tests are defined in \nhub/kojihub.py\n:\n\n\n\n\n\n\nbuildtag PATTERN1 [PATTERN2 ...]\n \n\n    true if the build tag of a build matches a pattern\n\n\n\n\n\n\nfromtag PATTERN1 [PATTERN2 ...]\n \n\n    true if the tag we're moving a package from matches a pattern\n\n\n\n\n\n\nhas_perm PATTERN1 [PATTERN2 ...]\n \n\n    true if user has any matching permission\n\n\n\n\n\n\nhastag TAG\n \n\n    true if the build has the tag TAG\n\n\n\n\n\n\nimported\n \n\n    true if the build was imported\n\n\n\n\n\n\nis_build_owner\n \n\n    true if the user doing this task owns the build\n\n\n\n\n\n\nis_child_task\n \n\n    true if the task is a child of some other task\n\n\n\n\n\n\nis_new_package\n \n\n    true if the package being looked at is new (i.e. doesn't have an 'id' yet)\n\n\n\n\n\n\nmethod PATTERN1 [PATTERN2 ...]\n \n\n    true if the method matches a pattern\n\n\n\n\n\n\noperation PATTERN1 [PATTERN2 ...]\n \n\n    true if current operation matches any of the patterns\n\n\n\n\n\n\npackage PATTERN1 [PATTERN2 ...]\n \n\n    true if the package name matches any of the patterns\n\n\n\n\n\n\npolicy POLICY\n \n\n    true if the named policy is true\n\n\n\n\n\n\nskip_tag\n \n\n    true if the skip_tag option is true\n\n\n\n\n\n\nsource PATTERN1 [PATTERN2 ...]\n \n\n    true if source matches patterns\n\n\n\n\n\n\ntag PATTERN1 [PATTERN2 ...]\n \n\n    true if the tag name matches any of the patterns\n\n\n\n\n\n\nuser PATTERN1 [PATTERN2 ...]\n \n\n    true if username matches a pattern\n\n\n\n\n\n\nuser_in_group PATTERN1 [PATTERN2 ...]\n \n\n    true if the user is in any matching group\n\n\n\n\n\n\nvm_name PATTERN1 [PATTERN2 ...]\n \n\n    true if vm name matches a pattern\n\n\n\n\n\n\nThe actions are:\n\n\n\n\n\n\nallow\n \n\n    allow the action\n\n\n\n\n\n\ndeny\n \n\n    deny the action\n\n\n\n\n\n\nreq\n \n\n    ?\n\n\n\n\n\n\nparent\n \n\n    ?\n\n\n\n\n\n\nuse default\n \n\n    ?\n\n\n\n\n\n\nDefault permissions\n\n\nThese are the permissions that people can be given in koji:\n\n\n\n\nadmin\n\n\nbuild\n\n\nrepo\n\n\nlivecd\n\n\nmaven-import\n\n\nwin-import\n\n\nwin-admin\n\n\nappliance\n\n\n\n\nAs far as I can tell, additional permissions have to be manually added into the 'permissions' table in postgres.\n\n\nThe following permissions are checked by name in the koji command-line utility (i.e. policies are not used):\n\n\n\n\n\n\nadmin\n: \n\n\nadd-group\n, \nadd-tag\n, \nadd-target\n, \nclone-tag\n, \nedit-target\n, \nremove-tag\n, \nremove-target\n, \nwrapper-rpm\n\n\n\n\n\n\nmaven-import\n: \n\n\nimport-archive\n with the \n--type=maven\n option\n\n\n\n\n\n\nwin-import\n: \n\n\nimport-archive\n with the \n--type=win\n option\n\n\n\n\n\n\nrepo\n: \n\n\nregen-repo\n\n\n\n\n\n\nI haven't found out where some of the other permissions are used.\n\n\nAdding permissions\n\n\nGo into postgres and run\n\n\ninsert into permissions values ((select nextval(\npermissions_id_seq\n)), \nNAME\n);\n\n\n\n\n\nwhere NAME is the name of the permission you want to create. You may now grant people that permission and use that name in policies.\n\n\nWhere policies are used and what policy data is passed on:\n\n\nbuild_from_srpm\n\n\nsource:\n\n\n\n\nbuilder/kojid:BuildTask.handler\n \n\n    used when source url points to an SRPM (as opposed to an scm) and the build is not a scratch build.\n\n\n\n\npolicy data:\n\n\n\n\n\n\nuser_id\n \n\n    the owner of the task\n\n\n\n\n\n\nsource\n \n\n    the url of the source file\n\n\n\n\n\n\ntask_id\n \n\n    the id of the task\n\n\n\n\n\n\nbuild_tag\n \n\n    the id of the build tag\n\n\n\n\n\n\nskip_tag\n \n\n    true if we're not tagging this build (\n--scratch\n or \n--skip-tag\n passed on the command line)\n\n\n\n\n\n\ntarget\n \n\n    the build target (only if we have one?)\n\n\n\n\n\n\ntag\n \n\n    the destination tag (only if \nskip_tag\n is false)\n\n\n\n\n\n\nbuild_from_repo_id\n\n\nsource:\n\n\n\n\nbuilder/kojid:BuildTask.handler\n \n\n    used when the \n--repo-id\n option is passed to \nkoji build\n\n\n\n\npolicy data:\n same as \nbuild_from_srpm\n\n\npackage_list\n\n\nsource:\n\n\n\n\nhub/kojihub.py:pkglist_add\n \n\n\nadd-pkg\n, \nblock-pkg\n, \nset-pkg-arches\n, \nset-pkg-owner\n commands\n\n\n\n\npolicy data:\n\n\n\n\n\n\naction\n \n\n    'add', 'update', 'block' depending on what is being done\n\n\n\n\n\n\nforce\n \n\n    true if \n--force\n is passed on the command line\n\n\n\n\n\n\npackage\n \n\n    package info (the id I think?)\n\n\n\n\n\n\ntag\n \n\n    the id of the tag we're trying to add the package to/package is in\n\n\n\n\n\n\nsource:\n\n\n\n\nhub/kojihub.py:pkglist_remove\n \n\n    used internally by the \nkoji clone-tag\n command?\n\n\n\n\npolicy data:\n same as above, except \naction\n is 'remove'\n\n\nsource:\n\n\n\n\nhub/kojihub.py:pkglist_unblock\n \n\n\nunblock-pkg\n command\n\n\n\n\npolicy data:\n same as above, except \naction\n is 'unblock'\n\n\ntag\n\n\n\n\nNote\n\n\nRootExports is the class containing functions exported via XMLRPC. In general, each function corresponds to a koji task.\n\n\n\n\nsource:\n\n\n\n\nhub/kojihub.py:RootExports.tagBuild\n \n\n    tagging builds\n\n\n\n\npolicy data:\n\n\n\n\n\n\nbuild\n \n\n    the id of the build\n\n\n\n\n\n\nfromtag\n \n\n    the id of the tag we're moving the build from, if there is one\n\n\n\n\n\n\noperation\n \n\n    'tag' or 'move'\n\n\n\n\n\n\ntag\n \n\n    the id of the tag\n\n\n\n\n\n\nsource:\n\n\n\n\nhub/kojihub.py:RootExports.untagBuild\n \n\n    untagging builds\n\n\n\n\npolicy data:\n same as above, except \noperation\n is 'untag', and \ntag\n is None\n\n\nsource:\n\n\n\n\nhub/kojihub.py:RootExports.moveAllBuilds\n \n\n    moving all builds of a package from tag1 to tag2\n\n\n\n\npolicy data:\n same as for \ntagBuild\n, except \noperation\n is 'move'. The policy is checked once for each build being moved.\n\n\nsource:\n\n\n\n\nhub/kojihub.py:HostExports.tagBuild\n \n\n    tagging builds (\"host version\" ?)\n\n\n\n\npolicy data:\n same as for \ntagBuild\n, plus \nuser_id\n\n\nvm\n\n\nsource:\n\n\n\n\nhub/kojihub.py:RootExports.winBuild\n \n\n    windows builds in a vm (\nwin-build\n command)\n\n\n\n\npolicy data:\n\n\n\n\n\n\ntag\n \n\n    the destination tag\n\n\n\n\n\n\nvm_name\n \n\n    the name of the vm\n\n\n\n\n\n\nExamples\n\n\nLet people with the \"build\" permission also add packages and build SRPMs\n\n\npackage_list = \n    has_perm admin :: allow\n    has_perm build \n match action add update :: allow\n    all :: deny\n\nbuild_from_srpm =\n    has_perm admin build :: allow\n    all :: deny\n\n\n\n\n\nPromotion policy for different teams\n\n\n\n\nSoftware team members can tag any package as testing/release.\n\n\nOperations team members can tag vo-clients as testing/release.\n\n\nSecurity team members can tag CA packages as testing/release.\n\n\n\n\n\n\n\npromotion =\n   has_perm software-team :: allow\n   has_perm operations-team \n package vo-client :: allow\n   has_perm security-team \n package *-ca-certs* :: allow\n   all :: deny\n\ntag =\n    has_perm admin :: allow\n    operation tag :: {\n        tag *testing *release* \n policy promotion :: allow\n        tag *testing *release* !! allow\n    }\n    operation untag :: {\n        fromtag *testing *release* \n policy promotion :: allow\n        fromtag *testing *release* !! allow\n    }\n    operation move :: {\n        tag *testing *release* \n policy promotion :: allow\n        fromtag *testing *release* \n policy promotion :: allow\n        tag *testing *release* !! {\n            fromtag *testing *release* !! allow\n        }\n    }\n    all :: deny", 
            "title": "Koji Policy Writing"
        }, 
        {
            "location": "/infrastructure/koji-policy-writing/#koji-permissions-and-policy", 
            "text": "These are some notes I wrote on Koji ACLs/policy after doing some source diving in the Koji code.\nThe version of Koji was 1.6.0.\nI later found some documentation at  https://docs.pagure.org/koji/defining_hub_policies/ . Go read it, that page has better examples.", 
            "title": "Koji permissions and policy"
        }, 
        {
            "location": "/infrastructure/koji-policy-writing/#default-policies-defined-in-hubkojixmlrpcpy", 
            "text": "build_from_srpm =\n       has_perm admin :: allow\n       all :: deny\nbuild_from_repo_id =\n       has_perm admin :: allow\n       all :: deny\nchannel =\n       has req_channel :: req\n       is_child_task :: parent\n       all :: use default\npackage_list =\n       has_perm admin :: allow\n       all :: deny\nvm =\n       has_perm admin win-admin :: allow\n       all :: deny  If  MissingPolicyOk  is true (default), then policies that do not exist default to \"allow\".", 
            "title": "Default policies (defined in hub/kojixmlrpc.py)"
        }, 
        {
            "location": "/infrastructure/koji-policy-writing/#policy-syntax", 
            "text": "Policies are set in the  [policy]  section of  /etc/koji-hub/hub.conf  Each policy definition starts with  policy_name =  The rest of the definition is indented. The lines in the policy definition have the following format:  Simple tests:  test [params] [  test [params] ...] :: action-if-true\ntest [params] [  test [params] ...] !! action-if-false  Complex tests:  test [params [  ...]] :: {\n   test [params [  ...]] :: action\n   test [params [  ...]] :: {\n      ...\n      }\n}  The following generic tests are defined in  koji/policy.py :    true  /  all   \n    always true    false  /  none   \n    always false    has FIELD   \n    true if policy data contains a field called FIELD    bool FIELD   \n    true if FIELD is true    match FIELD PATTERN1 [PATTERN2 ...]   \n    true if FIELD matches any of the patterns (globs)    compare FIELD OP NUMBER   \n    compare FIELD against a number. OP can be  ,  ,  =,  =, =, !=    the following koji-specific tests are defined in  hub/kojihub.py :    buildtag PATTERN1 [PATTERN2 ...]   \n    true if the build tag of a build matches a pattern    fromtag PATTERN1 [PATTERN2 ...]   \n    true if the tag we're moving a package from matches a pattern    has_perm PATTERN1 [PATTERN2 ...]   \n    true if user has any matching permission    hastag TAG   \n    true if the build has the tag TAG    imported   \n    true if the build was imported    is_build_owner   \n    true if the user doing this task owns the build    is_child_task   \n    true if the task is a child of some other task    is_new_package   \n    true if the package being looked at is new (i.e. doesn't have an 'id' yet)    method PATTERN1 [PATTERN2 ...]   \n    true if the method matches a pattern    operation PATTERN1 [PATTERN2 ...]   \n    true if current operation matches any of the patterns    package PATTERN1 [PATTERN2 ...]   \n    true if the package name matches any of the patterns    policy POLICY   \n    true if the named policy is true    skip_tag   \n    true if the skip_tag option is true    source PATTERN1 [PATTERN2 ...]   \n    true if source matches patterns    tag PATTERN1 [PATTERN2 ...]   \n    true if the tag name matches any of the patterns    user PATTERN1 [PATTERN2 ...]   \n    true if username matches a pattern    user_in_group PATTERN1 [PATTERN2 ...]   \n    true if the user is in any matching group    vm_name PATTERN1 [PATTERN2 ...]   \n    true if vm name matches a pattern    The actions are:    allow   \n    allow the action    deny   \n    deny the action    req   \n    ?    parent   \n    ?    use default   \n    ?", 
            "title": "Policy syntax"
        }, 
        {
            "location": "/infrastructure/koji-policy-writing/#default-permissions", 
            "text": "These are the permissions that people can be given in koji:   admin  build  repo  livecd  maven-import  win-import  win-admin  appliance   As far as I can tell, additional permissions have to be manually added into the 'permissions' table in postgres.  The following permissions are checked by name in the koji command-line utility (i.e. policies are not used):    admin :   add-group ,  add-tag ,  add-target ,  clone-tag ,  edit-target ,  remove-tag ,  remove-target ,  wrapper-rpm    maven-import :   import-archive  with the  --type=maven  option    win-import :   import-archive  with the  --type=win  option    repo :   regen-repo    I haven't found out where some of the other permissions are used.", 
            "title": "Default permissions"
        }, 
        {
            "location": "/infrastructure/koji-policy-writing/#adding-permissions", 
            "text": "Go into postgres and run  insert into permissions values ((select nextval( permissions_id_seq )),  NAME );  where NAME is the name of the permission you want to create. You may now grant people that permission and use that name in policies.", 
            "title": "Adding permissions"
        }, 
        {
            "location": "/infrastructure/koji-policy-writing/#where-policies-are-used-and-what-policy-data-is-passed-on", 
            "text": "", 
            "title": "Where policies are used and what policy data is passed on:"
        }, 
        {
            "location": "/infrastructure/koji-policy-writing/#build95from95srpm", 
            "text": "source:   builder/kojid:BuildTask.handler   \n    used when source url points to an SRPM (as opposed to an scm) and the build is not a scratch build.   policy data:    user_id   \n    the owner of the task    source   \n    the url of the source file    task_id   \n    the id of the task    build_tag   \n    the id of the build tag    skip_tag   \n    true if we're not tagging this build ( --scratch  or  --skip-tag  passed on the command line)    target   \n    the build target (only if we have one?)    tag   \n    the destination tag (only if  skip_tag  is false)", 
            "title": "build_from_srpm"
        }, 
        {
            "location": "/infrastructure/koji-policy-writing/#build95from95repo95id", 
            "text": "source:   builder/kojid:BuildTask.handler   \n    used when the  --repo-id  option is passed to  koji build   policy data:  same as  build_from_srpm", 
            "title": "build_from_repo_id"
        }, 
        {
            "location": "/infrastructure/koji-policy-writing/#package95list", 
            "text": "source:   hub/kojihub.py:pkglist_add    add-pkg ,  block-pkg ,  set-pkg-arches ,  set-pkg-owner  commands   policy data:    action   \n    'add', 'update', 'block' depending on what is being done    force   \n    true if  --force  is passed on the command line    package   \n    package info (the id I think?)    tag   \n    the id of the tag we're trying to add the package to/package is in    source:   hub/kojihub.py:pkglist_remove   \n    used internally by the  koji clone-tag  command?   policy data:  same as above, except  action  is 'remove'  source:   hub/kojihub.py:pkglist_unblock    unblock-pkg  command   policy data:  same as above, except  action  is 'unblock'", 
            "title": "package_list"
        }, 
        {
            "location": "/infrastructure/koji-policy-writing/#tag", 
            "text": "Note  RootExports is the class containing functions exported via XMLRPC. In general, each function corresponds to a koji task.   source:   hub/kojihub.py:RootExports.tagBuild   \n    tagging builds   policy data:    build   \n    the id of the build    fromtag   \n    the id of the tag we're moving the build from, if there is one    operation   \n    'tag' or 'move'    tag   \n    the id of the tag    source:   hub/kojihub.py:RootExports.untagBuild   \n    untagging builds   policy data:  same as above, except  operation  is 'untag', and  tag  is None  source:   hub/kojihub.py:RootExports.moveAllBuilds   \n    moving all builds of a package from tag1 to tag2   policy data:  same as for  tagBuild , except  operation  is 'move'. The policy is checked once for each build being moved.  source:   hub/kojihub.py:HostExports.tagBuild   \n    tagging builds (\"host version\" ?)   policy data:  same as for  tagBuild , plus  user_id", 
            "title": "tag"
        }, 
        {
            "location": "/infrastructure/koji-policy-writing/#vm", 
            "text": "source:   hub/kojihub.py:RootExports.winBuild   \n    windows builds in a vm ( win-build  command)   policy data:    tag   \n    the destination tag    vm_name   \n    the name of the vm", 
            "title": "vm"
        }, 
        {
            "location": "/infrastructure/koji-policy-writing/#examples", 
            "text": "", 
            "title": "Examples"
        }, 
        {
            "location": "/infrastructure/koji-policy-writing/#let-people-with-the-build-permission-also-add-packages-and-build-srpms", 
            "text": "package_list = \n    has_perm admin :: allow\n    has_perm build   match action add update :: allow\n    all :: deny\n\nbuild_from_srpm =\n    has_perm admin build :: allow\n    all :: deny", 
            "title": "Let people with the \"build\" permission also add packages and build SRPMs"
        }, 
        {
            "location": "/infrastructure/koji-policy-writing/#promotion-policy-for-different-teams", 
            "text": "Software team members can tag any package as testing/release.  Operations team members can tag vo-clients as testing/release.  Security team members can tag CA packages as testing/release.    promotion =\n   has_perm software-team :: allow\n   has_perm operations-team   package vo-client :: allow\n   has_perm security-team   package *-ca-certs* :: allow\n   all :: deny\n\ntag =\n    has_perm admin :: allow\n    operation tag :: {\n        tag *testing *release*   policy promotion :: allow\n        tag *testing *release* !! allow\n    }\n    operation untag :: {\n        fromtag *testing *release*   policy promotion :: allow\n        fromtag *testing *release* !! allow\n    }\n    operation move :: {\n        tag *testing *release*   policy promotion :: allow\n        fromtag *testing *release*   policy promotion :: allow\n        tag *testing *release* !! {\n            fromtag *testing *release* !! allow\n        }\n    }\n    all :: deny", 
            "title": "Promotion policy for different teams"
        }, 
        {
            "location": "/infrastructure/madison-itb/", 
            "text": "pre em { color: red; font-weight: normal; font-style: normal; }\n.old { color: red; }\n.off { color: blue; }\n\n\n\n\nSoftware/Release Team ITB Site Design\n\n\nMadison ITB Machines\n\n\nAll physical hosts are located in 3370A in the VDT rack.\n\n\n\n\n\n\n\n\nHost\n\n\nPurpose\n\n\nOS\n\n\nArch\n\n\nCPU Model\n\n\nCPUs\n\n\nRAM\n\n\nStorage\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nitb-data1\n\n\nworker node\n\n\nSL 6.9\n\n\nx86 64-bit\n\n\nCeleron G530 2.4Ghz\n\n\n2 / 2\n\n\n8 GB\n\n\n750 GB \u00d7 2 (RAID?)\n\n\nplanned as HDFS data node\n\n\n\n\n\n\nitb-data2\n\n\nworker node\n\n\nSL 6.9\n\n\nx86 64-bit\n\n\nCeleron G530 2.4Ghz\n\n\n2 / 2\n\n\n8 GB\n\n\n750 GB \u00d7 2 (RAID?)\n\n\nplanned as HDFS data node\n\n\n\n\n\n\nitb-data3\n\n\nworker node\n\n\nSL 7.4\n\n\nx86 64-bit\n\n\nCeleron G530 2.4Ghz\n\n\n2 / 2\n\n\n8 GB\n\n\n750 GB \u00d7 2 (RAID?)\n\n\nplanned as HDFS data node\n\n\n\n\n\n\nitb-data4\n\n\nworker node\n\n\nSL 6.9\n\n\nx86 64-bit\n\n\nCeleron G530 2.4Ghz\n\n\n2 / 2\n\n\n8 GB\n\n\n750 GB \u00d7 2 (RAID?)\n\n\nplanned as XRootD data node\n\n\n\n\n\n\nitb-data5\n\n\nworker node\n\n\nSL 6.9\n\n\nx86 64-bit\n\n\nXeon E3-1220 3.10GHz\n\n\n2 / 4\n\n\n8 GB\n\n\n750 GB \u00d7 2 (RAID?)\n\n\nplanned as XRootD data node\n\n\n\n\n\n\nitb-data6\n\n\nworker node\n\n\nSL 7.4\n\n\nx86 64-bit\n\n\nXeon E3-1220 3.10GHz\n\n\n2 / 4\n\n\n8 GB\n\n\n???\n\n\nplanned as XRootD data node\n\n\n\n\n\n\nitb-host-1\n\n\nKVM host\n\n\nSL 7.4\n\n\nx86 64-bit\n\n\nXeon E5-2450 2.10GHz\n\n\n16 / 32\n\n\n64 GB\n\n\n1 TB \u00d7 4 (HW RAID 5)\n\n\n\n\n\n\n\n\n\u00b7\u00a0 itb-ce1\n\n\nHTCondor-CE\n\n\nSL 6.9\n\n\nx86 64-bit\n\n\nVM\n\n\n4\n\n\n6 GB\n\n\n192 GB\n\n\n\n\n\n\n\n\n\u00b7\u00a0 itb-ce2\n\n\nHTCondor-CE\n\n\nSL 6.9\n\n\nx86 64-bit\n\n\nVM\n\n\n4\n\n\n6 GB\n\n\n192 GB\n\n\n\n\n\n\n\n\n\u00b7\u00a0 itb-cm\n\n\nHTCondor CM\n\n\nSL 7.4\n\n\nx86 64-bit\n\n\nVM\n\n\n4\n\n\n6 GB\n\n\n192 GB\n\n\n\n\n\n\n\n\n\u00b7\u00a0 \nitb-glidein\n\n\nGlideinWMS VO frontend?\n\n\nSL 6.3\n\n\nx86 64-bit\n\n\nVM\n\n\n3\n\n\n6 GB\n\n\n50 GB\n\n\n\n\n\n\n\n\n\u00b7\u00a0 \nitb-gums-rsv\n\n\nGUMS, RSV\n\n\nSL 6.3\n\n\nx86 64-bit\n\n\nVM\n\n\n3\n\n\n6 GB\n\n\n50 GB\n\n\n\n\n\n\n\n\n\u00b7\u00a0 \nitb-hdfs-name1\n\n\n\u2014 (so far)\n\n\nSL ?\n\n\nx86 64-bit\n\n\nVM\n\n\n4\n\n\n6 GB\n\n\n192 GB\n\n\n\n\n\n\n\n\n\u00b7\u00a0 \nitb-hdfs-name2\n\n\n\u2014 (so far)\n\n\nSL 6.3\n\n\nx86 64-bit\n\n\nVM\n\n\n3\n\n\n6 GB\n\n\n50 GB\n\n\n\n\n\n\n\n\n\u00b7\u00a0 \nitb-se-hdfs\n\n\n\u2014 (so far)\n\n\nSL 6.3\n\n\nx86 64-bit\n\n\nVM\n\n\n3\n\n\n6 GB\n\n\n50 GB\n\n\n\n\n\n\n\n\n\u00b7\u00a0 \nitb-se-xrootd\n\n\n\u2014 (so far)\n\n\nSL 6.3\n\n\nx86 64-bit\n\n\nVM\n\n\n3\n\n\n6 GB\n\n\n50 GB\n\n\n\n\n\n\n\n\n\u00b7\u00a0 itb-submit\n\n\nHTCondor submit\n\n\nSL 6.9\n\n\nx86 64-bit\n\n\nVM\n\n\n4\n\n\n6 GB\n\n\n192 GB\n\n\n\n\n\n\n\n\n\u00b7\u00a0 \nitb-xrootd\n\n\n\u2014 (so far)\n\n\nSL ?\n\n\nx86 64-bit\n\n\nVM\n\n\n4\n\n\n6 GB\n\n\n192 GB\n\n\n\n\n\n\n\n\nitb-host-2\n\n\nworker node\n\n\nSL 6.9\n\n\nx86 64-bit\n\n\nXeon E5-2450 2.10GHz\n\n\n16 / 32\n\n\n64 GB\n\n\n352 GB in $(EXECUTE)\n\n\n\n\n\n\n\n\nitb-host-3\n\n\nworker node\n\n\nSL 7.4\n\n\nx86 64-bit\n\n\nXeon E5-2450 2.10GHz\n\n\n16 / 32\n\n\n64 GB\n\n\n352 GB in $(EXECUTE)\n\n\n\n\n\n\n\n\n\n\n(Data last updated 2017-10-13 by Tim\u00a0C. \nRed\n indicates a host that has yet to be rebuilt; \nBlue\n is rebuilt but currently off.)\n\n\nITB Goals\n\n\nGoals for the Madison ITB site are now maintained in \na Google document\n.\n\n\nConfiguration\n\n\nBasic host configuration is handled by Ansible and a local Git repository of playbooks.\n\n\nGit Repository\n\n\nThe authoritative Git repository for Madison ITB configuration is \ngitolite@git.chtc.wisc.edu:osgitb\n.  Clone the\nrepository and push validated changes back to it.\n\n\nAnsible\n\n\nThe \nosghost\n machine has Ansible 2.3.1.0 installed via RPM.  Use other hosts and versions at your own risk.\n\n\nCommon Ansible commands\n\n\nNote:\n\n\n\n\nFor critical passwords, see Tim C. or other knowledgeable Madison OSG staff in person\n\n\nAll commands below are meant to be run from the \nosgitb\n directory from Git\n\n\n\n\nTo run Ansible for the first time on a new machine (using the \nroot\n password when prompted):\n\n\nansible-playbook secure.yml -i inventory -u root -k --ask-vault-pass -f 20 -l HOST-PATTERN\n\n\nansible-playbook site.yml -i inventory -u root -k -f 20 -l HOST-PATTERN\n\n\n\n\n\n\nThe \nHOST-PATTERN\n can be a glob-like pattern or a regular expression that matches host names in the inventory file; see\nAnsible documentation for details.\n\n\nAfter initial successful runs of both playbooks, subsequent runs should replace the \n-u root -k\n part with \n-bK\n to use\nyour own login and \nsudo\n.  For example:\n\n\nansible-playbook secure.yml -i inventory -bK --ask-vault-pass -f 20 -l HOST-PATTERN\n\n\nansible-playbook site.yml -i inventory -bK -f 20 [ -l HOST-PATTERN ]\n\n\n\n\n\n\nOmit the \n-l\n option to apply configuration to all hosts.\n\n\nIf you have your own playbook to manage personal configuration, run it as follows:\n\n\nansible-playbook PLAYBOOK-PATH -i inventory -f 20 [ -l HOST-PATTERN ]\n\n\n\n\n\n\nAdding host and other certificates\n\n\n(This is in very rough form, but the key bits are here.)\n\n\n\n\nAsk Mat to get new certificates\u00a0\u2014 be sure to think about \nhttp\n, \nrsv\n, and other service certificates\n\n\nWait for Mat to tell you that the new certificates are in \n/p/condor/home/certificates\n\n\nscp -p\n the certificate(s) (\n*cert.pem*\n and \n*key.pem\n) to your home directory on \nosghost\n or whatever machine you use for Ansible\n\n\nFind the corresponding certificate location(s) in the Ansible \nroles/certs/files\n directory\n\n\ncp -p\n the certificate files over the top of the existing Ansible ones (or create new, equivalent paths)\n\n\nRun \nansible-vault encrypt FILE(S)\n to encrypt the files\u00a0\u2014 get the Ansible vault password from Tim\u00a0C. if you need it\n\n\nVerify permissions, contents (you can \ncat\n the encrypted files), etc.\n\n\nApply the files with something like \nansible-playbook secure.yml -i inventory -bK -f 20 -t certs\n\n\nCommit changes (now or after applying)\n\n\nPush changes to origin\n\n\n\n\nDoing yum updates\n\n\n\n\n\n\nCheck to see if updates are needed and, if so, what would be updated:\n\n\nansible [ HOST | GROUP ] -i inventory -bK -f 20 -m command -a \nyum check-update\n\n\n\n\n\n\nYou can name a single \nHOST\n or an inventory \nGROUP\n (such as the handy \ncurrent\n group); with a group, you can\nfurther restrict the hosts with a \n-l\n option.\n\n\nNote:\n \nyum check-update\n exits with status code \n100\n when it succeeds in identifying packages to update;\ntherefore Ansible shows such results as failures.\n\n\n\n\n\n\nReview the package lists to be updated and decide whether to proceed with all updates or limited ones\n\n\n\n\n\n\nDo updates:\n\n\nansible [ HOST | GROUP ] -i inventory -bK -f 20 -m command -a \nyum --assumeyes update\n [ -l LIMITS ]\n\n\n\n\n\n\n\n\n\n\nIf needed (and if unsure, ask a sysadmin), reboot machines:\n\n\nansible [ HOST | GROUP ] -i inventory -bK -f 20 -m command -a \n/sbin/shutdown -r +1\n [ -l LIMITS ]\n\n\n\n\n\n\n\n\n\n\nUpdating HTCondor from Upcoming\n\n\nSomething like this:\n\n\nansible condordev -i inventory -bK -f 10 -m command -a \nyum --enablerepo=osg-upcoming --assumeyes update condor\n\n\n\n\n\n\nMonitoring\n\n\nHTCondor-CE View\n\n\nOnce we sort out our firewall rules, pilot, VO, and schedd availability graphs should be available\n\nhere\n through HTCondor-CE View.\n\n\nTracking payload jobs via Kibana\n\n\nAt this time, the easest way to verify that payload jobs are running within the glideinWMS pilots is to track their records via \nKibana\n. To view all payload jobs that have run on our ITB site in the past week, use \nthis query\n.\n\n\nMaking a New Virtual Machine on itb-host-1\n\n\nFor this procedure, you will need login access to the CHTC Cobbler website, which is separate from other CHTC logins.\nIf you do not have an account, request one from the CHTC system administrators.\n\n\n\n\n\n\nIf this is a new host (combination of MAC address, IP address, and hostname), set up host with CHTC Infrastructure\n\n\n\n\nPick a MAC address, starting with \n00:16:3e:\n followed by three random octets (e.g., \n00:16:3e:f7:29:ee\n)\n\n\nEmail \n with a request for a new OSG ITB VM, including the chosen MAC address\n\n\nWait to receive the associated IP address for the new host\n\n\n\n\n\n\n\n\nCreate or edit the Cobbler system object for the host\n\n\n\n\nAccess \nhttps://cobbler-widmir.chtc.wisc.edu/cobbler_web\n\n\nIn the left navigation area, under \u201cConfiguration\u201d, click the \u201cSystems\u201d link\n\n\nIf desired, filter (at the bottom) by \u201cname\u201d on something like \nitb-*.chtc.wisc.edu\n\n\nFor a new host, select an existing, similar one and click \u201cCopy\u201d to the right of its entry, then give it a name and click \u201cOK\u201d\n\n\nFor a newly copied or any existing host, click \u201cEdit\u201d to the right of its entry\n\n\nIn the \nfirst\n \u201cGeneral\u201d section: select a \u201cProfile\u201d of \u201cScientific_6_8_osg_vm\u201d or \u201cScientific_7_2_osg_vm\u201d\n\n\nIn the \nsecond\n \u201cGeneral\u201d section, check the \u201cNetboot Enabled\u201d checkbox\n\n\nIn the \u201cNetworking (Global)\u201d section, set \u201cHostname\u201d to the fully qualified hostname for the virtual machine\n\n\nIn the \u201cNetworking\u201d section, select the \u201ceth0\u201d interface to edit, and set the \u201cMAC Address\u201d, \u201cIP Address\u201d, and \u201cDNS Name\u201d fields for the host\n\n\nClick the \u201cSave\u201d button\n\n\nIn the left navigation area, under \u201cActions\u201d, click the \u201cSync\u201d link\n\n\n\n\n\n\n\n\nLog in to \nitb-host-1\n and become \nroot\n\n\n\n\n\n\nCreate the libvirt definition file\n\n\n\n\nCreate a new XML file named after the desired hostname (e.g., \nitb-ce2.xml\n) and copy in the template below\n\n\nReplace \n{{ HOSTNAME }}\n with the fully qualified hostname of the new virtual host\n\n\nReplace \n{{ MAC_ADDRESS }}\n with the MAC address of the new virtual host (from above)\n\n\nIf desired, edit other values in the XML definition file; ask CHTC Infrastructure for help, if needed\n\n\nSave the XML file\n\n\n\n\nCreate a new, empty disk image for the virtual host, in its correct location (as specified in the XML file):\n\n\ntruncate -s 192G /var/lib/libvirt/images/HOSTNAME.dd\n\n\nchown qemu:qemu /var/lib/libvirt/images/HOSTNAME.dd\n\n\n\n\n\n\n\n\n\n\nLoad the new host definition into libvirt:\n\n\nvirsh define XML-FILE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstall the new machine\n\n\n\n\n\n\nStart the virtual machine:\n\n\nvirsh start HOSTNAME\n\n\n\n\n\n\nAt this time, the machine will boot over the network, and Cobbler will install and minimally configure the OS,\nthen reboot the now-installed machine.  The whole process typically takes 15\u201320 minutes.  You may be able to\n\nssh\n into the machine during the install process, but there is no need to monitor or interfere.\n\n\n\n\n\n\nOnce the machine is available (which you can only guess at), \nssh\n in and verify that the machine basically works\n\n\n\n\nImmediately run Ansible on the machine, first with the \nsecure.yml\n playbook, then the \nsite.yml\n one (see above)\n\n\nLog in to the machine and look around to make sure it seems OK\n\n\nWhen things look good, tell virsh to start the virtual machine when the host itself starts:\nvirsh autostart HOSTNAME\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLibvirt VM Template\n\n\ndomain\n \ntype=\nkvm\n\n\n  \nname\n{{ HOSTNAME }}\n/name\n\n\n  \nmemory\n \nunit=\nGiB\n6\n/memory\n\n  \nvcpu\n4\n/vcpu\n\n  \nos\n\n    \ntype\nhvm\n/type\n\n    \nboot\n \ndev=\nnetwork\n/\n\n    \nboot\n \ndev=\nhd\n/\n\n    \nbios\n \nuseserial=\nyes\n \nrebootTimeout=\n0\n/\n\n  \n/os\n\n  \nfeatures\n\n    \nacpi/\n\n    \napic/\n\n    \npae/\n\n  \n/features\n\n\n  \ndevices\n\n\n    \nemulator\n/usr/libexec/qemu-kvm\n/emulator\n\n\n    \ndisk\n \ntype=\nfile\n \ndevice=\ndisk\n\n      \nsource\n \nfile=\n/var/lib/libvirt/images/{{ HOSTNAME }}.dd\n/\n\n      \ntarget\n \ndev=\nvda\n \nbus=\nvirtio\n/\n\n    \n/disk\n\n\n    \ninterface\n \ntype=\nbridge\n\n      \nmac\n \naddress=\n{{ MAC_ADDRESS }}\n/\n\n      \nsource\n \nbridge=\nbr0\n/\n\n      \nmodel\n \ntype=\nvirtio\n/\n\n    \n/interface\n\n\n    \nserial\n \ntype=\npty\n\n      \ntarget\n \nport=\n0\n/\n\n    \n/serial\n\n    \nconsole\n \ntype=\npty\n\n      \ntarget\n \ntype=\nserial\n \nport=\n0\n/\n\n    \n/console\n\n\n    \ngraphics\n \ntype=\nvnc\n \nautoport=\nyes\n \nlisten=\n127.0.0.1\n/\n\n\n  \n/devices\n\n\n/domain", 
            "title": "Madison ITB"
        }, 
        {
            "location": "/infrastructure/madison-itb/#softwarerelease-team-itb-site-design", 
            "text": "", 
            "title": "Software/Release Team ITB Site Design"
        }, 
        {
            "location": "/infrastructure/madison-itb/#madison-itb-machines", 
            "text": "All physical hosts are located in 3370A in the VDT rack.     Host  Purpose  OS  Arch  CPU Model  CPUs  RAM  Storage  Notes      itb-data1  worker node  SL 6.9  x86 64-bit  Celeron G530 2.4Ghz  2 / 2  8 GB  750 GB \u00d7 2 (RAID?)  planned as HDFS data node    itb-data2  worker node  SL 6.9  x86 64-bit  Celeron G530 2.4Ghz  2 / 2  8 GB  750 GB \u00d7 2 (RAID?)  planned as HDFS data node    itb-data3  worker node  SL 7.4  x86 64-bit  Celeron G530 2.4Ghz  2 / 2  8 GB  750 GB \u00d7 2 (RAID?)  planned as HDFS data node    itb-data4  worker node  SL 6.9  x86 64-bit  Celeron G530 2.4Ghz  2 / 2  8 GB  750 GB \u00d7 2 (RAID?)  planned as XRootD data node    itb-data5  worker node  SL 6.9  x86 64-bit  Xeon E3-1220 3.10GHz  2 / 4  8 GB  750 GB \u00d7 2 (RAID?)  planned as XRootD data node    itb-data6  worker node  SL 7.4  x86 64-bit  Xeon E3-1220 3.10GHz  2 / 4  8 GB  ???  planned as XRootD data node    itb-host-1  KVM host  SL 7.4  x86 64-bit  Xeon E5-2450 2.10GHz  16 / 32  64 GB  1 TB \u00d7 4 (HW RAID 5)     \u00b7\u00a0 itb-ce1  HTCondor-CE  SL 6.9  x86 64-bit  VM  4  6 GB  192 GB     \u00b7\u00a0 itb-ce2  HTCondor-CE  SL 6.9  x86 64-bit  VM  4  6 GB  192 GB     \u00b7\u00a0 itb-cm  HTCondor CM  SL 7.4  x86 64-bit  VM  4  6 GB  192 GB     \u00b7\u00a0  itb-glidein  GlideinWMS VO frontend?  SL 6.3  x86 64-bit  VM  3  6 GB  50 GB     \u00b7\u00a0  itb-gums-rsv  GUMS, RSV  SL 6.3  x86 64-bit  VM  3  6 GB  50 GB     \u00b7\u00a0  itb-hdfs-name1  \u2014 (so far)  SL ?  x86 64-bit  VM  4  6 GB  192 GB     \u00b7\u00a0  itb-hdfs-name2  \u2014 (so far)  SL 6.3  x86 64-bit  VM  3  6 GB  50 GB     \u00b7\u00a0  itb-se-hdfs  \u2014 (so far)  SL 6.3  x86 64-bit  VM  3  6 GB  50 GB     \u00b7\u00a0  itb-se-xrootd  \u2014 (so far)  SL 6.3  x86 64-bit  VM  3  6 GB  50 GB     \u00b7\u00a0 itb-submit  HTCondor submit  SL 6.9  x86 64-bit  VM  4  6 GB  192 GB     \u00b7\u00a0  itb-xrootd  \u2014 (so far)  SL ?  x86 64-bit  VM  4  6 GB  192 GB     itb-host-2  worker node  SL 6.9  x86 64-bit  Xeon E5-2450 2.10GHz  16 / 32  64 GB  352 GB in $(EXECUTE)     itb-host-3  worker node  SL 7.4  x86 64-bit  Xeon E5-2450 2.10GHz  16 / 32  64 GB  352 GB in $(EXECUTE)      (Data last updated 2017-10-13 by Tim\u00a0C.  Red  indicates a host that has yet to be rebuilt;  Blue  is rebuilt but currently off.)", 
            "title": "Madison ITB Machines"
        }, 
        {
            "location": "/infrastructure/madison-itb/#itb-goals", 
            "text": "Goals for the Madison ITB site are now maintained in  a Google document .", 
            "title": "ITB Goals"
        }, 
        {
            "location": "/infrastructure/madison-itb/#configuration", 
            "text": "Basic host configuration is handled by Ansible and a local Git repository of playbooks.", 
            "title": "Configuration"
        }, 
        {
            "location": "/infrastructure/madison-itb/#git-repository", 
            "text": "The authoritative Git repository for Madison ITB configuration is  gitolite@git.chtc.wisc.edu:osgitb .  Clone the\nrepository and push validated changes back to it.", 
            "title": "Git Repository"
        }, 
        {
            "location": "/infrastructure/madison-itb/#ansible", 
            "text": "The  osghost  machine has Ansible 2.3.1.0 installed via RPM.  Use other hosts and versions at your own risk.", 
            "title": "Ansible"
        }, 
        {
            "location": "/infrastructure/madison-itb/#common-ansible-commands", 
            "text": "Note:   For critical passwords, see Tim C. or other knowledgeable Madison OSG staff in person  All commands below are meant to be run from the  osgitb  directory from Git   To run Ansible for the first time on a new machine (using the  root  password when prompted):  ansible-playbook secure.yml -i inventory -u root -k --ask-vault-pass -f 20 -l HOST-PATTERN  ansible-playbook site.yml -i inventory -u root -k -f 20 -l HOST-PATTERN   The  HOST-PATTERN  can be a glob-like pattern or a regular expression that matches host names in the inventory file; see\nAnsible documentation for details.  After initial successful runs of both playbooks, subsequent runs should replace the  -u root -k  part with  -bK  to use\nyour own login and  sudo .  For example:  ansible-playbook secure.yml -i inventory -bK --ask-vault-pass -f 20 -l HOST-PATTERN  ansible-playbook site.yml -i inventory -bK -f 20 [ -l HOST-PATTERN ]   Omit the  -l  option to apply configuration to all hosts.  If you have your own playbook to manage personal configuration, run it as follows:  ansible-playbook PLAYBOOK-PATH -i inventory -f 20 [ -l HOST-PATTERN ]", 
            "title": "Common Ansible commands"
        }, 
        {
            "location": "/infrastructure/madison-itb/#adding-host-and-other-certificates", 
            "text": "(This is in very rough form, but the key bits are here.)   Ask Mat to get new certificates\u00a0\u2014 be sure to think about  http ,  rsv , and other service certificates  Wait for Mat to tell you that the new certificates are in  /p/condor/home/certificates  scp -p  the certificate(s) ( *cert.pem*  and  *key.pem ) to your home directory on  osghost  or whatever machine you use for Ansible  Find the corresponding certificate location(s) in the Ansible  roles/certs/files  directory  cp -p  the certificate files over the top of the existing Ansible ones (or create new, equivalent paths)  Run  ansible-vault encrypt FILE(S)  to encrypt the files\u00a0\u2014 get the Ansible vault password from Tim\u00a0C. if you need it  Verify permissions, contents (you can  cat  the encrypted files), etc.  Apply the files with something like  ansible-playbook secure.yml -i inventory -bK -f 20 -t certs  Commit changes (now or after applying)  Push changes to origin", 
            "title": "Adding host and other certificates"
        }, 
        {
            "location": "/infrastructure/madison-itb/#doing-yum-updates", 
            "text": "Check to see if updates are needed and, if so, what would be updated:  ansible [ HOST | GROUP ] -i inventory -bK -f 20 -m command -a  yum check-update   You can name a single  HOST  or an inventory  GROUP  (such as the handy  current  group); with a group, you can\nfurther restrict the hosts with a  -l  option.  Note:   yum check-update  exits with status code  100  when it succeeds in identifying packages to update;\ntherefore Ansible shows such results as failures.    Review the package lists to be updated and decide whether to proceed with all updates or limited ones    Do updates:  ansible [ HOST | GROUP ] -i inventory -bK -f 20 -m command -a  yum --assumeyes update  [ -l LIMITS ]     If needed (and if unsure, ask a sysadmin), reboot machines:  ansible [ HOST | GROUP ] -i inventory -bK -f 20 -m command -a  /sbin/shutdown -r +1  [ -l LIMITS ]", 
            "title": "Doing yum updates"
        }, 
        {
            "location": "/infrastructure/madison-itb/#updating-htcondor-from-upcoming", 
            "text": "Something like this:  ansible condordev -i inventory -bK -f 10 -m command -a  yum --enablerepo=osg-upcoming --assumeyes update condor", 
            "title": "Updating HTCondor from Upcoming"
        }, 
        {
            "location": "/infrastructure/madison-itb/#monitoring", 
            "text": "", 
            "title": "Monitoring"
        }, 
        {
            "location": "/infrastructure/madison-itb/#htcondor-ce-view", 
            "text": "Once we sort out our firewall rules, pilot, VO, and schedd availability graphs should be available here  through HTCondor-CE View.", 
            "title": "HTCondor-CE View"
        }, 
        {
            "location": "/infrastructure/madison-itb/#tracking-payload-jobs-via-kibana", 
            "text": "At this time, the easest way to verify that payload jobs are running within the glideinWMS pilots is to track their records via  Kibana . To view all payload jobs that have run on our ITB site in the past week, use  this query .", 
            "title": "Tracking payload jobs via Kibana"
        }, 
        {
            "location": "/infrastructure/madison-itb/#making-a-new-virtual-machine-on-itb-host-1", 
            "text": "For this procedure, you will need login access to the CHTC Cobbler website, which is separate from other CHTC logins.\nIf you do not have an account, request one from the CHTC system administrators.    If this is a new host (combination of MAC address, IP address, and hostname), set up host with CHTC Infrastructure   Pick a MAC address, starting with  00:16:3e:  followed by three random octets (e.g.,  00:16:3e:f7:29:ee )  Email   with a request for a new OSG ITB VM, including the chosen MAC address  Wait to receive the associated IP address for the new host     Create or edit the Cobbler system object for the host   Access  https://cobbler-widmir.chtc.wisc.edu/cobbler_web  In the left navigation area, under \u201cConfiguration\u201d, click the \u201cSystems\u201d link  If desired, filter (at the bottom) by \u201cname\u201d on something like  itb-*.chtc.wisc.edu  For a new host, select an existing, similar one and click \u201cCopy\u201d to the right of its entry, then give it a name and click \u201cOK\u201d  For a newly copied or any existing host, click \u201cEdit\u201d to the right of its entry  In the  first  \u201cGeneral\u201d section: select a \u201cProfile\u201d of \u201cScientific_6_8_osg_vm\u201d or \u201cScientific_7_2_osg_vm\u201d  In the  second  \u201cGeneral\u201d section, check the \u201cNetboot Enabled\u201d checkbox  In the \u201cNetworking (Global)\u201d section, set \u201cHostname\u201d to the fully qualified hostname for the virtual machine  In the \u201cNetworking\u201d section, select the \u201ceth0\u201d interface to edit, and set the \u201cMAC Address\u201d, \u201cIP Address\u201d, and \u201cDNS Name\u201d fields for the host  Click the \u201cSave\u201d button  In the left navigation area, under \u201cActions\u201d, click the \u201cSync\u201d link     Log in to  itb-host-1  and become  root    Create the libvirt definition file   Create a new XML file named after the desired hostname (e.g.,  itb-ce2.xml ) and copy in the template below  Replace  {{ HOSTNAME }}  with the fully qualified hostname of the new virtual host  Replace  {{ MAC_ADDRESS }}  with the MAC address of the new virtual host (from above)  If desired, edit other values in the XML definition file; ask CHTC Infrastructure for help, if needed  Save the XML file   Create a new, empty disk image for the virtual host, in its correct location (as specified in the XML file):  truncate -s 192G /var/lib/libvirt/images/HOSTNAME.dd  chown qemu:qemu /var/lib/libvirt/images/HOSTNAME.dd     Load the new host definition into libvirt:  virsh define XML-FILE       Install the new machine    Start the virtual machine:  virsh start HOSTNAME   At this time, the machine will boot over the network, and Cobbler will install and minimally configure the OS,\nthen reboot the now-installed machine.  The whole process typically takes 15\u201320 minutes.  You may be able to ssh  into the machine during the install process, but there is no need to monitor or interfere.    Once the machine is available (which you can only guess at),  ssh  in and verify that the machine basically works   Immediately run Ansible on the machine, first with the  secure.yml  playbook, then the  site.yml  one (see above)  Log in to the machine and look around to make sure it seems OK  When things look good, tell virsh to start the virtual machine when the host itself starts: virsh autostart HOSTNAME", 
            "title": "Making a New Virtual Machine on itb-host-1"
        }, 
        {
            "location": "/infrastructure/madison-itb/#libvirt-vm-template", 
            "text": "domain   type= kvm \n\n   name {{ HOSTNAME }} /name \n\n   memory   unit= GiB 6 /memory \n   vcpu 4 /vcpu \n   os \n     type hvm /type \n     boot   dev= network / \n     boot   dev= hd / \n     bios   useserial= yes   rebootTimeout= 0 / \n   /os \n   features \n     acpi/ \n     apic/ \n     pae/ \n   /features \n\n   devices \n\n     emulator /usr/libexec/qemu-kvm /emulator \n\n     disk   type= file   device= disk \n       source   file= /var/lib/libvirt/images/{{ HOSTNAME }}.dd / \n       target   dev= vda   bus= virtio / \n     /disk \n\n     interface   type= bridge \n       mac   address= {{ MAC_ADDRESS }} / \n       source   bridge= br0 / \n       model   type= virtio / \n     /interface \n\n     serial   type= pty \n       target   port= 0 / \n     /serial \n     console   type= pty \n       target   type= serial   port= 0 / \n     /console \n\n     graphics   type= vnc   autoport= yes   listen= 127.0.0.1 / \n\n   /devices  /domain", 
            "title": "Libvirt VM Template"
        }, 
        {
            "location": "/documentation/writing-documentation/", 
            "text": "Tips for Writing Software Documentation\n\n\nWriting Procedural (Step-by-Step) Instructions\n\n\n\n\n\n\nTitle the procedure with the user goal, usually starting with a gerund; e.g.:\n\n\nInstalling the Frobnosticator\n\n\n\n\n\n\nNumber all steps (as opposed to using bullets)\n\n\n\n\n\n\nList steps in order in which they are performed\n\n\n\n\n\n\nEach step should begin with a single-line instruction in plain English, in command form; e.g.:\n\n\n\n\nMake sure that the Frobnosticator configuration file is world-writable\n\n\n\n\n\n\n\n\nIf the means of carrying out the instruction is unclear or complex, include clarification, ideally in the form of a working example; e.g.:\n  \nchmod a+x /usr/share/frobnosticator/frob.conf\n\n\n\n\n\n\nPut clarifying information in separate paragraphs within the step\n\n\n\n\n\n\nPut critical information about the \nwhole\n procedure in one or more paragraphs before the numbered steps\n\n\n\n\n\n\nPut supplemental information about the \nwhole\n procedure in one or more paragraphs after the numbered steps\n\n\n\n\n\n\nAvoid pronouns when writing technical articles or documentation e.g., \ninstall foo\n rather than \ninstall it\n.", 
            "title": "Writing Documentation"
        }, 
        {
            "location": "/documentation/writing-documentation/#tips-for-writing-software-documentation", 
            "text": "", 
            "title": "Tips for Writing Software Documentation"
        }, 
        {
            "location": "/documentation/writing-documentation/#writing-procedural-step-by-step-instructions", 
            "text": "Title the procedure with the user goal, usually starting with a gerund; e.g.:  Installing the Frobnosticator    Number all steps (as opposed to using bullets)    List steps in order in which they are performed    Each step should begin with a single-line instruction in plain English, in command form; e.g.:   Make sure that the Frobnosticator configuration file is world-writable     If the means of carrying out the instruction is unclear or complex, include clarification, ideally in the form of a working example; e.g.:\n   chmod a+x /usr/share/frobnosticator/frob.conf    Put clarifying information in separate paragraphs within the step    Put critical information about the  whole  procedure in one or more paragraphs before the numbered steps    Put supplemental information about the  whole  procedure in one or more paragraphs after the numbered steps    Avoid pronouns when writing technical articles or documentation e.g.,  install foo  rather than  install it .", 
            "title": "Writing Procedural (Step-by-Step) Instructions"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/", 
            "text": "Notes on Koji initial install\n\n\n\n\nNote\n\n\nThis document was written for the original install of koji/koji-hub version\n1.6 to \nkoji-hub.batlab.org\n.  The document is kept for historical\npurposes, as this setup is no longer accurate with newer versions of Koji,\nmachine moves and renames, cert authority changes, etc.\n\n\n\n\nMachine setup\n\n\nkoji is run on \nkoji-hub.batlab.org\n; right now a single machine was used for the hub (koji-hub), the web frontend (koji-web), repo generation (kojira), and el5 building (kojid). A second machine, \nkojibuilder1.batlab.org\n was used for el6 building.\n\n\nAs instructions for setting up the machine, I'm just going to take the Fedora guide at \nhttp://fedoraproject.org/wiki/Koji/ServerHowTo\n, and add modifications / comments to suit our setup.\n\n\nSections taken from the Fedora guide will be between dividers like this:\n\n\n\n\n\n\n\n\nFor an overview of yum, mock, Koji (and all its subcomponents), mash, and how they all work together, see the excellent slides put together by Steve Traylen at CERN \nhttp://indico.cern.ch/event/55091\n\n\n\n\n\n\n\n\nPackages to install\n\n\n\n\n\n\nOn the server (koji-hub/koji-web)\n\n\n\n\nhttpd\n\n\nmod_ssl\n\n\npostgresql-server\n\n\nmod_python (\n= 3.3.1 for Kerberos authentication)\n\n\n\n\nOn the builder (koji-builder)\n\n\n\n\nmock\n\n\nsetarch (for some archs you'll require a patched version)\n\n\nrpm-build\n\n\ncreaterepo\n\n\n\n\n\n\n\n\nWe used one machine for both these roles, so all of the above had to be installed.\n\n\nThe koji packages to install are:\n\n\n\n\nkoji-hub\n\n\nkoji-web\n\n\nkoji-builder\n\n\nkoji-utils\n\n\n\n\nFilesystems\n\n\n\n\n\n\nA note on filesystem space\n\n\nKoji will consume copious amounts of disk space under the primary KojiDir directory (as set in the kojihub.conf file).\n\n\n\n\n\n\nThis is \n/mnt/koji\n by default. Koji keeps \nall RPMs built\n in here so a complete history is kept. In addition, repositories are kept here. A repository is just a set of text files, but since a new one is generated for every build, the space will add up. Koji does clean up the repositories once they get old enough. For \nkoji-hub.batlab.org\n, we allocated a 120G partition for \n/mnt/koji\n.\n\n\n\n\n\n\nHowever, as koji makes use of mock on the backend to actually create build roots and perform the builds in those build roots, it might come to a surprise to users that a running koji server will consume large amounts of disk space under /var/lib/mock and /var/cache/mock as well.\n\n\n\n\n\n\nkojid\n (the builder daemon) will refuse to start a build if it does not have sufficient space under \n/var/lib/mock\n. By default, this is an 8G surplus, but we lowered it to 4G for \nkoji-hub.batlab.org\n. The build roots will be wiped after successful builds, but the build roots for failed builds will be kept around for 2 weeks (?); as a result, disk usage of \n/var/lib/mock\n can swing wildly. For \nkoji-hub.batlab.org\n, we have allocated 35G for \n/var/lib/mock\n. \n/var/cache/mock\n doesn't use up that much space, so we didn't make a separate partition for it, it just uses what's under \n/\n.\n\n\nAuthentication\n\n\nMost of the \"Koji Authentication Selection\" section can be skipped. We got our certs from DigiCert. \nNote\n that certs are sensitive to line ending issues -- you may have to run \ndos2unix\n on them before they would work.\n\n\nWe have 3 certs, with the following subjects:\n\n\n\n\n/DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=koji-hub.batlab.org\n (for koji-hub, koji-web, and kojid)\n\n\n/DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=kojira/koji-hub.batlab.org\n (for kojira)\n\n\n/DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=kojibuilder1.batlab.org\n (for kojid on kojibuilder1)\n\n\n\n\nkojid needed a different cert than kojira because the two would connect to koji-hub at the same time and log each other off.\n\n\nCopies of the certs exist in AFS at \n/p/condor/home/certificates/koji-hub.batlab.org/\n and \n/p/condor/home/certificates/kojibuilder1.batlab.org/\n\n\nClients connecting to koji-hub need a file containing the CA certs; this is just a concatenation of the DigiCert Grid Root CA and the DigiCert Grid CA-1 certs, which can be obtained from DigiCert's website (get PEM format). We have a \ndigicert-chain.crt\n in \n/etc/pki/tls/certs/digicert-chain.crt\n on \nkoji-hub\n\n\nWe made a \nkojiadmin\n user, but we didn't make a cert for it so we just used user/pass authentication until we set up our own accounts and then disabled \nkojiadmin\n.\n\n\nUsing proxy certs\n\n\n/etc/sysconfig/httpd\n needed to be changed to include the following lines:\n\n\nOPENSSL_ALLOW_PROXY\n=\n1\n\n\nOPENSSL_ALLOW_PROXY_CERTS\n=\n1\n\n\n\nexport\n OPENSSL_ALLOW_PROXY\n\nexport\n OPENSSL_ALLOW_PROXY_CERTS\n\n\n\n\n\nThe user must use RFC proxies and must have a version of the koji client of 1.6.0-6.osg or newer.\n\n\nPostgres Database\n\n\n\n\nInstall postgres:\n\n\n[root@koji-hub]# yum install postgresql-server\n\n\nInit the db:\n\n\n[root@koji-hub]# su - postgres -c \"PGDATA=/var/lib/pgsql/data\" initdb\n\n\nStart the db:\n\n\n[root@koji-hub]# service postgresql start\n\n\nMake a koji account:\n\n\n[root@koji-hub]# useradd koji; passwd -d koji\n\n\n\n\n\n\n\n\n\n\n\nSetup PostgreSQL and populate schema:\n\n\nThe following commands will create the \nkoji\n user within PostgreSQL and will then create the koji database using the schema within the \n/usr/share/doc/koji*/docs/schema.sql\n directory\n\n\nroot@localhost$ su - postgres\npostgres@localhost$ createuser koji\nShall the new role be a superuser? (y/n) n\nShall the new role be allowed to create databases? (y/n) n\nShall the new role be allowed to create more new roles? (y/n) n\npostgres@localhost$ createdb -O koji koji\npostgres@localhost$ logout\nroot@localhost$ su - koji\nkoji@localhost$ psql koji koji \n /usr/share/doc/koji*/docs/schema.sql\nkoji@localhost$ exit\n\n\n\n\n\nNOTE:\n When issuing the command to import the psql schema into the new database it is important to ensure that the directory path \n/usr/share/doc/koji*/docs/schema.sql\n remains intact and is not resolved to a specific version of koji. In test it was discovered that when the path is resolved to a specific version of koji then not all of the tables were created correctly.\n\n\n\n\n\n\nTo authorize the koji-web and koji-hub resources, make the following additions to \n/var/lib/pgsql/data/pg_hba.conf\n (IP addresses will vary):\n\n\n# access for koji\nhost    koji        postgres    127.0.0.1/32          trust\nhost    koji        koji        127.0.0.1/32          trust\nhost    koji        koji        128.104.100.41/32     trust\nhost    koji        koji        ::1/128               trust\n\n\n\n\n\nNext, we'll need a koji admin user to run some commands. We'll need to add it to the database manually. Once again, we used user/pass authentication for the kojiadmin user until we disabled it. All database commands should be done as the \nkoji\n user:\n\n\n[root@koji-hub]#\n sudo -u koji psql\n\n\n\n\n\nkoji=\n  \ninsert\n \ninto\n \nusers\n \n(\nname\n,\n \npassword\n,\n \nstatus\n,\n \nusertype\n)\n \nvalues\n\n          \n(\nkojiadmin\n,\n \nsome-throwaway-admin-password-in-plain-text\n,\n \n0\n,\n \n0\n);\n\n\nkoji=\n  \ninsert\n \ninto\n \nuser_perms\n \n(\nuser_id\n,\n \nperm_id\n,\n \ncreator_id\n)\n \nvalues\n\n          \n((\nselect\n \nid\n \nfrom\n \nusers\n \nwhere\n \nname\n=\nkojiadmin\n),\n \n1\n,\n\n           \n(\nselect\n \nid\n \nfrom\n \nusers\n \nwhere\n \nname\n=\nkojiadmin\n));\n\n\n\n\n\n\nKoji-Hub\n\n\n\n\n\n\nKoji-hub is the center of all Koji operations. It is an XML-RPC server running under mod_python in Apache. koji-hub is passive in that it only receives XML-RPC calls and relies upon the build daemons and other components to initiate communication. Koji-hub is the only component that has direct access to the database and is one of the two components that have write access to the file system.\n\n\n\n\n\n\nConfig files we care about:\n\n\n\n\n/etc/httpd/conf/httpd.conf\n\n\n/etc/httpd/conf.d/kojihub.conf\n\n\n/etc/httpd/conf.d/ssl.conf\n\n\n/etc/koji-hub/hub.conf\n\n\n\n\nInstall the necessary packages.\n\n\n[root@koji-hub]#\n yum install koji-hub httpd mod_ssl mod_python\n\n\n\n\n\n\n\n\n\n/etc/httpd/conf/httpd.conf:\n\n\nThe apache web server has two places that it sets maximum requests a server will handle before the server restarts. The xmlrpc interface in kojihub is a python application, and mod_python can sometimes grow outrageously large when it doesn't reap memory often enough. As a result, it is strongly recommended that you set both instances of MaxRequestsPerChild in \nhttpd.conf\n to something reasonable in order to prevent the server from becoming overloaded and crashing (at 100 the \nhttpd\n processes will grow to about 75MB resident set size before respawning).\n\n\nIfModule\n \nprefork.c\n\n...\nMaxRequestsPerChild  100\n\n/IfModule\n\n\nIfModule\n \nworker.c\n\n...\nMaxRequestsPerChild  100\n\n/IfModule\n\n\n\n\n\n\n/etc/koji-hub/hub.conf:\n\n\nThis file contains the configuration information for the hub. You will need to edit this configuration to point Koji Hub to the database you are using and to setup Koji Hub to utilize the authentication scheme you selected in the beginning.\n\n\n\n\n\n\nWe made multiple changes to this config file.\n\n\nNotable changes:\n\n\n\n\nWe turned off \nLoginCreatesUser\n\n\nKojiWebURL\n is \nhttp://koji-hub.batlab.org/koji\n\n\nPluginPath\n is \n/usr/lib/koji-hub-plugins\n\n\nPlugins = sign\n since there is an RPM signing plugin (package name: \nkoji-plugin-sign\n) that we use\n\n\n\n\nOther notes:\n\n\n\n\nShould not be world-readable\n\n\nMust be readable by the \napache\n user though\n\n\n\n\nFinally, there is a \n[policy]\n section for controlling ACLs. The contents of that are described later in this document.\n\n\n\n\n\n\n/etc/httpd/conf.d/kojihub.conf:\n\n\nIf using SSL auth, uncomment these lines for kojiweb to allow logins.\n\n\nLocation\n \n/kojihub\n\nSSLOptions +StdEnvVars\n\n/Location\n\n\n\n\n\n\n\n\n\n\nThis is actually outdated information, useful for Koji \n 1.4.0. Instead, we add the following:\n\n\nLocation\n \n/kojihub/ssllogin\n\n        \nSSLVerifyClient\n require\n        \nSSLVerifyDepth\n  \n10\n\n        \nSSLOptions\n +StdEnvVars\n\n/Location\n\n\n\n\n\n\n\n\n\n\n/etc/httpd/conf.d/ssl.conf:\n\n\nIf using SSL you will also need to add the needed SSL options for apache. These options should point to where the certificates are located on the hub.\n\n\n\n\n\n\nThese are the config lines we use:\n\n\n## The host cert for koji-hub\n\n\nSSLCertificateFile\n \n/etc/pki/tls/certs/digicert_hostcert.crt\n\n\n\n## The private key for koji-hub\n\n\nSSLCertificateKeyFile\n \n/etc/pki/tls/private/digicert_hostkey.key\n\n\n\n## The concatenation of the DigiCert CA certs we made above.\n\n\nSSLCertificateChainFile\n \n/etc/pki/tls/certs/digicert-chain.crt\n\n\n\n## All the CA certs on the system.\n\n\nSSLCACertificateFile\n \n/etc/pki/tls/certs/ca-bundle.crt\n\n\n\nSSLVerifyClient\n require\n\nSSLVerifyDepth\n  \n10\n\n\n\n\n\n\nRestart \nhttpd\n after doing this.\n\n\nAdding a real admin user\n\n\nAt this point, SSL should be set up on Koji Hub. Create an account for yourself (your CN) and give yourself the admin bit:\n\n\n[kojiadmin@koji-hub]$\n koji add-user \nYour Name 123456\n\n\n[kojiadmin@koji-hub]$\n koji grant-permission admin \nYour Name 123456\n\n\n\n\n\n\nOnce you're done with all the setup\n, you should remove the admin bit from \nkojiadmin\n, block the user, and remove its password from the database.\n\n\n[you@koji-hub]$\n koji revoke-permission admin kojiadmin\n\n[you@koji-hub]$\n koji disable-user kojiadmin\n\n\n\n\n\nIn the database:\n\n\nkoji=\n  \nupdate\n \nusers\n \nset\n \npassword\n \n=\n \nnull\n \nwhere\n \nname\n=\nkojiadmin\n;\n\n\n\n\n\n\nKoji filesystem\n\n\nKojiDir\n is \n/mnt/koji\n, so the following tree should be created:\n\n\n[root@koji-hub]#\n mkdir -p /mnt/koji/\n{\npackages,repos,work,scratch\n}\n\n\n[root@koji-hub]#\n chown apache:apache *\n\n\n\n\n\nAccounts for kojira and kojid\n\n\n\n\n\n\nIt is important to note that the kojira component needs repo privileges, but if you just let the account get auto created the first time you run kojira, it won't have that privilege, so you should pre-create the account and grant it the repo privilege now.\n\n\nkojiadmin@koji-hub$ koji add-user kojira\nkojiadmin@koji-hub$ koji grant-permission repo kojira\n\n\n\n\n\nFor similar technical reasons, you need to add-host each build host prior to starting kojid on that host the first time and could also do that now.\n\n\n\n\n\n\nWe turned off auto account creation, so there won't be a problem with accounts with bad perms getting created. This means they have to be created manually. We have kojid running on koji-hub, so this is the command we used to add it:\n\n\n[you@koji-hub]$\n koji add-host koji-hub.batlab.org i386 x86_64\n\n\n\n\n\nKoji-Web, Kojid\n\n\nInstalling:\n\n\n[root@koji-hub]#\n yum install koji-web mod_ssl\n\n[root@koji-hub]#\n yum install koji-builder\n\n\n\n\n\nNote that we use a patched koji-builder, so be sure to install what's in the OSG repository. (The patch is not likely to be accepted upstream).\n\n\nSSL certs\n\n\nThe following files should be in the \n/etc/pki/tls\n tree:\n\n\n\n\n\n\n/etc/pki/tls/private/digicert_hostkey.key\n\n    the host key from DigiCert\n\n\n\n\n\n\n/etc/pki/tls/private/kojiweb.pem\n\n    digicert_hostcert.crt catted together with digicert_hostkey.key (public key first)\n\n\n\n\n\n\n/etc/pki/tls/private/kojira.key\n\n    kojira's private key from DigiCert\n\n\n\n\n\n\n/etc/pki/tls/private/kojira.pem\n\n    kojira.crt catted together with kojira.key (public key first)\n\n\n\n\n\n\n/etc/pki/tls/cert.pem\n\n    symlink to certs/ca-bundle.crt\n\n\n\n\n\n\n/etc/pki/tls/certs/ca-bundle.crt\n\n    all certs available on the system; the DigiCert Grid certs should be in here\n\n\n\n\n\n\n/etc/pki/tls/certs/kojira.crt\n\n    kojira's cert\n\n\n\n\n\n\n/etc/pki/tls/certs/digicert_hostcert.crt\n\n    the host cert from DigiCert\n\n\n\n\n\n\n/etc/pki/tls/certs/digicert-chain.crt\n\n    the DigiCert CAs catted together\n\n\n\n\n\n\nAdding koji-builder host to database\n\n\n[you@koji-hub]$\n koji add-host-to-channel koji-hub.batlab.org createrepo\n\n\n\n\n\nIn the database:\n\n\nkoji=\n#\n  \nupdate\n \nhost\n \nset\n \ncapacity\n \n=\n \n4\n \nwhere\n \nname\n \n=\n \nkoji-hub.batlab.org\n;\n\n\n\n\n\n\nStart kojid\n\n\n[root@koji-hub]#\n /sbin/service kojid start\n\n\n\n\n\nCheck \n/var/log/kojid.log\n to make sure everything started up.\n\n\nKojira\n\n\nInstalling:\n\n\n[root@koji-hub]#\n yum install koji-utils\n\n\n\n\n\nKojira also needs a user\n\n\n[you@koji-hub]$\n koji add-user kojira\n\n[you@koji-hub]$\n koji grant-permission repo kojira\n\n\n\n\n\nStart it up\n\n\n[root@koji-hub]#\n /sbin/service kojira start", 
            "title": "Koji Initial Install"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#notes-on-koji-initial-install", 
            "text": "Note  This document was written for the original install of koji/koji-hub version\n1.6 to  koji-hub.batlab.org .  The document is kept for historical\npurposes, as this setup is no longer accurate with newer versions of Koji,\nmachine moves and renames, cert authority changes, etc.", 
            "title": "Notes on Koji initial install"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#machine-setup", 
            "text": "koji is run on  koji-hub.batlab.org ; right now a single machine was used for the hub (koji-hub), the web frontend (koji-web), repo generation (kojira), and el5 building (kojid). A second machine,  kojibuilder1.batlab.org  was used for el6 building.  As instructions for setting up the machine, I'm just going to take the Fedora guide at  http://fedoraproject.org/wiki/Koji/ServerHowTo , and add modifications / comments to suit our setup.  Sections taken from the Fedora guide will be between dividers like this:     For an overview of yum, mock, Koji (and all its subcomponents), mash, and how they all work together, see the excellent slides put together by Steve Traylen at CERN  http://indico.cern.ch/event/55091", 
            "title": "Machine setup"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#packages-to-install", 
            "text": "On the server (koji-hub/koji-web)   httpd  mod_ssl  postgresql-server  mod_python ( = 3.3.1 for Kerberos authentication)   On the builder (koji-builder)   mock  setarch (for some archs you'll require a patched version)  rpm-build  createrepo     We used one machine for both these roles, so all of the above had to be installed.  The koji packages to install are:   koji-hub  koji-web  koji-builder  koji-utils", 
            "title": "Packages to install"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#filesystems", 
            "text": "", 
            "title": "Filesystems"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#a-note-on-filesystem-space", 
            "text": "Koji will consume copious amounts of disk space under the primary KojiDir directory (as set in the kojihub.conf file).    This is  /mnt/koji  by default. Koji keeps  all RPMs built  in here so a complete history is kept. In addition, repositories are kept here. A repository is just a set of text files, but since a new one is generated for every build, the space will add up. Koji does clean up the repositories once they get old enough. For  koji-hub.batlab.org , we allocated a 120G partition for  /mnt/koji .    However, as koji makes use of mock on the backend to actually create build roots and perform the builds in those build roots, it might come to a surprise to users that a running koji server will consume large amounts of disk space under /var/lib/mock and /var/cache/mock as well.    kojid  (the builder daemon) will refuse to start a build if it does not have sufficient space under  /var/lib/mock . By default, this is an 8G surplus, but we lowered it to 4G for  koji-hub.batlab.org . The build roots will be wiped after successful builds, but the build roots for failed builds will be kept around for 2 weeks (?); as a result, disk usage of  /var/lib/mock  can swing wildly. For  koji-hub.batlab.org , we have allocated 35G for  /var/lib/mock .  /var/cache/mock  doesn't use up that much space, so we didn't make a separate partition for it, it just uses what's under  / .", 
            "title": "A note on filesystem space"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#authentication", 
            "text": "Most of the \"Koji Authentication Selection\" section can be skipped. We got our certs from DigiCert.  Note  that certs are sensitive to line ending issues -- you may have to run  dos2unix  on them before they would work.  We have 3 certs, with the following subjects:   /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=koji-hub.batlab.org  (for koji-hub, koji-web, and kojid)  /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=kojira/koji-hub.batlab.org  (for kojira)  /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=kojibuilder1.batlab.org  (for kojid on kojibuilder1)   kojid needed a different cert than kojira because the two would connect to koji-hub at the same time and log each other off.  Copies of the certs exist in AFS at  /p/condor/home/certificates/koji-hub.batlab.org/  and  /p/condor/home/certificates/kojibuilder1.batlab.org/  Clients connecting to koji-hub need a file containing the CA certs; this is just a concatenation of the DigiCert Grid Root CA and the DigiCert Grid CA-1 certs, which can be obtained from DigiCert's website (get PEM format). We have a  digicert-chain.crt  in  /etc/pki/tls/certs/digicert-chain.crt  on  koji-hub  We made a  kojiadmin  user, but we didn't make a cert for it so we just used user/pass authentication until we set up our own accounts and then disabled  kojiadmin .", 
            "title": "Authentication"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#using-proxy-certs", 
            "text": "/etc/sysconfig/httpd  needed to be changed to include the following lines:  OPENSSL_ALLOW_PROXY = 1  OPENSSL_ALLOW_PROXY_CERTS = 1  export  OPENSSL_ALLOW_PROXY export  OPENSSL_ALLOW_PROXY_CERTS  The user must use RFC proxies and must have a version of the koji client of 1.6.0-6.osg or newer.", 
            "title": "Using proxy certs"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#postgres-database", 
            "text": "Install postgres:  [root@koji-hub]# yum install postgresql-server  Init the db:  [root@koji-hub]# su - postgres -c \"PGDATA=/var/lib/pgsql/data\" initdb  Start the db:  [root@koji-hub]# service postgresql start  Make a koji account:  [root@koji-hub]# useradd koji; passwd -d koji", 
            "title": "Postgres Database"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#setup-postgresql-and-populate-schema", 
            "text": "The following commands will create the  koji  user within PostgreSQL and will then create the koji database using the schema within the  /usr/share/doc/koji*/docs/schema.sql  directory  root@localhost$ su - postgres\npostgres@localhost$ createuser koji\nShall the new role be a superuser? (y/n) n\nShall the new role be allowed to create databases? (y/n) n\nShall the new role be allowed to create more new roles? (y/n) n\npostgres@localhost$ createdb -O koji koji\npostgres@localhost$ logout\nroot@localhost$ su - koji\nkoji@localhost$ psql koji koji   /usr/share/doc/koji*/docs/schema.sql\nkoji@localhost$ exit  NOTE:  When issuing the command to import the psql schema into the new database it is important to ensure that the directory path  /usr/share/doc/koji*/docs/schema.sql  remains intact and is not resolved to a specific version of koji. In test it was discovered that when the path is resolved to a specific version of koji then not all of the tables were created correctly.    To authorize the koji-web and koji-hub resources, make the following additions to  /var/lib/pgsql/data/pg_hba.conf  (IP addresses will vary):  # access for koji\nhost    koji        postgres    127.0.0.1/32          trust\nhost    koji        koji        127.0.0.1/32          trust\nhost    koji        koji        128.104.100.41/32     trust\nhost    koji        koji        ::1/128               trust  Next, we'll need a koji admin user to run some commands. We'll need to add it to the database manually. Once again, we used user/pass authentication for the kojiadmin user until we disabled it. All database commands should be done as the  koji  user:  [root@koji-hub]#  sudo -u koji psql  koji=    insert   into   users   ( name ,   password ,   status ,   usertype )   values \n           ( kojiadmin ,   some-throwaway-admin-password-in-plain-text ,   0 ,   0 );  koji=    insert   into   user_perms   ( user_id ,   perm_id ,   creator_id )   values \n           (( select   id   from   users   where   name = kojiadmin ),   1 , \n            ( select   id   from   users   where   name = kojiadmin ));", 
            "title": "Setup PostgreSQL and populate schema:"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#koji-hub", 
            "text": "Koji-hub is the center of all Koji operations. It is an XML-RPC server running under mod_python in Apache. koji-hub is passive in that it only receives XML-RPC calls and relies upon the build daemons and other components to initiate communication. Koji-hub is the only component that has direct access to the database and is one of the two components that have write access to the file system.    Config files we care about:   /etc/httpd/conf/httpd.conf  /etc/httpd/conf.d/kojihub.conf  /etc/httpd/conf.d/ssl.conf  /etc/koji-hub/hub.conf   Install the necessary packages.  [root@koji-hub]#  yum install koji-hub httpd mod_ssl mod_python", 
            "title": "Koji-Hub"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#etchttpdconfhttpdconf", 
            "text": "The apache web server has two places that it sets maximum requests a server will handle before the server restarts. The xmlrpc interface in kojihub is a python application, and mod_python can sometimes grow outrageously large when it doesn't reap memory often enough. As a result, it is strongly recommended that you set both instances of MaxRequestsPerChild in  httpd.conf  to something reasonable in order to prevent the server from becoming overloaded and crashing (at 100 the  httpd  processes will grow to about 75MB resident set size before respawning).  IfModule   prefork.c \n...\nMaxRequestsPerChild  100 /IfModule  IfModule   worker.c \n...\nMaxRequestsPerChild  100 /IfModule", 
            "title": "/etc/httpd/conf/httpd.conf:"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#etckoji-hubhubconf", 
            "text": "This file contains the configuration information for the hub. You will need to edit this configuration to point Koji Hub to the database you are using and to setup Koji Hub to utilize the authentication scheme you selected in the beginning.    We made multiple changes to this config file.  Notable changes:   We turned off  LoginCreatesUser  KojiWebURL  is  http://koji-hub.batlab.org/koji  PluginPath  is  /usr/lib/koji-hub-plugins  Plugins = sign  since there is an RPM signing plugin (package name:  koji-plugin-sign ) that we use   Other notes:   Should not be world-readable  Must be readable by the  apache  user though   Finally, there is a  [policy]  section for controlling ACLs. The contents of that are described later in this document.", 
            "title": "/etc/koji-hub/hub.conf:"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#etchttpdconfdkojihubconf", 
            "text": "If using SSL auth, uncomment these lines for kojiweb to allow logins.  Location   /kojihub \nSSLOptions +StdEnvVars /Location     This is actually outdated information, useful for Koji   1.4.0. Instead, we add the following:  Location   /kojihub/ssllogin \n         SSLVerifyClient  require\n         SSLVerifyDepth    10 \n         SSLOptions  +StdEnvVars /Location", 
            "title": "/etc/httpd/conf.d/kojihub.conf:"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#etchttpdconfdsslconf", 
            "text": "If using SSL you will also need to add the needed SSL options for apache. These options should point to where the certificates are located on the hub.    These are the config lines we use:  ## The host cert for koji-hub  SSLCertificateFile   /etc/pki/tls/certs/digicert_hostcert.crt  ## The private key for koji-hub  SSLCertificateKeyFile   /etc/pki/tls/private/digicert_hostkey.key  ## The concatenation of the DigiCert CA certs we made above.  SSLCertificateChainFile   /etc/pki/tls/certs/digicert-chain.crt  ## All the CA certs on the system.  SSLCACertificateFile   /etc/pki/tls/certs/ca-bundle.crt  SSLVerifyClient  require SSLVerifyDepth    10   Restart  httpd  after doing this.", 
            "title": "/etc/httpd/conf.d/ssl.conf:"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#adding-a-real-admin-user", 
            "text": "At this point, SSL should be set up on Koji Hub. Create an account for yourself (your CN) and give yourself the admin bit:  [kojiadmin@koji-hub]$  koji add-user  Your Name 123456  [kojiadmin@koji-hub]$  koji grant-permission admin  Your Name 123456   Once you're done with all the setup , you should remove the admin bit from  kojiadmin , block the user, and remove its password from the database.  [you@koji-hub]$  koji revoke-permission admin kojiadmin [you@koji-hub]$  koji disable-user kojiadmin  In the database:  koji=    update   users   set   password   =   null   where   name = kojiadmin ;", 
            "title": "Adding a real admin user"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#koji-filesystem", 
            "text": "KojiDir  is  /mnt/koji , so the following tree should be created:  [root@koji-hub]#  mkdir -p /mnt/koji/ { packages,repos,work,scratch }  [root@koji-hub]#  chown apache:apache *", 
            "title": "Koji filesystem"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#accounts-for-kojira-and-kojid", 
            "text": "It is important to note that the kojira component needs repo privileges, but if you just let the account get auto created the first time you run kojira, it won't have that privilege, so you should pre-create the account and grant it the repo privilege now.  kojiadmin@koji-hub$ koji add-user kojira\nkojiadmin@koji-hub$ koji grant-permission repo kojira  For similar technical reasons, you need to add-host each build host prior to starting kojid on that host the first time and could also do that now.    We turned off auto account creation, so there won't be a problem with accounts with bad perms getting created. This means they have to be created manually. We have kojid running on koji-hub, so this is the command we used to add it:  [you@koji-hub]$  koji add-host koji-hub.batlab.org i386 x86_64", 
            "title": "Accounts for kojira and kojid"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#koji-web-kojid", 
            "text": "Installing:  [root@koji-hub]#  yum install koji-web mod_ssl [root@koji-hub]#  yum install koji-builder  Note that we use a patched koji-builder, so be sure to install what's in the OSG repository. (The patch is not likely to be accepted upstream).", 
            "title": "Koji-Web, Kojid"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#ssl-certs", 
            "text": "The following files should be in the  /etc/pki/tls  tree:    /etc/pki/tls/private/digicert_hostkey.key \n    the host key from DigiCert    /etc/pki/tls/private/kojiweb.pem \n    digicert_hostcert.crt catted together with digicert_hostkey.key (public key first)    /etc/pki/tls/private/kojira.key \n    kojira's private key from DigiCert    /etc/pki/tls/private/kojira.pem \n    kojira.crt catted together with kojira.key (public key first)    /etc/pki/tls/cert.pem \n    symlink to certs/ca-bundle.crt    /etc/pki/tls/certs/ca-bundle.crt \n    all certs available on the system; the DigiCert Grid certs should be in here    /etc/pki/tls/certs/kojira.crt \n    kojira's cert    /etc/pki/tls/certs/digicert_hostcert.crt \n    the host cert from DigiCert    /etc/pki/tls/certs/digicert-chain.crt \n    the DigiCert CAs catted together", 
            "title": "SSL certs"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#adding-koji-builder-host-to-database", 
            "text": "[you@koji-hub]$  koji add-host-to-channel koji-hub.batlab.org createrepo  In the database:  koji= #    update   host   set   capacity   =   4   where   name   =   koji-hub.batlab.org ;", 
            "title": "Adding koji-builder host to database"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#start-kojid", 
            "text": "[root@koji-hub]#  /sbin/service kojid start  Check  /var/log/kojid.log  to make sure everything started up.", 
            "title": "Start kojid"
        }, 
        {
            "location": "/infrastructure/koji-initial-install/#kojira", 
            "text": "Installing:  [root@koji-hub]#  yum install koji-utils  Kojira also needs a user  [you@koji-hub]$  koji add-user kojira [you@koji-hub]$  koji grant-permission repo kojira  Start it up  [root@koji-hub]#  /sbin/service kojira start", 
            "title": "Kojira"
        }, 
        {
            "location": "/projects/sha2-support/", 
            "text": "SHA-2 Compliance\n\n\nWhen a certificate authority signs a certificate, it uses one of several possible hash algorithms. \nHistorically, the most popular algorithms were MD5 (now retired due to security issues) and the SHA-1 family.\nSHA-1 certificates are being phased out due to perceived weaknesses \u2014 as of February 2017, a practical attack for generating collisions was demonstrated by \nGoogle researchers\n.\n These days, the preferred hash algorithm family is SHA-2.\n\n\nThe certificate authorities (CAs), which issue host and user certificates used widely in the OSG, defaulted to SHA-2-based certificates on 1 October 2013; all sites will need to make sure that their software supports certificates using the SHA-2 algorithms. All supported OSG releases support SHA-2.\n\n\nThe table below denotes indicates the minimum releases necessary to support SHA-2 certificates.\n\n\n\n\n\n\n\n\nComponent\n\n\nVersion\n\n\nIn Release\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nBeStMan 2\n\n\nbestman2-2.3.0-9.osg\n\n\n3.1.13\n\n\nSHA-2 support; also see jGlobus, below\n\n\n\n\n\n\ndCache SRM client\n\n\ndcache-srmclient-2.2.11.1-2.osg\n\n\n3.1.22\n\n\nMajor update includes SHA-2 support\n\n\n\n\n\n\nGlobus GRAM\n\n\nglobus-gram-job-manager-13.45-1.2.osg, globus-gram-job-manager-condor-1.0-13.1.osg, globus-gram-job-manager-pbs-1.6-1.1.osg\n\n\n3.1.9\n\n\nCritical bug fixes (not SHA-2 specific)\n\n\n\n\n\n\nGUMS\n\n\ngums-1.3.18.009-15.2.osg\n\n\n3.1.13\n\n\nSwitched to jGlobus 2 with SHA-2 support; also see jGlobus, below\n\n\n\n\n\n\njGlobus (for BeStMan 2)\n\n\njglobus-2.0.5-3.osg\n\n\n3.1.18\n\n\nFixed CRL refresh bug (not SHA-2 specific)\n\n\n\n\n\n\nVOMS\n\n\nvoms-2.0.8-1.5.osg\n\n\n3.1.17\n\n\nSHA-2 fix for voms-proxy-init\n\n\n\n\n\n\n\n\nIf a component does not appear in the above table, it already has SHA-2 support.", 
            "title": "SHA-2 Support"
        }, 
        {
            "location": "/projects/sha2-support/#sha-2-compliance", 
            "text": "When a certificate authority signs a certificate, it uses one of several possible hash algorithms. \nHistorically, the most popular algorithms were MD5 (now retired due to security issues) and the SHA-1 family.\nSHA-1 certificates are being phased out due to perceived weaknesses \u2014 as of February 2017, a practical attack for generating collisions was demonstrated by  Google researchers .\n These days, the preferred hash algorithm family is SHA-2.  The certificate authorities (CAs), which issue host and user certificates used widely in the OSG, defaulted to SHA-2-based certificates on 1 October 2013; all sites will need to make sure that their software supports certificates using the SHA-2 algorithms. All supported OSG releases support SHA-2.  The table below denotes indicates the minimum releases necessary to support SHA-2 certificates.     Component  Version  In Release  Notes      BeStMan 2  bestman2-2.3.0-9.osg  3.1.13  SHA-2 support; also see jGlobus, below    dCache SRM client  dcache-srmclient-2.2.11.1-2.osg  3.1.22  Major update includes SHA-2 support    Globus GRAM  globus-gram-job-manager-13.45-1.2.osg, globus-gram-job-manager-condor-1.0-13.1.osg, globus-gram-job-manager-pbs-1.6-1.1.osg  3.1.9  Critical bug fixes (not SHA-2 specific)    GUMS  gums-1.3.18.009-15.2.osg  3.1.13  Switched to jGlobus 2 with SHA-2 support; also see jGlobus, below    jGlobus (for BeStMan 2)  jglobus-2.0.5-3.osg  3.1.18  Fixed CRL refresh bug (not SHA-2 specific)    VOMS  voms-2.0.8-1.5.osg  3.1.17  SHA-2 fix for voms-proxy-init     If a component does not appear in the above table, it already has SHA-2 support.", 
            "title": "SHA-2 Compliance"
        }, 
        {
            "location": "/meetings/2017/TechArea20170227/", 
            "text": "OSG Technology Area Meeting, 27 February 2017\n\n\nCoordinates:\n Conference: 857-216-4999, PIN: 32390; \nhttps://www.uberconference.com/osgcat\n  \n\n\nAttending:\n   \n\n\nAnnouncements\n\n\n\n\nNo meeting next week, OSG All Hands\n\n\n\n\nTriage Duty\n\n\n\n\nThis week: Mat\n\n\nNext week: Suchandra\n\n\n6 (0) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n141\n\n\n(\n17)\n\n\nOpen\n\n\n\n\n\n\n33\n\n\n(+9)\n\n\nIn Progress\n\n\n\n\n\n\n4\n\n\n(+2)\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n(\n12)\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n3.3.22\n\n\n2017-02-27\n\n\n2017-03-06\n\n\n2017-03-14\n\n\n\n\n\n\n\n\n3.3.23\n\n\n2017-03-27\n\n\n2017-04-03\n\n\n2017-04-11\n\n\n\n\n\n\n\n\n3.3.24\n\n\n2017-04-25\n\n\n2017-05-01\n\n\n2017-05-09\n\n\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\nDiscussions\n\n\nSoftware freeze today  \n\n\n\n\n\n\n\n\nAssignee\n\n\nTickets Not RFT\n\n\n\n\n\n\n\n\n\n\nBrianL\n\n\n7\n\n\n\n\n\n\nMat\n\n\n7\n\n\n\n\n\n\nCarl\n\n\n4\n\n\n\n\n\n\nEdgar\n\n\n2\n\n\n\n\n\n\nDerek\n\n\n2\n\n\n\n\n\n\nMarian\n\n\n1\n\n\n\n\n\n\n\n\nSupport Update\n\n\n\n\nBaylor (BrianL) - CE installation help, troubleshooting mapping and blahp issues\n\n\nClemson (BrianL) - blahp segfault, held jobs due to bad folder ownership\n\n\nGeorgia Tech (BrianL) - accounting records reporting start date as 1/1/1970\n\n\n\n\nOSG Release Team\n\n\n\n\nMarch 11th Release - OSG 3.3.22  \n\n\nDevelopment Freeze 2/27\n\n\n\n\n\n\nData Release - IGTF 1.80\n\n\n\n\n\n\n\n\n\n\n3.3.22\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n15\n\n\n(+15)\n\n\nOpen\n\n\n\n\n\n\n11\n\n\n(+11)\n\n\nIn Progress\n\n\n\n\n\n\n2\n\n\n(\n2)\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n(\n3)\n\n\nReady for Release\n\n\n\n\n\n\n28\n\n\n(+16)\n\n\nTotal\n\n\n\n\n\n\n\n\nOSG 3.3.22\n\n\n\n\nTesting  \n\n\nXRootD 4.6.0\n\n\nfrontier-squid 3.5.24-1.1 in Upcoming\n\n\n\n\n\n\n\n\nDiscussions\n\n\nNone this week  \n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nGRACC Transfer Summaries\n\n\nSyracuse StashCache running now.\n\n\nPackaged Auth StashCache is coming from Marian.\n\n\n\n\nThis Week\n\n\n\n\nFinish GRACC Tickets\n\n\nBlahp Merge\n\n\nBegin PEARC paper\n\n\n\n\nOngoing\n\n\n\n\nGratia V2: Derek will be working on this.  Jira project: \nGRACC\n.  Project documentation located at \nhttps://opensciencegrid.github.io/gracc\n.\n\n\nNew StashCache server packaging that is coming out of our collaboration with Syracuse. Authenticated StashCache Package incoming after papers completed.\n\n\nUNL setting up authenticated StashCache as well\n\n\nSee Support Update section - StashCache troubleshooting at BNL, host migration to el7 caused some odd xrootd behavior, investigating", 
            "title": "February 27, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170227/#osg-technology-area-meeting-27-february-2017", 
            "text": "Coordinates:  Conference: 857-216-4999, PIN: 32390;  https://www.uberconference.com/osgcat     Attending:", 
            "title": "OSG Technology Area Meeting, 27 February 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170227/#announcements", 
            "text": "No meeting next week, OSG All Hands", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170227/#triage-duty", 
            "text": "This week: Mat  Next week: Suchandra  6 (0) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170227/#jira", 
            "text": "# of tickets   State      141  ( 17)  Open    33  (+9)  In Progress    4  (+2)  Ready for Testing    0  ( 12)  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170227/#release-schedule", 
            "text": "Version  Development Freeze  Package Freeze  Release  Notes      3.3.22  2017-02-27  2017-03-06  2017-03-14     3.3.23  2017-03-27  2017-04-03  2017-04-11     3.3.24  2017-04-25  2017-05-01  2017-05-09      Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170227/#osg-software-team", 
            "text": "", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170227/#discussions", 
            "text": "Software freeze today       Assignee  Tickets Not RFT      BrianL  7    Mat  7    Carl  4    Edgar  2    Derek  2    Marian  1", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170227/#support-update", 
            "text": "Baylor (BrianL) - CE installation help, troubleshooting mapping and blahp issues  Clemson (BrianL) - blahp segfault, held jobs due to bad folder ownership  Georgia Tech (BrianL) - accounting records reporting start date as 1/1/1970", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170227/#osg-release-team", 
            "text": "March 11th Release - OSG 3.3.22    Development Freeze 2/27    Data Release - IGTF 1.80      3.3.22   Status      15  (+15)  Open    11  (+11)  In Progress    2  ( 2)  Ready for Testing    0  ( 3)  Ready for Release    28  (+16)  Total", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170227/#osg-3322", 
            "text": "Testing    XRootD 4.6.0  frontier-squid 3.5.24-1.1 in Upcoming", 
            "title": "OSG 3.3.22"
        }, 
        {
            "location": "/meetings/2017/TechArea20170227/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170227/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170227/#last-week", 
            "text": "GRACC Transfer Summaries  Syracuse StashCache running now.  Packaged Auth StashCache is coming from Marian.", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170227/#this-week", 
            "text": "Finish GRACC Tickets  Blahp Merge  Begin PEARC paper", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170227/#ongoing", 
            "text": "Gratia V2: Derek will be working on this.  Jira project:  GRACC .  Project documentation located at  https://opensciencegrid.github.io/gracc .  New StashCache server packaging that is coming out of our collaboration with Syracuse. Authenticated StashCache Package incoming after papers completed.  UNL setting up authenticated StashCache as well  See Support Update section - StashCache troubleshooting at BNL, host migration to el7 caused some odd xrootd behavior, investigating", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170417/", 
            "text": "OSG Technology Area Meeting, 17 April 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n  \n\n\nAttending:\n   \n\n\nAnnouncements\n\n\nTriage Duty\n\n\n\n\nThis week: Suchandra\n\n\nNext week: Mat\n\n\n5 (+1) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n166\n\n\n+14\n\n\nOpen\n\n\n\n\n\n\n17\n\n\n+1\n\n\nIn Progress\n\n\n\n\n\n\n0\n\n\n0\n\n\nReady for Testing\n\n\n\n\n\n\n15\n\n\n15\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n3.3.24\n\n\n2017-04-25\n\n\n2017-05-01\n\n\n2017-05-09\n\n\n\n\n\n\n\n\n3.3.25\n\n\n2017-05-30\n\n\n2017-06-05\n\n\n2017-06-13\n\n\n5 week cycle\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\n\n\nDev freeze next week but limited testing time due to HTCondor week and Suchandra's vacation\n\n\nXRootD 4.6.1 release candidate testing at UNL\n\n\n\n\nDiscussions\n\n\n\n\nMarian will speak with Mat about proper XRootD release candidate versioning name\n\n\nBrianL will look into system dashboard solution for missing project summary page\n\n\nFor testing gratia-probe/GRACC interaction, there is a GRACC testing interface that doesn't store the records but provides the proper responses to the probes\n\n\nUPITT (Marian) - XRootD assistance (\nhttps://ticket.grid.iu.edu/32605\n)\n\n\n\n\nSupport Update\n\n\n\n\nBNL (BrianL/Derek) - Worked with Xin to investigate missing CE ScheddAd from central collector caused by \nthis\n bug\n\n\n\n\nOSG Release Team\n\n\n\n\n\n\n\n\n3.3.24\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n17\n\n\n+17\n\n\nOpen\n\n\n\n\n\n\n6\n\n\n+6\n\n\nIn Progress\n\n\n\n\n\n\n0\n\n\n0\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n0\n\n\nReady for Release\n\n\n\n\n\n\n23\n\n\n+23\n\n\nTotal\n\n\n\n\n\n\n\n\nOSG 3.3.24\n\n\n\n\nReady for Testing  \n\n\nNone yet\n\n\n\n\n\n\nReady for Release  \n\n\nNone yet\n\n\n\n\n\n\n\n\nDiscussions\n\n\nIf XRootD 4.6.1 isn't ready by the package freeze, we will revisit the stability of the release candidates\n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nGRACC operations transition\n\n\nXRootD bugs in caching and HTTPS connections. Seems to be fixed.\n\n\nBlahp merge work continues.  Now on to getting binaries in the correct areas.\n\n\n\n\nThis Week\n\n\n\n\nMore GRACC Operations transition\n\n\nMore BLAHP merge\n\n\nmove \n*.osgstorage.org\n CVMFS repos to new host\n\n\n\n\nOngoing\n\n\n\n\nGratia V2: Derek will be working on this.  Jira project: \nGRACC\n.  Project documentation located at \nhttps://opensciencegrid.github.io/gracc\n.\n\n\nNew StashCache server packaging that is coming out of our collaboration with Syracuse. Authenticated StashCache Package incoming after papers completed.", 
            "title": "April 17, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170417/#osg-technology-area-meeting-17-april-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin     Attending:", 
            "title": "OSG Technology Area Meeting, 17 April 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170417/#announcements", 
            "text": "", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170417/#triage-duty", 
            "text": "This week: Suchandra  Next week: Mat  5 (+1) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170417/#jira", 
            "text": "# of tickets   State      166  +14  Open    17  +1  In Progress    0  0  Ready for Testing    15  15  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170417/#release-schedule", 
            "text": "Version  Development Freeze  Package Freeze  Release  Notes      3.3.24  2017-04-25  2017-05-01  2017-05-09     3.3.25  2017-05-30  2017-06-05  2017-06-13  5 week cycle     Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170417/#osg-software-team", 
            "text": "Dev freeze next week but limited testing time due to HTCondor week and Suchandra's vacation  XRootD 4.6.1 release candidate testing at UNL", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170417/#discussions", 
            "text": "Marian will speak with Mat about proper XRootD release candidate versioning name  BrianL will look into system dashboard solution for missing project summary page  For testing gratia-probe/GRACC interaction, there is a GRACC testing interface that doesn't store the records but provides the proper responses to the probes  UPITT (Marian) - XRootD assistance ( https://ticket.grid.iu.edu/32605 )", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170417/#support-update", 
            "text": "BNL (BrianL/Derek) - Worked with Xin to investigate missing CE ScheddAd from central collector caused by  this  bug", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170417/#osg-release-team", 
            "text": "3.3.24   Status      17  +17  Open    6  +6  In Progress    0  0  Ready for Testing    0  0  Ready for Release    23  +23  Total", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170417/#osg-3324", 
            "text": "Ready for Testing    None yet    Ready for Release    None yet", 
            "title": "OSG 3.3.24"
        }, 
        {
            "location": "/meetings/2017/TechArea20170417/#discussions_1", 
            "text": "If XRootD 4.6.1 isn't ready by the package freeze, we will revisit the stability of the release candidates", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170417/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170417/#last-week", 
            "text": "GRACC operations transition  XRootD bugs in caching and HTTPS connections. Seems to be fixed.  Blahp merge work continues.  Now on to getting binaries in the correct areas.", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170417/#this-week", 
            "text": "More GRACC Operations transition  More BLAHP merge  move  *.osgstorage.org  CVMFS repos to new host", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170417/#ongoing", 
            "text": "Gratia V2: Derek will be working on this.  Jira project:  GRACC .  Project documentation located at  https://opensciencegrid.github.io/gracc .  New StashCache server packaging that is coming out of our collaboration with Syracuse. Authenticated StashCache Package incoming after papers completed.", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170424/", 
            "text": "OSG Technology Area Meeting, 24 April 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n  \n\n\nAttending:\n BrianL, Carl, Derek, Edgar, Jeff, Marian, Mat, TimC, TimT\n\n\nAnnouncements\n\n\n\n\nBrianL on vacation Wednesday through Friday\n\n\nSuchandra on vacation until 5/5\n\n\nCondor week starts next Tuesday, 5/2\n\n\n\n\nTriage Duty\n\n\n\n\nThis week: Mat\n\n\nNext week: TimT\n\n\n4 (\n1) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n171\n\n\n+5\n\n\nOpen\n\n\n\n\n\n\n23\n\n\n+6\n\n\nIn Progress\n\n\n\n\n\n\n3\n\n\n+3\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n0\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n3.3.24\n\n\n2017-04-25\n\n\n2017-05-01\n\n\n2017-05-09\n\n\n\n\n\n\n\n\n3.3.25\n\n\n2017-05-30\n\n\n2017-06-05\n\n\n2017-06-13\n\n\n5 week cycle\n\n\n\n\n\n\n3.3.26\n\n\n2017-06-26\n\n\n2017-07-03\n\n\n2017-07-11\n\n\nIndependence Day\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\n\n\n\n\nDev freeze today, tickets not RFT by owner:  \n\n\n\n\n\n\n\n\nDeveloper\n\n\n#\n\n\n\n\n\n\n\n\n\n\nMat\n\n\n12\n\n\n\n\n\n\nMarian\n\n\n2\n\n\n\n\n\n\nBrianL\n\n\n1\n\n\n\n\n\n\nCarl\n\n\n1\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussions\n\n\nosg-build does not yet support building packages for 3.4. Expected support available in May.\n\n\nSupport Update\n\n\n\n\nUMich (BrianL/Jeff/Marian) - Bob Ball's rebuilt CE is experiencing security handshake timeouts. Also had an issue where pilots were running even though his site was in downtime\n\n\n\n\nOSG Release Team\n\n\n\n\n\n\n\n\n3.3.24\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n8\n\n\n-9\n\n\nOpen\n\n\n\n\n\n\n8\n\n\n+2\n\n\nIn Progress\n\n\n\n\n\n\n3\n\n\n+3\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n0\n\n\nReady for Release\n\n\n\n\n\n\n19\n\n\n-4\n\n\nTotal\n\n\n\n\n\n\n\n\nOSG 3.3.24\n\n\n\n\nTim Theisen is handling the \nMay 9th\n release\n\n\nDevelopment freeze today\n\n\nVO Package ??\n\n\n\n\nNeed help testing; Xin gone, Suchandra on vacation, Brian Lin mired in management, HTCondor Week next week\n\n\n\n\n\n\nReady for Testing  \n\n\n\n\nCVMFS X.509 Helper 1.0\n\n\ngsissh in tarballs\n\n\nUpcoming: GlideinWMS 3.3.2 (Needs factory testing)\n\n\n\n\n\n\nReady for Release  \n\n\nNone yet\n\n\n\n\n\n\n\n\nDiscussions\n\n\nNone this week  \n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nGRACC operations transition.  Lots of fires\n\n\nCloud provider is adding more storage, and reconfiguring storage.  Causes IO timeouts for VMs\n\n\n\n\n\n\nBlahp merge work continues. Now on to getting binaries in the correct areas.\n\n\nStash CVMFS repo had to be built from scratch due to bug in CVMFS's auto-catalog (only affects stratum 0's)\n\n\n\n\nThis Week\n\n\n\n\nMore GRACC Operations transition\n\n\nMore BLAHP merge\n\n\nStashCP work to work with multiple origins.\n\n\n\n\nOngoing\n\n\n\n\nGratia V2: Derek will be working on this.  Jira project: \nGRACC\n.  Project documentation located at \nhttps://opensciencegrid.github.io/gracc\n.\n\n\nNew StashCache server packaging that is coming out of our collaboration with Syracuse. Authenticated StashCache Package incoming after papers completed.", 
            "title": "April 24, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170424/#osg-technology-area-meeting-24-april-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin     Attending:  BrianL, Carl, Derek, Edgar, Jeff, Marian, Mat, TimC, TimT", 
            "title": "OSG Technology Area Meeting, 24 April 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170424/#announcements", 
            "text": "BrianL on vacation Wednesday through Friday  Suchandra on vacation until 5/5  Condor week starts next Tuesday, 5/2", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170424/#triage-duty", 
            "text": "This week: Mat  Next week: TimT  4 ( 1) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170424/#jira", 
            "text": "# of tickets   State      171  +5  Open    23  +6  In Progress    3  +3  Ready for Testing    0  0  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170424/#release-schedule", 
            "text": "Version  Development Freeze  Package Freeze  Release  Notes      3.3.24  2017-04-25  2017-05-01  2017-05-09     3.3.25  2017-05-30  2017-06-05  2017-06-13  5 week cycle    3.3.26  2017-06-26  2017-07-03  2017-07-11  Independence Day     Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170424/#osg-software-team", 
            "text": "Dev freeze today, tickets not RFT by owner:       Developer  #      Mat  12    Marian  2    BrianL  1    Carl  1", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170424/#discussions", 
            "text": "osg-build does not yet support building packages for 3.4. Expected support available in May.", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170424/#support-update", 
            "text": "UMich (BrianL/Jeff/Marian) - Bob Ball's rebuilt CE is experiencing security handshake timeouts. Also had an issue where pilots were running even though his site was in downtime", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170424/#osg-release-team", 
            "text": "3.3.24   Status      8  -9  Open    8  +2  In Progress    3  +3  Ready for Testing    0  0  Ready for Release    19  -4  Total", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170424/#osg-3324", 
            "text": "Tim Theisen is handling the  May 9th  release  Development freeze today  VO Package ??   Need help testing; Xin gone, Suchandra on vacation, Brian Lin mired in management, HTCondor Week next week    Ready for Testing     CVMFS X.509 Helper 1.0  gsissh in tarballs  Upcoming: GlideinWMS 3.3.2 (Needs factory testing)    Ready for Release    None yet", 
            "title": "OSG 3.3.24"
        }, 
        {
            "location": "/meetings/2017/TechArea20170424/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170424/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170424/#last-week", 
            "text": "GRACC operations transition.  Lots of fires  Cloud provider is adding more storage, and reconfiguring storage.  Causes IO timeouts for VMs    Blahp merge work continues. Now on to getting binaries in the correct areas.  Stash CVMFS repo had to be built from scratch due to bug in CVMFS's auto-catalog (only affects stratum 0's)", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170424/#this-week", 
            "text": "More GRACC Operations transition  More BLAHP merge  StashCP work to work with multiple origins.", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170424/#ongoing", 
            "text": "Gratia V2: Derek will be working on this.  Jira project:  GRACC .  Project documentation located at  https://opensciencegrid.github.io/gracc .  New StashCache server packaging that is coming out of our collaboration with Syracuse. Authenticated StashCache Package incoming after papers completed.", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170501/", 
            "text": "OSG Technology Area Meeting,  1 May 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n  \n\n\nAttending:\n BrianL, Carl, Derek, Marian, Mat, TimT  \n\n\nAnnouncements\n\n\n\n\nHTCondor Week Tuesday - Friday\n\n\nSuchandra on vacation until 5/5\n\n\n\n\nTriage Duty\n\n\n\n\nThis week: TimT\n\n\nNext week: BrianL\n\n\n10 (+6) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n167\n\n\n4\n\n\nOpen\n\n\n\n\n\n\n18\n\n\n5\n\n\nIn Progress\n\n\n\n\n\n\n18\n\n\n+15\n\n\nReady for Testing\n\n\n\n\n\n\n1\n\n\n+1\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n3.3.24\n\n\n2017-04-25\n\n\n2017-05-01\n\n\n2017-05-09\n\n\n\n\n\n\n\n\n3.3.25\n\n\n2017-05-30\n\n\n2017-06-05\n\n\n2017-06-13\n\n\n5 week cycle\n\n\n\n\n\n\n3.3.26\n\n\n2017-06-26\n\n\n2017-07-03\n\n\n2017-07-11\n\n\nIndependence Day\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\n\n\nPackage freeze today\n\n\nUtilizing JIRA ticket type and dropping unused types:\n\n\nAccess\n\n\nChange\n\n\nFault\n\n\nIT Help\n\n\nOngoing\n\n\nPurchase\n\n\nRequest\n\n\nStory\n\n\nSub-task-ongoing\n\n\nTechnical task\n\n\n\n\n\n\n\n\nDiscussions\n\n\n\n\nXRootD 4.6.1 release candidate has CMS/OSG approval (Marian) but no new version cut yet. Punting to the next release.\n\n\nVOMS Admin Server also punted to next release.\n\n\nBrianL will start removing above unused JIRA ticket types next week and writing a summary for remaining ticket types\n\n\n\n\nSupport Update\n\n\n\n\nUMich (BrianL) - Bob Ball's security handshakes were due to a local network issue\n\n\nUW-Madison (Carl) - Working on discrepancies with APEL reporting\n\n\n\n\nOSG Release Team\n\n\n\n\nTim Theisen is handling the \nMay 9th\n release\n\n\nPackage freeze today\n\n\nVO Package v73\n\n\nNeed help testing; Xin gone, Suchandra on vacation, Brian Lin mired in management, HTCondor Week this week\n\n\n\n\n\n\n\n\n\n\n3.3.24\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n2\n\n\n-6\n\n\nOpen\n\n\n\n\n\n\n3\n\n\n-5\n\n\nIn Progress\n\n\n\n\n\n\n14\n\n\n+11\n\n\nReady for Testing\n\n\n\n\n\n\n1\n\n\n+1\n\n\nReady for Release\n\n\n\n\n\n\n20\n\n\n+1\n\n\nTotal\n\n\n\n\n\n\n\n\nOSG 3.3.24\n\n\n\n\nReady for Testing  \n\n\nosg-configure 1.7.0\n\n\nEdit lcmaps.db to use the VOMS plugin\n\n\nAdd template lcmaps.db files\n\n\n\n\n\n\ndrop unused BDII data\n\n\nDon't error out if user-vo-map missing, issue warning with suggestion\n\n\n\n\n\n\nFix HTCondor Gratia probe to not call .eval() if not present\n\n\nCVMFS X.509 helper - fix for running inside a container\n\n\ngsissh in tarballs\n\n\nHTCondor 8.6.2 in Upcoming\n\n\nosg-build 1.9.0\n\n\nSplit osg-build into subpackages\n\n\nadd supprt for git repos in .source files (for HCC)\n\n\nosg-build notes default options\n\n\nadd support for 3.4   \n\n\n\n\n\n\n\n\n\n\nReady for Release  \n\n\nUpcoming: GlideinWMS 3.3.2\n\n\n\n\n\n\n\n\nDiscussions\n\n\nOSG Investigations Team\n\n\nTop priority\n is the APEL reports from GRACC.  They need to debugged this week! \nGRACC-19\n\n\nLast Week\n\n\n\n\nLots of GRACC work, still taking some time\n\n\nBLAHP work, lots of little changes here and there.\n\n\nStashcp to handle multiple origins\n\n\nStashCache documentation for admins of caches \n origins\n\n\n\n\nThis Week\n\n\n\n\nMore GRACC transition.\n\n\nFinish up BLAHP initial pull request\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project", 
            "title": "May 1, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170501/#osg-technology-area-meeting-1-may-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin     Attending:  BrianL, Carl, Derek, Marian, Mat, TimT", 
            "title": "OSG Technology Area Meeting,  1 May 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170501/#announcements", 
            "text": "HTCondor Week Tuesday - Friday  Suchandra on vacation until 5/5", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170501/#triage-duty", 
            "text": "This week: TimT  Next week: BrianL  10 (+6) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170501/#jira", 
            "text": "# of tickets   State      167  4  Open    18  5  In Progress    18  +15  Ready for Testing    1  +1  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170501/#release-schedule", 
            "text": "Version  Development Freeze  Package Freeze  Release  Notes      3.3.24  2017-04-25  2017-05-01  2017-05-09     3.3.25  2017-05-30  2017-06-05  2017-06-13  5 week cycle    3.3.26  2017-06-26  2017-07-03  2017-07-11  Independence Day     Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170501/#osg-software-team", 
            "text": "Package freeze today  Utilizing JIRA ticket type and dropping unused types:  Access  Change  Fault  IT Help  Ongoing  Purchase  Request  Story  Sub-task-ongoing  Technical task", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170501/#discussions", 
            "text": "XRootD 4.6.1 release candidate has CMS/OSG approval (Marian) but no new version cut yet. Punting to the next release.  VOMS Admin Server also punted to next release.  BrianL will start removing above unused JIRA ticket types next week and writing a summary for remaining ticket types", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170501/#support-update", 
            "text": "UMich (BrianL) - Bob Ball's security handshakes were due to a local network issue  UW-Madison (Carl) - Working on discrepancies with APEL reporting", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170501/#osg-release-team", 
            "text": "Tim Theisen is handling the  May 9th  release  Package freeze today  VO Package v73  Need help testing; Xin gone, Suchandra on vacation, Brian Lin mired in management, HTCondor Week this week      3.3.24   Status      2  -6  Open    3  -5  In Progress    14  +11  Ready for Testing    1  +1  Ready for Release    20  +1  Total", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170501/#osg-3324", 
            "text": "Ready for Testing    osg-configure 1.7.0  Edit lcmaps.db to use the VOMS plugin  Add template lcmaps.db files    drop unused BDII data  Don't error out if user-vo-map missing, issue warning with suggestion    Fix HTCondor Gratia probe to not call .eval() if not present  CVMFS X.509 helper - fix for running inside a container  gsissh in tarballs  HTCondor 8.6.2 in Upcoming  osg-build 1.9.0  Split osg-build into subpackages  add supprt for git repos in .source files (for HCC)  osg-build notes default options  add support for 3.4         Ready for Release    Upcoming: GlideinWMS 3.3.2", 
            "title": "OSG 3.3.24"
        }, 
        {
            "location": "/meetings/2017/TechArea20170501/#discussions_1", 
            "text": "", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170501/#osg-investigations-team", 
            "text": "Top priority  is the APEL reports from GRACC.  They need to debugged this week!  GRACC-19", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170501/#last-week", 
            "text": "Lots of GRACC work, still taking some time  BLAHP work, lots of little changes here and there.  Stashcp to handle multiple origins  StashCache documentation for admins of caches   origins", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170501/#this-week", 
            "text": "More GRACC transition.  Finish up BLAHP initial pull request", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170501/#ongoing", 
            "text": "GRACC Project", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170508/", 
            "text": "OSG Technology Area Meeting,  8 May 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n  \n\n\nAttending:\n BrianB, BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT  \n\n\nAnnouncements\n\n\nTriage Duty\n\n\n\n\nThis week: BrianL\n\n\nNext week: Carl\n\n\n7 (\n3) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n161\n\n\n3\n\n\nOpen\n\n\n\n\n\n\n20\n\n\n+2\n\n\nIn Progress\n\n\n\n\n\n\n4\n\n\n14\n\n\nReady for Testing\n\n\n\n\n\n\n16\n\n\n+15\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n3.3.24\n\n\n2017-04-25\n\n\n2017-05-01\n\n\n2017-05-09\n\n\n\n\n\n\n\n\n3.3.25\n\n\n2017-05-30\n\n\n2017-06-05\n\n\n2017-06-13\n\n\n5 week cycle\n\n\n\n\n\n\n3.3.26\n\n\n2017-06-26\n\n\n2017-07-03\n\n\n2017-07-11\n\n\nIndependence Day\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\n\n\nDropped unused JIRA ticket types; still need to summarize remaining types\n\n\nOSG 3.4 preparation in full force\n\n\nTWiki -\n GH doc transition needs to begin\n\n\n\n\nDiscussions\n\n\nNone this week  \n\n\nSupport Update\n\n\n\n\nCHTC (BrianL/Jeff) - Asssisted Moate with bringing their frontend back up; turned out to just be a dead httpd service\n\n\nUW-Madison (Carl) - Working on discrepancies with APEL reporting\n\n\n\n\nOSG Release Team\n\n\n\n\nTim Theisen is handling the \nMay 9th\n release\n\n\nRelease tomorrow\n\n\nVO Package v73 - release this week (Wednesday or Thursday)\n\n\n\n\n\n\n\n\n\n\n3.3.24\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n0\n\n\n-2\n\n\nOpen\n\n\n\n\n\n\n0\n\n\n-3\n\n\nIn Progress\n\n\n\n\n\n\n0\n\n\n-14\n\n\nReady for Testing\n\n\n\n\n\n\n17\n\n\n+16\n\n\nReady for Release\n\n\n\n\n\n\n17\n\n\n-3\n\n\nTotal\n\n\n\n\n\n\n\n\nOSG 3.3.24\n\n\n\n\nReady for Release  \n\n\nosg-configure 1.7.0\n\n\nEdit lcmaps.db to use the VOMS plugin\n\n\nAdd template lcmaps.db files\n\n\n\n\n\n\nMake all attributes relating to the defunct BDII service optional\n\n\nDon't error out if user-vo-map missing, issue warning with suggestion\n\n\n\n\n\n\nCVMFS X.509 helper - fix for running inside a container\n\n\ngsissh in tarballs\n\n\nFix HTCondor Gratia probe to not call .eval() if not present\n\n\nosg-build 1.9.0\n\n\nSplit osg-build into subpackages\n\n\nadd supprt for git repos in .source files (for HCC)\n\n\nosg-build notes default options\n\n\nadd support for 3.4   \n\n\n\n\n\n\nUpcoming: HTCondor 8.6.2\n\n\nUpcoming: GlideinWMS 3.3.2\n\n\n\n\n\n\n\n\nDiscussions\n\n\nNone this week\n\n\n\n\n\nOSG Investigations Team\n\n\nInvestigations team is taking a week or 2 of intense effort towards packaging StashCache Authenticated Server.\n\n\nLast Week\n\n\n\n\nDebug some GRACC issues with new changes to OIM VO Field of Science\n\n\nBLAHP work, lots of little changes here and there.\n\n\nStashCache documentation for admins of caches \n origins https://opensciencegrid.github.io/StashCache/\n\n\nGather investigation publications for Tim et. al.\n\n\n\n\nThis Week\n\n\n\n\nHopefully limited GRACC transition.\n\n\nLots of StashCache authenticated packaging.\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project", 
            "title": "May 8, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170508/#osg-technology-area-meeting-8-may-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin     Attending:  BrianB, BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT", 
            "title": "OSG Technology Area Meeting,  8 May 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170508/#announcements", 
            "text": "", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170508/#triage-duty", 
            "text": "This week: BrianL  Next week: Carl  7 ( 3) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170508/#jira", 
            "text": "# of tickets   State      161  3  Open    20  +2  In Progress    4  14  Ready for Testing    16  +15  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170508/#release-schedule", 
            "text": "Version  Development Freeze  Package Freeze  Release  Notes      3.3.24  2017-04-25  2017-05-01  2017-05-09     3.3.25  2017-05-30  2017-06-05  2017-06-13  5 week cycle    3.3.26  2017-06-26  2017-07-03  2017-07-11  Independence Day     Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170508/#osg-software-team", 
            "text": "Dropped unused JIRA ticket types; still need to summarize remaining types  OSG 3.4 preparation in full force  TWiki -  GH doc transition needs to begin", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170508/#discussions", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170508/#support-update", 
            "text": "CHTC (BrianL/Jeff) - Asssisted Moate with bringing their frontend back up; turned out to just be a dead httpd service  UW-Madison (Carl) - Working on discrepancies with APEL reporting", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170508/#osg-release-team", 
            "text": "Tim Theisen is handling the  May 9th  release  Release tomorrow  VO Package v73 - release this week (Wednesday or Thursday)      3.3.24   Status      0  -2  Open    0  -3  In Progress    0  -14  Ready for Testing    17  +16  Ready for Release    17  -3  Total", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170508/#osg-3324", 
            "text": "Ready for Release    osg-configure 1.7.0  Edit lcmaps.db to use the VOMS plugin  Add template lcmaps.db files    Make all attributes relating to the defunct BDII service optional  Don't error out if user-vo-map missing, issue warning with suggestion    CVMFS X.509 helper - fix for running inside a container  gsissh in tarballs  Fix HTCondor Gratia probe to not call .eval() if not present  osg-build 1.9.0  Split osg-build into subpackages  add supprt for git repos in .source files (for HCC)  osg-build notes default options  add support for 3.4       Upcoming: HTCondor 8.6.2  Upcoming: GlideinWMS 3.3.2", 
            "title": "OSG 3.3.24"
        }, 
        {
            "location": "/meetings/2017/TechArea20170508/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170508/#osg-investigations-team", 
            "text": "Investigations team is taking a week or 2 of intense effort towards packaging StashCache Authenticated Server.", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170508/#last-week", 
            "text": "Debug some GRACC issues with new changes to OIM VO Field of Science  BLAHP work, lots of little changes here and there.  StashCache documentation for admins of caches   origins https://opensciencegrid.github.io/StashCache/  Gather investigation publications for Tim et. al.", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170508/#this-week", 
            "text": "Hopefully limited GRACC transition.  Lots of StashCache authenticated packaging.", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170508/#ongoing", 
            "text": "GRACC Project  StashCache Project", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170515/", 
            "text": "OSG Technology Area Meeting, 15 May 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n  \n\n\nAttending:\n BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT  \n\n\nAnnouncements\n\n\nTriage Duty\n\n\n\n\nThis week: Carl\n\n\nNext week: Derek\n\n\n6 (\n1) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n163\n\n\n+2\n\n\nOpen\n\n\n\n\n\n\n26\n\n\n+6\n\n\nIn Progress\n\n\n\n\n\n\n4\n\n\n0\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n16\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n3.3.25\n\n\n2017-05-30\n\n\n2017-06-05\n\n\n2017-06-13\n\n\n5 week cycle\n\n\n\n\n\n\n3.3.26\n\n\n2017-06-26\n\n\n2017-07-03\n\n\n2017-07-11\n\n\nIndependence Day\n\n\n\n\n\n\n3.3.27\n\n\n2017-07-24\n\n\n2017-07-31\n\n\n2017-08-08\n\n\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\n\n\nOSG 3.4 targeted for June, will revisit progress at Software Freeze\n\n\n\n\nDiscussions\n\n\n\n\n3.4.0 testing strategy: mostly automated testing with strategic manual testing, particularly with Globus-dependent software\n\n\nEdgar training new UCSD staff on OSG basics this week\n\n\n\n\nSupport Update\n\n\n\n\nOU (Derek) - slurm gratia-probe not reporting for a few days.  Then suddenly re-appeared.  Going to take some slurm magic to figure it out.  But it's working now.\n\n\n\n\nOSG Release Team\n\n\n\n\nSuchandra Thapa is handling the \nJune 13th\n release\n\n\nDevelopment Release in two weeks\n\n\n\n\nVO Package v73 - release tomorrow\n\n\n\n\n\n\n\n\n3.3.25\n\n\n\n\nBoth\n\n\n\n\n3.4.0\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n20\n\n\n20\n\n\n1\n\n\n1\n\n\n11\n\n\n11\n\n\n32\n\n\n32\n\n\nOpen\n\n\n\n\n\n\n7\n\n\n7\n\n\n0\n\n\n0\n\n\n6\n\n\n6\n\n\n13\n\n\n13\n\n\nIn Progress\n\n\n\n\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\nReady for Release\n\n\n\n\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\nClosed\n\n\n\n\n\n\n29\n\n\n29\n\n\n1\n\n\n1\n\n\n17\n\n\n17\n\n\n47\n\n\n47\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\n\n\nOSG 3.3.25\n\n\n\n\nReady for Testing\n\n\nUpcoming: HTCondor 8.6.3\n\n\n\n\n\n\n\n\nDiscussions\n\n\nNone this week  \n\n\nOSG Investigations Team\n\n\nWeek 2 of StashCache focus.  Investigations team is taking a week or 2 of intense effort towards packaging StashCache Authenticated Server.  \n\n\nLast Week\n\n\n\n\nSetup GRACC-ITB instance - Ongoing\n\n\nBetter monitoring for CVMFS StashCache\n\n\nStashCache documentation for admins of caches \n origins \nhttps://opensciencegrid.github.io/StashCache/\n\n\n\n\nThis Week\n\n\n\n\nImprove docs even more through feedback from sites.\n\n\nLots of StashCache authenticated packaging.\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)", 
            "title": "May 15, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170515/#osg-technology-area-meeting-15-may-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin     Attending:  BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT", 
            "title": "OSG Technology Area Meeting, 15 May 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170515/#announcements", 
            "text": "", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170515/#triage-duty", 
            "text": "This week: Carl  Next week: Derek  6 ( 1) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170515/#jira", 
            "text": "# of tickets   State      163  +2  Open    26  +6  In Progress    4  0  Ready for Testing    0  16  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170515/#release-schedule", 
            "text": "Version  Development Freeze  Package Freeze  Release  Notes      3.3.25  2017-05-30  2017-06-05  2017-06-13  5 week cycle    3.3.26  2017-06-26  2017-07-03  2017-07-11  Independence Day    3.3.27  2017-07-24  2017-07-31  2017-08-08      Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170515/#osg-software-team", 
            "text": "OSG 3.4 targeted for June, will revisit progress at Software Freeze", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170515/#discussions", 
            "text": "3.4.0 testing strategy: mostly automated testing with strategic manual testing, particularly with Globus-dependent software  Edgar training new UCSD staff on OSG basics this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170515/#support-update", 
            "text": "OU (Derek) - slurm gratia-probe not reporting for a few days.  Then suddenly re-appeared.  Going to take some slurm magic to figure it out.  But it's working now.", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170515/#osg-release-team", 
            "text": "Suchandra Thapa is handling the  June 13th  release  Development Release in two weeks   VO Package v73 - release tomorrow     3.3.25   Both   3.4.0   Total   Status      20  20  1  1  11  11  32  32  Open    7  7  0  0  6  6  13  13  In Progress    1  1  0  0  0  0  1  1  Ready for Testing    0  0  0  0  0  0  0  0  Ready for Release    1  1  0  0  0  0  1  1  Closed    29  29  1  1  17  17  47  47  Total", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170515/#osg-3325", 
            "text": "Ready for Testing  Upcoming: HTCondor 8.6.3", 
            "title": "OSG 3.3.25"
        }, 
        {
            "location": "/meetings/2017/TechArea20170515/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170515/#osg-investigations-team", 
            "text": "Week 2 of StashCache focus.  Investigations team is taking a week or 2 of intense effort towards packaging StashCache Authenticated Server.", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170515/#last-week", 
            "text": "Setup GRACC-ITB instance - Ongoing  Better monitoring for CVMFS StashCache  StashCache documentation for admins of caches   origins  https://opensciencegrid.github.io/StashCache/", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170515/#this-week", 
            "text": "Improve docs even more through feedback from sites.  Lots of StashCache authenticated packaging.", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170515/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170522/", 
            "text": "OSG Technology Area Meeting, 22 May 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n  \n\n\nAttending:\n BrianL, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT  \n\n\nAnnouncements\n\n\nMemorial day next Monday, meeting on Tuesday instead.\n\n\nTriage Duty\n\n\n\n\nThis week: Derek\n\n\nNext week: Edgar\n\n\n6 (0) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n156\n\n\n7\n\n\nOpen\n\n\n\n\n\n\n32\n\n\n+6\n\n\nIn Progress\n\n\n\n\n\n\n4\n\n\n0\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n0\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n3.3.25\n\n\n2017-05-30\n\n\n2017-06-05\n\n\n2017-06-13\n\n\n5 week cycle\n\n\n\n\n\n\n3.3.26\n\n\n2017-06-26\n\n\n2017-07-03\n\n\n2017-07-11\n\n\nIndependence Day\n\n\n\n\n\n\n3.3.27\n\n\n2017-07-24\n\n\n2017-07-31\n\n\n2017-08-08\n\n\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\nOSG 3.4.0 targeted for June\n\n\nDiscussions\n\n\nSingularity may be updated in EPEL, Derek will test it this week and if successful, this may be able to be dropped from 3.4.\n\n\nSupport Update\n\n\n\n\nFIT (BrianL) - Assisting with GRAM -\n HTCondor-CE transition\n\n\nMIT (BrianL) - Investigating multiple GridFTP processes spawning on the real server when starting keepalived on the load balancing node\n\n\n\n\nOSG Release Team\n\n\n\n\nSuchandra Thapa is handling the \nJune 13th\n release\n\n\n\n\nDevelopment Freeze next Tuesday\n\n\n\n\n\n\n\n\n3.3.25\n\n\n\n\nBoth\n\n\n\n\n3.4.0\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n2\n\n\n18\n\n\n16\n\n\n15\n\n\n7\n\n\n4\n\n\n25\n\n\n7\n\n\nOpen\n\n\n\n\n\n\n1\n\n\n6\n\n\n9\n\n\n9\n\n\n10\n\n\n4\n\n\n20\n\n\n7\n\n\nIn Progress\n\n\n\n\n\n\n1\n\n\n0\n\n\n3\n\n\n3\n\n\n0\n\n\n0\n\n\n4\n\n\n3\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\nReady for Release\n\n\n\n\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\nClosed\n\n\n\n\n\n\n4\n\n\n25\n\n\n29\n\n\n28\n\n\n17\n\n\n0\n\n\n50\n\n\n3\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\n\n\nReady for Testing\n\n\n\n\nOSG 3.3.25\n\n\nDrop timeout_close.patch in globus-xio\n\n\n\n\n\n\nBoth\n\n\nosg-update-vos: clean yum cache before downloading vo-client\n\n\nUpdate to rsv-perfsonar 1.3.1+\n\n\n\n\n\n\nOSG 3.4.0\n\n\nUpcoming\n\n\nUpdate to HTCondor 8.6.3+ in Upcoming\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussions\n\n\nNone this week  \n\n\nOSG Investigations Team\n\n\nWeek 3 of StashCache focus.  Effort decreasing...  Investigations team is taking a week or 2 of intense effort towards packaging StashCache Authenticated Server.  \n\n\nLast Week\n\n\n\n\nSetup GRACC-ITB instance - Ongoing\n\n\nBetter GRACC Alerting\n\n\nBetter StashCache Cache Alerting\n\n\n\n\nThis Week\n\n\n\n\nContinue to improve StashCache alerting\n\n\nHelp debug HTTP stalls on XrootD\n\n\nImprove StashCache docs even more through feedback from sites. (hopefully we get some)\n\n\nGRACC improvements to some memory leaky daemons\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)", 
            "title": "May 22, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170522/#osg-technology-area-meeting-22-may-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin     Attending:  BrianL, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT", 
            "title": "OSG Technology Area Meeting, 22 May 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170522/#announcements", 
            "text": "Memorial day next Monday, meeting on Tuesday instead.", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170522/#triage-duty", 
            "text": "This week: Derek  Next week: Edgar  6 (0) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170522/#jira", 
            "text": "# of tickets   State      156  7  Open    32  +6  In Progress    4  0  Ready for Testing    0  0  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170522/#release-schedule", 
            "text": "Version  Development Freeze  Package Freeze  Release  Notes      3.3.25  2017-05-30  2017-06-05  2017-06-13  5 week cycle    3.3.26  2017-06-26  2017-07-03  2017-07-11  Independence Day    3.3.27  2017-07-24  2017-07-31  2017-08-08      Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170522/#osg-software-team", 
            "text": "OSG 3.4.0 targeted for June", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170522/#discussions", 
            "text": "Singularity may be updated in EPEL, Derek will test it this week and if successful, this may be able to be dropped from 3.4.", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170522/#support-update", 
            "text": "FIT (BrianL) - Assisting with GRAM -  HTCondor-CE transition  MIT (BrianL) - Investigating multiple GridFTP processes spawning on the real server when starting keepalived on the load balancing node", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170522/#osg-release-team", 
            "text": "Suchandra Thapa is handling the  June 13th  release   Development Freeze next Tuesday     3.3.25   Both   3.4.0   Total   Status      2  18  16  15  7  4  25  7  Open    1  6  9  9  10  4  20  7  In Progress    1  0  3  3  0  0  4  3  Ready for Testing    0  0  0  0  0  0  0  0  Ready for Release    0  1  1  1  0  0  1  0  Closed    4  25  29  28  17  0  50  3  Total       Ready for Testing   OSG 3.3.25  Drop timeout_close.patch in globus-xio    Both  osg-update-vos: clean yum cache before downloading vo-client  Update to rsv-perfsonar 1.3.1+    OSG 3.4.0  Upcoming  Update to HTCondor 8.6.3+ in Upcoming", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170522/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170522/#osg-investigations-team", 
            "text": "Week 3 of StashCache focus.  Effort decreasing...  Investigations team is taking a week or 2 of intense effort towards packaging StashCache Authenticated Server.", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170522/#last-week", 
            "text": "Setup GRACC-ITB instance - Ongoing  Better GRACC Alerting  Better StashCache Cache Alerting", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170522/#this-week", 
            "text": "Continue to improve StashCache alerting  Help debug HTTP stalls on XrootD  Improve StashCache docs even more through feedback from sites. (hopefully we get some)  GRACC improvements to some memory leaky daemons", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170522/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170530/", 
            "text": "OSG Technology Area Meeting, 30 May 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n  \n\n\nAttending:\n BrianB, BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimT, Vaibhav  \n\n\nAnnouncements\n\n\nTriage Duty\n\n\n\n\nThis week: Edgar\n\n\nNext week: Mat\n\n\n9 (+3) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n160\n\n\n+4\n\n\nOpen\n\n\n\n\n\n\n37\n\n\n+5\n\n\nIn Progress\n\n\n\n\n\n\n6\n\n\n+2\n\n\nReady for Testing\n\n\n\n\n\n\n1\n\n\n+1\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n3.3.25 / 3.4.0\n\n\n2017-05-30\n\n\n2017-06-05\n\n\n2017-06-13\n\n\n5 week cycle\n\n\n\n\n\n\n3.3.26 / 3.4.1\n\n\n2017-06-26\n\n\n2017-07-03\n\n\n2017-07-11\n\n\nIndependence Day\n\n\n\n\n\n\n3.3.27 / 3.4.2\n\n\n2017-07-24\n\n\n2017-07-31\n\n\n2017-08-08\n\n\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\n\n\n\n\n\n\nDeveloper\n\n\nTickets not RFT\n\n\n\n\n\n\n\n\n\n\nMat\n\n\n17\n\n\n\n\n\n\nBrian\n\n\n10\n\n\n\n\n\n\nMarian\n\n\n2\n\n\n\n\n\n\nEdgar\n\n\n2\n\n\n\n\n\n\nCarl\n\n\n1\n\n\n\n\n\n\n\n\n\n\nSingularity build\n in EPEL testing looks good, we should be able to drop it from 3.4.0\n\n\nBob Ball starting GUMS -\n LCMAPS VOMS transition this week, Horst will start the edg-mkgridmap -\n LCMAPS VOMS transition next week\n\n\nGlobus Toolkit support ends Jan 2018\n\n\n\n\nDiscussions\n\n\nSupport Update\n\n\n\n\nHosted CE GSISSH meeting today; Suchandra, BrianL, and possibly Jaime Frey will attend\n\n\nSuchandra had questions about running multiple CE services on a single host to limit IPv4 addresses. Will try IPv6 instead and coordinate with Edgar for Factory integration.\n\n\n\n\nSupport Update\n\n\n\n\nMIT (BrianL) - Investigated open UDP ports for GridFTP: turned out to be an old version of GridFTP\n\n\nUFL (BrianL) - Assisted Bockjoo with Slurm timeout issues\n\n\nPurdue (Derek) - Assist in configuration of OSG_WN_TMP for their cluster.\n\n\nALICE (Derek) - Alice VO had issues with submitting usage to GRACC.  So far, seems like it's the probe.\n\n\nFermilab (Suchandra) - disabling SSLv2/SSLv3 for BeStMan, BrianB to speak to them about transitioning them to another storage solution\n\n\n\n\nOSG Release Team\n\n\n\n\nTim Theisen is handling the \nJune 13th\n release\n\n\nDevelopment Freeze today\n\n\nData Release Coming: IGTF Update, VO Package??\n\n\n\n\n\n\n\n\n\n\n3.3.25\n\n\n\n\nBoth\n\n\n\n\n3.4.0\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n0\n\n\n2\n\n\n4\n\n\n12\n\n\n1\n\n\n6\n\n\n5\n\n\n20\n\n\nOpen\n\n\n\n\n\n\n3\n\n\n+2\n\n\n8\n\n\n1\n\n\n16\n\n\n+6\n\n\n27\n\n\n+7\n\n\nIn Progress\n\n\n\n\n\n\n1\n\n\n+0\n\n\n3\n\n\n+0\n\n\n2\n\n\n+2\n\n\n6\n\n\n+2\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n1\n\n\n+1\n\n\n0\n\n\n+0\n\n\n1\n\n\n+1\n\n\nReady for Release\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n1\n\n\n0\n\n\n+0\n\n\n0\n\n\n1\n\n\nClosed\n\n\n\n\n\n\n4\n\n\n+0\n\n\n16\n\n\n13\n\n\n19\n\n\n+2\n\n\n39\n\n\n11\n\n\nTotal\n\n\n\n\n\n\n\n\nReady for Testing\n\n\n\n\nOSG 3.3.25\n\n\nDrop timeout_close.patch in globus-xio\n\n\n\n\n\n\nBoth\n\n\nosg-update-vos: clean yum cache before downloading vo-client\n\n\nChange software.grid.iu.edu to repo.grid.iu.edu in osg-ca-scripts\n\n\n\n\n\n\nOSG 3.4.0\n\n\nDrop conflicts from cvmfs-config-osg\n\n\nDrop bestman2 and globus*run RSV metrics\n\n\n\n\n\n\nUpcoming\n\n\nUpdate to HTCondor 8.6.3+ in Upcoming (labeled for both releases but in Upcoming)\n\n\n\n\n\n\n\n\nReady for Release\n\n\n\n\nOSG 3.3.25\n\n\nBoth\n\n\nUpdate to rsv-perfsonar 1.3.1+\n\n\n\n\n\n\nOSG 3.4.0\n\n\nUpcoming\n\n\n\n\nDiscussions\n\n\nNone this week\n\n\nOSG Investigations Team\n\n\nFocused StashCache effort is over.\n\n\nLast Week\n\n\n\n\nSetup GRACC-ITB instance - Ongoing\n\n\nBetter GRACC Alerting - Now alert on all the things!\n\n\nBetter StashCache Cache Alerting - HTTP Accesses\n\n\nGRACC daemons no longer leak memory like it's their job\n\n\nPython tar file objects keep the metadata for every object added to the tar file, be sure to clear it!\n\n\n\n\n\n\nCVMFS syncing with Stash now works, increased time out.\n\n\nPackaging of GRACC Agent daemons in Docker\n\n\n\n\nThis Week\n\n\n\n\nComplete packaging of GRACC agents in docker.\n\n\nHelp debug HTTP stalls on XrootD\n\n\nImprove StashCache docs even more through feedback from sites. (hopefully we get some)\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)", 
            "title": "May 30, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170530/#osg-technology-area-meeting-30-may-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin     Attending:  BrianB, BrianL, Carl, Derek, Edgar, Marian, Mat, Suchandra, TimT, Vaibhav", 
            "title": "OSG Technology Area Meeting, 30 May 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170530/#announcements", 
            "text": "", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170530/#triage-duty", 
            "text": "This week: Edgar  Next week: Mat  9 (+3) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170530/#jira", 
            "text": "# of tickets   State      160  +4  Open    37  +5  In Progress    6  +2  Ready for Testing    1  +1  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170530/#release-schedule", 
            "text": "Version  Development Freeze  Package Freeze  Release  Notes      3.3.25 / 3.4.0  2017-05-30  2017-06-05  2017-06-13  5 week cycle    3.3.26 / 3.4.1  2017-06-26  2017-07-03  2017-07-11  Independence Day    3.3.27 / 3.4.2  2017-07-24  2017-07-31  2017-08-08      Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170530/#osg-software-team", 
            "text": "Developer  Tickets not RFT      Mat  17    Brian  10    Marian  2    Edgar  2    Carl  1      Singularity build  in EPEL testing looks good, we should be able to drop it from 3.4.0  Bob Ball starting GUMS -  LCMAPS VOMS transition this week, Horst will start the edg-mkgridmap -  LCMAPS VOMS transition next week  Globus Toolkit support ends Jan 2018", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170530/#discussions", 
            "text": "", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170530/#support-update", 
            "text": "Hosted CE GSISSH meeting today; Suchandra, BrianL, and possibly Jaime Frey will attend  Suchandra had questions about running multiple CE services on a single host to limit IPv4 addresses. Will try IPv6 instead and coordinate with Edgar for Factory integration.", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170530/#support-update_1", 
            "text": "MIT (BrianL) - Investigated open UDP ports for GridFTP: turned out to be an old version of GridFTP  UFL (BrianL) - Assisted Bockjoo with Slurm timeout issues  Purdue (Derek) - Assist in configuration of OSG_WN_TMP for their cluster.  ALICE (Derek) - Alice VO had issues with submitting usage to GRACC.  So far, seems like it's the probe.  Fermilab (Suchandra) - disabling SSLv2/SSLv3 for BeStMan, BrianB to speak to them about transitioning them to another storage solution", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170530/#osg-release-team", 
            "text": "Tim Theisen is handling the  June 13th  release  Development Freeze today  Data Release Coming: IGTF Update, VO Package??      3.3.25   Both   3.4.0   Total   Status      0  2  4  12  1  6  5  20  Open    3  +2  8  1  16  +6  27  +7  In Progress    1  +0  3  +0  2  +2  6  +2  Ready for Testing    0  +0  1  +1  0  +0  1  +1  Ready for Release    0  +0  0  1  0  +0  0  1  Closed    4  +0  16  13  19  +2  39  11  Total", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170530/#ready-for-testing", 
            "text": "OSG 3.3.25  Drop timeout_close.patch in globus-xio    Both  osg-update-vos: clean yum cache before downloading vo-client  Change software.grid.iu.edu to repo.grid.iu.edu in osg-ca-scripts    OSG 3.4.0  Drop conflicts from cvmfs-config-osg  Drop bestman2 and globus*run RSV metrics    Upcoming  Update to HTCondor 8.6.3+ in Upcoming (labeled for both releases but in Upcoming)", 
            "title": "Ready for Testing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170530/#ready-for-release", 
            "text": "OSG 3.3.25  Both  Update to rsv-perfsonar 1.3.1+    OSG 3.4.0  Upcoming", 
            "title": "Ready for Release"
        }, 
        {
            "location": "/meetings/2017/TechArea20170530/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170530/#osg-investigations-team", 
            "text": "Focused StashCache effort is over.", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170530/#last-week", 
            "text": "Setup GRACC-ITB instance - Ongoing  Better GRACC Alerting - Now alert on all the things!  Better StashCache Cache Alerting - HTTP Accesses  GRACC daemons no longer leak memory like it's their job  Python tar file objects keep the metadata for every object added to the tar file, be sure to clear it!    CVMFS syncing with Stash now works, increased time out.  Packaging of GRACC Agent daemons in Docker", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170530/#this-week", 
            "text": "Complete packaging of GRACC agents in docker.  Help debug HTTP stalls on XrootD  Improve StashCache docs even more through feedback from sites. (hopefully we get some)", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170530/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170605/", 
            "text": "OSG Technology Area Meeting,  5 June 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n  \n\n\nAttending:\n BrianL, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT, Vaibhav  \n\n\nAnnouncements\n\n\nTriage Duty\n\n\n\n\nThis week: Mat\n\n\nNext week: Suchandra\n\n\n8 (\n1) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n165\n\n\n+5\n\n\nOpen\n\n\n\n\n\n\n14\n\n\n23\n\n\nIn Progress\n\n\n\n\n\n\n33\n\n\n+27\n\n\nReady for Testing\n\n\n\n\n\n\n1\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n3.4.0 / 3.3.25\n\n\n2017-05-30\n\n\n2017-06-05\n\n\n2017-06-13\n\n\n5 week cycle\n\n\n\n\n\n\n3.4.1 / 3.3.26\n\n\n2017-06-26\n\n\n2017-07-03\n\n\n2017-07-11\n\n\nIndependence Day\n\n\n\n\n\n\n3.4.2 / 3.3.27\n\n\n2017-07-24\n\n\n2017-07-31\n\n\n2017-08-08\n\n\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\n\n\nSingularity build\n in EPEL stable so we can exclude it from OSG 3.4.0\n\n\nBob Ball completed GUMS -\n LCMAPS VOMS transition on one host last week, Horst will start the edg-mkgridmap -\n LCMAPS VOMS transition this week\n\n\nGridFTP/XRootD docs need updating for us with the LCMAPS VOMS plugin, any ideas on how to track down TWiki section usage?\n\n\nRHEL VMU tests working for the being even though our RHEL subscription ended. Moate working on a RHEL7 VM exec host using his developer license.\n\n\nHadoop + Ganglia and GridFTP + umask questions sitting in osg-software list\n\n\n\n\nDiscussions\n\n\n\n\nOSG 3.4 singularity policy needs to be clear and set\n\n\nDocs need to specify that the LCMAPS VOMS plugin is the preferred authentication method\n\n\nKyle should be able to help search for TWiki %STARTSECTION% usage\n\n\nMat will follow up with AGLT2 about a bug with the testing version of osg-configure that affects CEs\n\n\nBrianL will find other edg-mkgridmap sites and coordinate with Suchandra to transition hosted CEs to LCMAPS VOMS plugin\n\n\nBrianL will find owners for software mailing list issues\n\n\nTimC and BrianL to discuss the future of the many OSG Software mailing lists\n\n\n\n\nSupport Update\n\n\n\n\nPurdue used OSG_WN_TMP configuration in osg-configure to set it to condor's scratch directory (execute directory?) (Derek)\n\n\nUtah renewed certificate, and is now working (Derek).\n\n\nAPEL update from ALICE usage (Derek).\n\n\n\n\nOSG Release Team\n\n\n\n\nTim Theisen is handling the \nJune 13th\n release\n\n\nPackage Freeze today\n\n\nData Release Coming: IGTF Update, VO Package??\n\n\n\n\n\n\n\n\n\n\n3.3.25\n\n\n\n\nBoth\n\n\n\n\n3.4.0\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n0\n\n\n+0\n\n\n2\n\n\n-2\n\n\n0\n\n\n-1\n\n\n2\n\n\n-3\n\n\nOpen\n\n\n\n\n\n\n0\n\n\n-3\n\n\n2\n\n\n-6\n\n\n1\n\n\n-15\n\n\n3\n\n\n-24\n\n\nIn Progress\n\n\n\n\n\n\n4\n\n\n+3\n\n\n10\n\n\n+7\n\n\n19\n\n\n+17\n\n\n33\n\n\n+27\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n1\n\n\n+0\n\n\n0\n\n\n+0\n\n\n1\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\nClosed\n\n\n\n\n\n\n4\n\n\n+0\n\n\n15\n\n\n-1\n\n\n20\n\n\n+1\n\n\n39\n\n\n+0\n\n\nTotal\n\n\n\n\n\n\n\n\nReady for Testing\n\n\n\n\nOSG 3.3.25\n\n\nDrop timeout_close.patch in globus-xio\n\n\nRelease voms-admin-server-2.7.0-1.22+\n\n\nRelease osg-configure 1.8.1\n\n\nEnable JSP implementation for tomcat webapps\n\n\n\n\n\n\nBoth\n\n\nosg-update-vos: clean yum cache before downloading vo-client\n\n\nChange software.grid.iu.edu to repo.grid.iu.edu in osg-ca-scripts\n\n\nAdd ability to request whole node jobs\n\n\nosg-configure: reject empty allowed_vos in subclusters\n\n\nunnecessary check for OSG_APP and OSG_DATA in osg-configure\n\n\nxrootd-lcmaps-1.3.2-2 build fails for EL6\n\n\nUpdate to XRootD to 4.6.1\n\n\nRelease StashCache metapackage 0.7+\n\n\nosg-configure: Get default allowed_vos with lcmaps voms plugin\n\n\nAdd OSG VOMS mapfile to osg-ce\n\n\n\n\n\n\nOSG 3.4.0\n\n\nDrop conflicts from cvmfs-config-osg\n\n\nUpdate to HTCondor 8.6.3+ in OSG 3.4\n\n\nRelease osg-ce-3.4-1+\n\n\nDrop conflicts from globus-gridftp-osg-extensions\n\n\nRemove requirements for packages dropped in 3.4 in osg-tested-internal\n\n\nosg-configure: Drop glexec support for 3.4\n\n\nRelease osg-configure 2.0.0\n\n\nPrepare lcmaps for 3.4\n\n\nDrop conflicts from HTCondor-CE packaging\n\n\nDrop bestman2 and globus*run RSV metrics\n\n\nosg-configure: Drop managedfork and network config from 2.0.0\n\n\nRemove gridftp from the CE metapackages\n\n\nosg-configure: Drop osg-cleanup options from 10-misc.ini\n\n\nosg-configure: Deprecate GUMS support\n\n\nDrop client tools from osg-ce metapackages\n\n\nosg-configure: Disable GRAM configuration (2.0.0)\n\n\nosg-configure: Drop 'rsv is not installed' warning\n\n\nDrop glexec and java from osg-wn-client\n\n\nosg-configure: Remove \"configure-osg\" alias\n\n\n\n\n\n\nUpcoming\n\n\nNothing\n\n\n\n\n\n\n\n\nReady for Release\n\n\n\n\nOSG 3.3.25\n\n\nBoth\n\n\nUpdate to rsv-perfsonar 1.3.1+\n\n\n\n\n\n\nOSG 3.4.0\n\n\nUpcoming\n\n\n\n\nDiscussions\n\n\nNone this week\n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nSetup GRACC-ITB instance - Ongoing\n\n\nBetter GRACC Alerting\n\n\nBetter StashCache Cache Alerting\n\n\nDocker'ification of GRACC Agents\n\n\nFix unknown projectnames\n\n\n\n\nThis Week\n\n\n\n\nContinue to improve StashCache alerting\n\n\nContinue to dockerify GRACC agents and services.\n\n\nImprove StashCache docs even more through feedback from sites. (hopefully we get some)\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)", 
            "title": "June 6, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170605/#osg-technology-area-meeting-5-june-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin     Attending:  BrianL, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT, Vaibhav", 
            "title": "OSG Technology Area Meeting,  5 June 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170605/#announcements", 
            "text": "", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170605/#triage-duty", 
            "text": "This week: Mat  Next week: Suchandra  8 ( 1) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170605/#jira", 
            "text": "# of tickets   State      165  +5  Open    14  23  In Progress    33  +27  Ready for Testing    1  +0  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170605/#release-schedule", 
            "text": "Version  Development Freeze  Package Freeze  Release  Notes      3.4.0 / 3.3.25  2017-05-30  2017-06-05  2017-06-13  5 week cycle    3.4.1 / 3.3.26  2017-06-26  2017-07-03  2017-07-11  Independence Day    3.4.2 / 3.3.27  2017-07-24  2017-07-31  2017-08-08      Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170605/#osg-software-team", 
            "text": "Singularity build  in EPEL stable so we can exclude it from OSG 3.4.0  Bob Ball completed GUMS -  LCMAPS VOMS transition on one host last week, Horst will start the edg-mkgridmap -  LCMAPS VOMS transition this week  GridFTP/XRootD docs need updating for us with the LCMAPS VOMS plugin, any ideas on how to track down TWiki section usage?  RHEL VMU tests working for the being even though our RHEL subscription ended. Moate working on a RHEL7 VM exec host using his developer license.  Hadoop + Ganglia and GridFTP + umask questions sitting in osg-software list", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170605/#discussions", 
            "text": "OSG 3.4 singularity policy needs to be clear and set  Docs need to specify that the LCMAPS VOMS plugin is the preferred authentication method  Kyle should be able to help search for TWiki %STARTSECTION% usage  Mat will follow up with AGLT2 about a bug with the testing version of osg-configure that affects CEs  BrianL will find other edg-mkgridmap sites and coordinate with Suchandra to transition hosted CEs to LCMAPS VOMS plugin  BrianL will find owners for software mailing list issues  TimC and BrianL to discuss the future of the many OSG Software mailing lists", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170605/#support-update", 
            "text": "Purdue used OSG_WN_TMP configuration in osg-configure to set it to condor's scratch directory (execute directory?) (Derek)  Utah renewed certificate, and is now working (Derek).  APEL update from ALICE usage (Derek).", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170605/#osg-release-team", 
            "text": "Tim Theisen is handling the  June 13th  release  Package Freeze today  Data Release Coming: IGTF Update, VO Package??      3.3.25   Both   3.4.0   Total   Status      0  +0  2  -2  0  -1  2  -3  Open    0  -3  2  -6  1  -15  3  -24  In Progress    4  +3  10  +7  19  +17  33  +27  Ready for Testing    0  +0  1  +0  0  +0  1  +0  Ready for Release    0  +0  0  +0  0  +0  0  +0  Closed    4  +0  15  -1  20  +1  39  +0  Total", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170605/#ready-for-testing", 
            "text": "OSG 3.3.25  Drop timeout_close.patch in globus-xio  Release voms-admin-server-2.7.0-1.22+  Release osg-configure 1.8.1  Enable JSP implementation for tomcat webapps    Both  osg-update-vos: clean yum cache before downloading vo-client  Change software.grid.iu.edu to repo.grid.iu.edu in osg-ca-scripts  Add ability to request whole node jobs  osg-configure: reject empty allowed_vos in subclusters  unnecessary check for OSG_APP and OSG_DATA in osg-configure  xrootd-lcmaps-1.3.2-2 build fails for EL6  Update to XRootD to 4.6.1  Release StashCache metapackage 0.7+  osg-configure: Get default allowed_vos with lcmaps voms plugin  Add OSG VOMS mapfile to osg-ce    OSG 3.4.0  Drop conflicts from cvmfs-config-osg  Update to HTCondor 8.6.3+ in OSG 3.4  Release osg-ce-3.4-1+  Drop conflicts from globus-gridftp-osg-extensions  Remove requirements for packages dropped in 3.4 in osg-tested-internal  osg-configure: Drop glexec support for 3.4  Release osg-configure 2.0.0  Prepare lcmaps for 3.4  Drop conflicts from HTCondor-CE packaging  Drop bestman2 and globus*run RSV metrics  osg-configure: Drop managedfork and network config from 2.0.0  Remove gridftp from the CE metapackages  osg-configure: Drop osg-cleanup options from 10-misc.ini  osg-configure: Deprecate GUMS support  Drop client tools from osg-ce metapackages  osg-configure: Disable GRAM configuration (2.0.0)  osg-configure: Drop 'rsv is not installed' warning  Drop glexec and java from osg-wn-client  osg-configure: Remove \"configure-osg\" alias    Upcoming  Nothing", 
            "title": "Ready for Testing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170605/#ready-for-release", 
            "text": "OSG 3.3.25  Both  Update to rsv-perfsonar 1.3.1+    OSG 3.4.0  Upcoming", 
            "title": "Ready for Release"
        }, 
        {
            "location": "/meetings/2017/TechArea20170605/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170605/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170605/#last-week", 
            "text": "Setup GRACC-ITB instance - Ongoing  Better GRACC Alerting  Better StashCache Cache Alerting  Docker'ification of GRACC Agents  Fix unknown projectnames", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170605/#this-week", 
            "text": "Continue to improve StashCache alerting  Continue to dockerify GRACC agents and services.  Improve StashCache docs even more through feedback from sites. (hopefully we get some)", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170605/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170612/", 
            "text": "OSG Technology Area Meeting, 12 June 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n  \n\n\nAttending:\n BrianL, Carl, Derek, Edgar, Marian, Suchandra, TimT, Vaibhav  \n\n\nAnnouncements\n\n\n\n\nVaibhav at UW Madison this week\n\n\nMat out this week\n\n\n\n\nTriage Duty\n\n\n\n\nThis week: Suchandra\n\n\nNext week: TimT\n\n\n9 (+) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n157\n\n\n8\n\n\nOpen\n\n\n\n\n\n\n18\n\n\n+4\n\n\nIn Progress\n\n\n\n\n\n\n14\n\n\n29\n\n\nReady for Testing\n\n\n\n\n\n\n42\n\n\n+1\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n3.4.0 / 3.3.25\n\n\n2017-05-30\n\n\n2017-06-05\n\n\n2017-06-13\n\n\n5 week cycle\n\n\n\n\n\n\n3.4.1 / 3.3.26\n\n\n2017-06-26\n\n\n2017-07-03\n\n\n2017-07-11\n\n\nIndependence Day\n\n\n\n\n\n\n3.4.2 / 3.3.27\n\n\n2017-07-24\n\n\n2017-07-31\n\n\n2017-08-08\n\n\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\n\n\nLCMAPS VOMS plugin bug\n: all FQANs are considered instead of the first one. Blocker for June release.\n\n\nPre-release tests failed due to install/update failures \n required \nosg-koji regen-repo osg-3.4-el{6,7}-prerelease\n\n\nNeed to start transitioning hosted CEs to LCMAPS VOMS plugin this week, after the release\n\n\nRHEL VMU tests working for the time being even though our RHEL subscription ended. Erin rebuilt RHEL7 exec node, working on building RHEL7 VM image using Aaron's subscription\n\n\nHorst completed the edg-mkgridmap -\n LCMAPS VOMS transition successfully last week\n\n\nxrootd-cmstfc\n (contrib) fails to build for EL7, need help with CMake to place libs in \n/usr/lib64\n rather than \n/usr/lib/\n\n\n\n\nDiscussions\n\n\nNone this week  \n\n\nSupport Update\n\n\n\n\nFIT (BrianL) - Investing CE that doesn't appear to be running\n\n\nTAMU (BrianL) - SAM tests failing intermittently due to transient issues submitting to the CE\n\n\nUFL (BrianL) - Bockjoo found a blahp bug that resulted in incorrect multicore job requests\n\n\n\n\nOSG Release Team\n\n\n\n\nTim Theisen is handling the \nJune 13th\n release\n\n\nSoftware Release tomorrow\n\n\nData Release Coming: IGTF Update, VO Package\n\n\n\n\n\n\n\n\n\n\n3.3.25\n\n\n\n\nBoth\n\n\n\n\n3.4.0\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n-2\n\n\n0\n\n\n+0\n\n\n0\n\n\n-2\n\n\nOpen\n\n\n\n\n\n\n0\n\n\n+0\n\n\n1\n\n\n-1\n\n\n0\n\n\n-1\n\n\n1\n\n\n-2\n\n\nIn Progress\n\n\n\n\n\n\n0\n\n\n-4\n\n\n0\n\n\n-10\n\n\n0\n\n\n-19\n\n\n0\n\n\n-33\n\n\nReady for Testing\n\n\n\n\n\n\n4\n\n\n+4\n\n\n15\n\n\n+14\n\n\n22\n\n\n+22\n\n\n41\n\n\n+40\n\n\nReady for Release\n\n\n\n\n\n\n0\n\n\n+0\n\n\n1\n\n\n+1\n\n\n0\n\n\n+0\n\n\n1\n\n\n+1\n\n\nClosed\n\n\n\n\n\n\n4\n\n\n+0\n\n\n17\n\n\n+2\n\n\n22\n\n\n+2\n\n\n43\n\n\n+4\n\n\nTotal\n\n\n\n\n\n\n\n\nLate breaking update\n\n\n\n\nlcmaps-plugins-voms maps all FQANs\n\n\n\n\nReady for Release\n\n\n\n\nOSG 3.3.25\n\n\nDrop timeout_close.patch in globus-xio\n\n\nRelease voms-admin-server-2.7.0-1.22+\n\n\nRelease osg-configure 1.8.1\n\n\nEnable JSP implementation for tomcat webapps\n\n\n\n\n\n\nBoth\n\n\nosg-update-vos: clean yum cache before downloading vo-client\n\n\nChange software.grid.iu.edu to repo.grid.iu.edu in osg-ca-scripts\n\n\nAdd ability to request whole node jobs\n\n\nosg-configure: reject empty allowed_vos in subclusters\n\n\nunnecessary check for OSG_APP and OSG_DATA in osg-configure\n\n\nRelease xrootd-lcmaps-1.3.2-2 +\n\n\nUpdate to XRootD to 4.6.1\n\n\nRelease StashCache metapackage 0.7+\n\n\nosg-configure: Get default allowed_vos with lcmaps voms plugin\n\n\nAdd OSG VOMS mapfile to osg-ce\n\n\nlcmaps-plugins-voms maps all FQANs\n\n\nDocument configuration of lcmaps-voms-plugin\n\n\nRelease Glideinwms v3.2.19+\n\n\nRelease osg-build 1.10.0\n\n\nosg-build: drop vdt-build\n\n\nosg-build: drop ~/.osg-build.ini\n\n\nAdd vo-client-lcmaps-voms dependency to osg-gridftp\n\n\n\n\n\n\nOSG 3.4.0\n\n\nDrop conflicts from cvmfs-config-osg\n\n\nUpdate to HTCondor 8.6.3+ in OSG 3.4\n\n\nRelease osg-ce-3.4-1+\n\n\nDrop conflicts from globus-gridftp-osg-extensions\n\n\nRemove requirements for packages dropped in 3.4 in osg-tested-internal\n\n\nosg-configure: Drop glexec support for 3.4\n\n\nRelease osg-configure 2.0.0\n\n\nPrepare lcmaps for 3.4\n\n\nDrop conflicts from HTCondor-CE packaging\n\n\nDrop bestman2 and globus*run RSV metrics\n\n\nosg-configure: Drop managedfork and network config from 2.0.0\n\n\nRemove gridftp from the CE metapackages\n\n\nosg-configure: Drop osg-cleanup options from 10-misc.ini\n\n\nosg-configure: Deprecate GUMS support\n\n\nDrop client tools from osg-ce metapackages\n\n\nosg-configure: Disable GRAM configuration (2.0.0)\n\n\nosg-configure: Drop 'rsv is not installed' warning\n\n\nDrop glexec and java from osg-wn-client\n\n\nosg-configure: Remove \"configure-osg\" alias\n\n\nDrop edg-mkgridmap from OSG 3.4\n\n\nDrop bestman2 from OSG 3.4\n\n\nDrop GUMS from 3.4\n\n\n\n\n\n\n\n\nDiscussions\n\n\nNone this week\n\n\nOSG Investigations Team\n\n\nLots of vacation from Investigations team this week.  Not much to update.\n\n\nLast Week\n\n\n\n\nSetup GRACC-ITB instance - Ongoing\n\n\nBetter StashCache Cache Alerting\n\n\nDocker'ification of GRACC Agents\n\n\n\n\nThis Week\n\n\n\n\nContinue to dockerify GRACC agents and services.  Next on the list is gracc-summary.\n\n\nImprove StashCache docs even more through feedback from sites. (hopefully we get some)\n\n\nWrite StashCache article for user support team.\n\n\nSome BLAHP work.\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)", 
            "title": "June 12, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170612/#osg-technology-area-meeting-12-june-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin     Attending:  BrianL, Carl, Derek, Edgar, Marian, Suchandra, TimT, Vaibhav", 
            "title": "OSG Technology Area Meeting, 12 June 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170612/#announcements", 
            "text": "Vaibhav at UW Madison this week  Mat out this week", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170612/#triage-duty", 
            "text": "This week: Suchandra  Next week: TimT  9 (+) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170612/#jira", 
            "text": "# of tickets   State      157  8  Open    18  +4  In Progress    14  29  Ready for Testing    42  +1  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170612/#release-schedule", 
            "text": "Version  Development Freeze  Package Freeze  Release  Notes      3.4.0 / 3.3.25  2017-05-30  2017-06-05  2017-06-13  5 week cycle    3.4.1 / 3.3.26  2017-06-26  2017-07-03  2017-07-11  Independence Day    3.4.2 / 3.3.27  2017-07-24  2017-07-31  2017-08-08      Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170612/#osg-software-team", 
            "text": "LCMAPS VOMS plugin bug : all FQANs are considered instead of the first one. Blocker for June release.  Pre-release tests failed due to install/update failures   required  osg-koji regen-repo osg-3.4-el{6,7}-prerelease  Need to start transitioning hosted CEs to LCMAPS VOMS plugin this week, after the release  RHEL VMU tests working for the time being even though our RHEL subscription ended. Erin rebuilt RHEL7 exec node, working on building RHEL7 VM image using Aaron's subscription  Horst completed the edg-mkgridmap -  LCMAPS VOMS transition successfully last week  xrootd-cmstfc  (contrib) fails to build for EL7, need help with CMake to place libs in  /usr/lib64  rather than  /usr/lib/", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170612/#discussions", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170612/#support-update", 
            "text": "FIT (BrianL) - Investing CE that doesn't appear to be running  TAMU (BrianL) - SAM tests failing intermittently due to transient issues submitting to the CE  UFL (BrianL) - Bockjoo found a blahp bug that resulted in incorrect multicore job requests", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170612/#osg-release-team", 
            "text": "Tim Theisen is handling the  June 13th  release  Software Release tomorrow  Data Release Coming: IGTF Update, VO Package      3.3.25   Both   3.4.0   Total   Status      0  +0  0  -2  0  +0  0  -2  Open    0  +0  1  -1  0  -1  1  -2  In Progress    0  -4  0  -10  0  -19  0  -33  Ready for Testing    4  +4  15  +14  22  +22  41  +40  Ready for Release    0  +0  1  +1  0  +0  1  +1  Closed    4  +0  17  +2  22  +2  43  +4  Total", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170612/#late-breaking-update", 
            "text": "lcmaps-plugins-voms maps all FQANs", 
            "title": "Late breaking update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170612/#ready-for-release", 
            "text": "OSG 3.3.25  Drop timeout_close.patch in globus-xio  Release voms-admin-server-2.7.0-1.22+  Release osg-configure 1.8.1  Enable JSP implementation for tomcat webapps    Both  osg-update-vos: clean yum cache before downloading vo-client  Change software.grid.iu.edu to repo.grid.iu.edu in osg-ca-scripts  Add ability to request whole node jobs  osg-configure: reject empty allowed_vos in subclusters  unnecessary check for OSG_APP and OSG_DATA in osg-configure  Release xrootd-lcmaps-1.3.2-2 +  Update to XRootD to 4.6.1  Release StashCache metapackage 0.7+  osg-configure: Get default allowed_vos with lcmaps voms plugin  Add OSG VOMS mapfile to osg-ce  lcmaps-plugins-voms maps all FQANs  Document configuration of lcmaps-voms-plugin  Release Glideinwms v3.2.19+  Release osg-build 1.10.0  osg-build: drop vdt-build  osg-build: drop ~/.osg-build.ini  Add vo-client-lcmaps-voms dependency to osg-gridftp    OSG 3.4.0  Drop conflicts from cvmfs-config-osg  Update to HTCondor 8.6.3+ in OSG 3.4  Release osg-ce-3.4-1+  Drop conflicts from globus-gridftp-osg-extensions  Remove requirements for packages dropped in 3.4 in osg-tested-internal  osg-configure: Drop glexec support for 3.4  Release osg-configure 2.0.0  Prepare lcmaps for 3.4  Drop conflicts from HTCondor-CE packaging  Drop bestman2 and globus*run RSV metrics  osg-configure: Drop managedfork and network config from 2.0.0  Remove gridftp from the CE metapackages  osg-configure: Drop osg-cleanup options from 10-misc.ini  osg-configure: Deprecate GUMS support  Drop client tools from osg-ce metapackages  osg-configure: Disable GRAM configuration (2.0.0)  osg-configure: Drop 'rsv is not installed' warning  Drop glexec and java from osg-wn-client  osg-configure: Remove \"configure-osg\" alias  Drop edg-mkgridmap from OSG 3.4  Drop bestman2 from OSG 3.4  Drop GUMS from 3.4", 
            "title": "Ready for Release"
        }, 
        {
            "location": "/meetings/2017/TechArea20170612/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170612/#osg-investigations-team", 
            "text": "Lots of vacation from Investigations team this week.  Not much to update.", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170612/#last-week", 
            "text": "Setup GRACC-ITB instance - Ongoing  Better StashCache Cache Alerting  Docker'ification of GRACC Agents", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170612/#this-week", 
            "text": "Continue to dockerify GRACC agents and services.  Next on the list is gracc-summary.  Improve StashCache docs even more through feedback from sites. (hopefully we get some)  Write StashCache article for user support team.  Some BLAHP work.", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170612/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170619/", 
            "text": "OSG Technology Area Meeting, 19 June 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n  \n\n\nAttending:\n   \n\n\nAnnouncements\n\n\nTriage Duty\n\n\n\n\nThis week: TimT\n\n\nNext week: BrianL\n\n\n8 (\n1) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n156\n\n\n8\n\n\nOpen\n\n\n\n\n\n\n13\n\n\n5\n\n\nIn Progress\n\n\n\n\n\n\n3\n\n\n1\n\n\nReady for Testing\n\n\n\n\n\n\n1\n\n\n41\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n3.4.1 / 3.3.26\n\n\n2017-06-26\n\n\n2017-07-03\n\n\n2017-07-11\n\n\nIndependence Day\n\n\n\n\n\n\n3.4.2 / 3.3.27\n\n\n2017-07-24\n\n\n2017-07-31\n\n\n2017-08-08\n\n\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\n\n\nDev freeze next Monday\n\n\nStarted transitioning hosted CEs to LCMAPS VOMS plugin last week\n\n\nRHEL VMU tests working for the time being even though our RHEL subscription ended. Erin rebuilt RHEL7 exec node, working on building RHEL7 VM image using Aaron's subscription\n\n\nxrootd-cmstfc\n (contrib) fails to build for EL7, need help with CMake to place libs in \n/usr/lib64\n rather than \n/usr/lib/\n\n\n\n\nDiscussions\n\n\nNone this week  \n\n\nSupport Update\n\n\n\n\nCTSC (BrianL) - Provided rough size estimate of OSG CE packaging\n\n\nFIT (BrianL) - Assisting transition from GRAM to HTCondor-CE\n\n\nTAMU (BrianL) - SAM tests are still getting held with \"Job not found\"\n\n\nSyracuse (Derek) - Implemented GPU submission on their HTCondor-CE.  Fermilab is creating an entry on the ITB GlideinWMS factories to utilize and test this new GPU capability.  OSG will follow suite once singularity configuration is developed.\n\n\nUCSD (Derek) - Gave some assistance to job route configuration for GPUs.\n\n\nNebraska (Derek) - Assisted in debugging missing hours in WLCG report.  Turns out that removed jobs were not being accounted correctly.  Also, Multi-CPU jobs and whole node jobs where not being accounted.\n\n\n\n\nOSG Release Team\n\n\n\n\nTim Theisen is handling the \nJuly 11th\n release\n\n\nData Release Coming: IGTF Update, VO Package\n\n\n\n\n\n\n\n\n\n\n3.3.26\n\n\n\n\nBoth\n\n\n\n\n3.4.1\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n1\n\n\n+1\n\n\n12\n\n\n+12\n\n\n5\n\n\n+5\n\n\n18\n\n\n+18\n\n\nOpen\n\n\n\n\n\n\n1\n\n\n+1\n\n\n2\n\n\n+2\n\n\n1\n\n\n+1\n\n\n4\n\n\n+4\n\n\nIn Progress\n\n\n\n\n\n\n0\n\n\n+0\n\n\n3\n\n\n+3\n\n\n0\n\n\n+0\n\n\n3\n\n\n+3\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\nClosed\n\n\n\n\n\n\n2\n\n\n+2\n\n\n17\n\n\n+0\n\n\n6\n\n\n+6\n\n\n25\n\n\n+25\n\n\nTotal\n\n\n\n\n\n\n\n\nReady for Testing\n\n\n\n\nFix for CVMFS client failing to mount when very large groups exist\n\n\nAdded ability to include arbitrary ClassAd attributes in Gratia records\n\n\nAdded osg-configure-misc dependency to osg-gridftp\n\n\n\n\nDiscussions\n\n\nNone this week  \n\n\nOSG Investigations Team\n\n\nLots of vacation from Investigations team this week.  Not much to update.  \n\n\nLast Week\n\n\n\n\nGRACC debugging of Underreported sites\n\n\nDebug Nova CVMFS repo, will need to redo repo with chunking.\n\n\nBlog post on StashCache: https://djw8605.github.io/2017/06/14/stashcache/ and on Planet OSG http://blogs.grid.iu.edu/\n\n\n\n\nThis Week\n\n\n\n\nReprocess underreported sites and upload new APEL report to WLCG\n\n\nDocker'ification of GRACC Agents\n\n\nPackaging of CVMFS-Sync and configurations.\n\n\nInvestigate backups of GRACC peripheral services\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)", 
            "title": "June 19, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170619/#osg-technology-area-meeting-19-june-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin     Attending:", 
            "title": "OSG Technology Area Meeting, 19 June 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170619/#announcements", 
            "text": "", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170619/#triage-duty", 
            "text": "This week: TimT  Next week: BrianL  8 ( 1) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170619/#jira", 
            "text": "# of tickets   State      156  8  Open    13  5  In Progress    3  1  Ready for Testing    1  41  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170619/#release-schedule", 
            "text": "Version  Development Freeze  Package Freeze  Release  Notes      3.4.1 / 3.3.26  2017-06-26  2017-07-03  2017-07-11  Independence Day    3.4.2 / 3.3.27  2017-07-24  2017-07-31  2017-08-08      Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170619/#osg-software-team", 
            "text": "Dev freeze next Monday  Started transitioning hosted CEs to LCMAPS VOMS plugin last week  RHEL VMU tests working for the time being even though our RHEL subscription ended. Erin rebuilt RHEL7 exec node, working on building RHEL7 VM image using Aaron's subscription  xrootd-cmstfc  (contrib) fails to build for EL7, need help with CMake to place libs in  /usr/lib64  rather than  /usr/lib/", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170619/#discussions", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170619/#support-update", 
            "text": "CTSC (BrianL) - Provided rough size estimate of OSG CE packaging  FIT (BrianL) - Assisting transition from GRAM to HTCondor-CE  TAMU (BrianL) - SAM tests are still getting held with \"Job not found\"  Syracuse (Derek) - Implemented GPU submission on their HTCondor-CE.  Fermilab is creating an entry on the ITB GlideinWMS factories to utilize and test this new GPU capability.  OSG will follow suite once singularity configuration is developed.  UCSD (Derek) - Gave some assistance to job route configuration for GPUs.  Nebraska (Derek) - Assisted in debugging missing hours in WLCG report.  Turns out that removed jobs were not being accounted correctly.  Also, Multi-CPU jobs and whole node jobs where not being accounted.", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170619/#osg-release-team", 
            "text": "Tim Theisen is handling the  July 11th  release  Data Release Coming: IGTF Update, VO Package      3.3.26   Both   3.4.1   Total   Status      1  +1  12  +12  5  +5  18  +18  Open    1  +1  2  +2  1  +1  4  +4  In Progress    0  +0  3  +3  0  +0  3  +3  Ready for Testing    0  +0  0  +0  0  +0  0  +0  Ready for Release    0  +0  0  +0  0  +0  0  +0  Closed    2  +2  17  +0  6  +6  25  +25  Total", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170619/#ready-for-testing", 
            "text": "Fix for CVMFS client failing to mount when very large groups exist  Added ability to include arbitrary ClassAd attributes in Gratia records  Added osg-configure-misc dependency to osg-gridftp", 
            "title": "Ready for Testing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170619/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170619/#osg-investigations-team", 
            "text": "Lots of vacation from Investigations team this week.  Not much to update.", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170619/#last-week", 
            "text": "GRACC debugging of Underreported sites  Debug Nova CVMFS repo, will need to redo repo with chunking.  Blog post on StashCache: https://djw8605.github.io/2017/06/14/stashcache/ and on Planet OSG http://blogs.grid.iu.edu/", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170619/#this-week", 
            "text": "Reprocess underreported sites and upload new APEL report to WLCG  Docker'ification of GRACC Agents  Packaging of CVMFS-Sync and configurations.  Investigate backups of GRACC peripheral services", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170619/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170626/", 
            "text": "OSG Technology Area Meeting, 26 June 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n  \n\n\nAttending:\n BrianL, Carl, Derek, Marian, Mat, Suchandra, TimC, Vaibhav  \n\n\nAnnouncements\n\n\n\n\nTimT, Edgar out until next week\n\n\nDerek will be 50% OSG starting July 1, down from 75%\n\n\n\n\nTriage Duty\n\n\n\n\nThis week: BrianL\n\n\nNext week: Carl\n\n\n8 (+0) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n149\n\n\n7\n\n\nOpen\n\n\n\n\n\n\n18\n\n\n+5\n\n\nIn Progress\n\n\n\n\n\n\n6\n\n\n+3\n\n\nReady for Testing\n\n\n\n\n\n\n1\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n3.4.1 / 3.3.26\n\n\n2017-06-26\n\n\n2017-07-03\n\n\n2017-07-11\n\n\nIndependence Day\n\n\n\n\n\n\n3.4.2 / 3.3.27\n\n\n2017-07-24\n\n\n2017-07-31\n\n\n2017-08-08\n\n\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\n\n\n\n\nDev freeze today:  \n\n\n\n\n\n\n\n\nOwner\n\n\n# open tickets\n\n\n\n\n\n\n\n\n\n\nBrianL\n\n\n3\n\n\n\n\n\n\nCarl\n\n\n2\n\n\n\n\n\n\nMat\n\n\n2\n\n\n\n\n\n\n\n\n\n\n\n\nSuchandra: How's the LCMAPS VOMS transition going?\n\n\n\n\nHTCondor-CE whole node accounting + memory request issues\n\n\nVMU tests: Erin investigating network issues with new RHEL VMs using the dev subscription. Dakota was brought up to speed on image generation/troubleshooting.\n\n\nITB progress: New pool configs sprayed out for separated CM and an additional CE for testing pre-release\n\n\nInternal doc migration started and lives \nhere\n (formerly https://github.com/brianhlin/technology/tree/internal_migration)\n\n\nxrootd-cmstfc\n (contrib) fails to build for EL7, need help with CMake to place libs in \n/usr/lib64\n rather than \n/usr/lib/\n\n\n\n\nDiscussions\n\n\n\n\nRSV JIRA ticket incoming to fix querying \ncondor_q\n output\n\n\nIncoming Gratia probe changes for whole node accounting issues by Derek, Carl to review\n\n\nHosted CE LCMAPS VOMS transition complete\n\n\n\n\nSupport Update\n\n\n\n\nFIT (BrianL) - Jobs held, passed on troubleshooting doc  \n\n\nTAMU (BrianL) - SAM tests are better with updates to their scheduler config. Jobs submitted to their backend condor are being held with \"job not found\"\n\n\nUNL (Derek) - Reprocessed Accounting records for whole node jobs.  Pull request for HTCondor-CE (https://github.com/opensciencegrid/htcondor-ce/pull/151) and Gratia-Probes (https://github.com/opensciencegrid/gratia-probe/pull/18)\n\n\nSyracuse \n UCSD (Derek) - GPU nodes are starting up.  Small amount of support last week, but this week I expect production use to start, so possibly some user support.\n\n\n\n\nOSG Release Team\n\n\n\n\nTim Theisen is handling the \nJuly 11th\n release\n\n\n\n\n\n\n\n\n\n\n3.3.26\n\n\n\n\nBoth\n\n\n\n\n3.4.1\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n0\n\n\n\n\n1\n\n\n1\n\n\n0\n\n\n5\n\n\n1\n\n\n17\n\n\nOpen\n\n\n\n\n\n\n0\n\n\n1\n\n\n6\n\n\n+4\n\n\n0\n\n\n1\n\n\n6\n\n\n+2\n\n\nIn Progress\n\n\n\n\n\n\n1\n\n\n+1\n\n\n6\n\n\n+3\n\n\n3\n\n\n+3\n\n\n10\n\n\n+7\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\nClosed\n\n\n\n\n\n\n1\n\n\n1\n\n\n13\n\n\n4\n\n\n3\n\n\n3\n\n\n17\n\n\n8\n\n\nTotal\n\n\n\n\n\n\n\n\nReady for Testing\n\n\n\n\nCondor 8.6.4 and 8.7.2 in 3.4 and upcoming, respectively. Including af ix for HTCondor-CE-Bosco without certs\n\n\nblahp fix for multicore requests for SLURM batch systems\n\n\nNew package, \ngridftp-dsi-posix\n, to replace \nxrootd-dsi\n\n\nFix for GridFTP startup to use the correct plugin configuration\n\n\nFix for CVMFS client failing to mount when very large groups exist\n\n\nAdded ability to include arbitrary ClassAd attributes in Gratia records\n\n\nAdded osg-configure-misc dependency to osg-gridftp\n\n\nFix for HDFS NameNode infinite loop\n\n\n\n\nDiscussions\n\n\nNone this week  \n\n\nOSG Investigations Team\n\n\nLots of vacation from Investigations team this week.  Not much to update.  \n\n\nLast Week\n\n\n\n\nReprocess underreported sites and upload new APEL report to WLCG\n\n\nDebug Nova CVMFS repo, will need to redo repo with chunking.\n\n\nPackaging of CVMFS-Sync and configurations. https://github.com/bbockelm/cvmfs-sync/pull/1\n\n\nInvestigate backups of GRACC peripheral services\n\n\n\n\nThis Week\n\n\n\n\nWill need to redo nova repo with chunking.\n\n\nDocker'ification of GRACC Agents\n\n\nStart backups of grafana configurations (dashboards and datasources)\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)", 
            "title": "June 26, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170626/#osg-technology-area-meeting-26-june-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin     Attending:  BrianL, Carl, Derek, Marian, Mat, Suchandra, TimC, Vaibhav", 
            "title": "OSG Technology Area Meeting, 26 June 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170626/#announcements", 
            "text": "TimT, Edgar out until next week  Derek will be 50% OSG starting July 1, down from 75%", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170626/#triage-duty", 
            "text": "This week: BrianL  Next week: Carl  8 (+0) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170626/#jira", 
            "text": "# of tickets   State      149  7  Open    18  +5  In Progress    6  +3  Ready for Testing    1  +0  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170626/#release-schedule", 
            "text": "Version  Development Freeze  Package Freeze  Release  Notes      3.4.1 / 3.3.26  2017-06-26  2017-07-03  2017-07-11  Independence Day    3.4.2 / 3.3.27  2017-07-24  2017-07-31  2017-08-08      Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170626/#osg-software-team", 
            "text": "Dev freeze today:       Owner  # open tickets      BrianL  3    Carl  2    Mat  2       Suchandra: How's the LCMAPS VOMS transition going?   HTCondor-CE whole node accounting + memory request issues  VMU tests: Erin investigating network issues with new RHEL VMs using the dev subscription. Dakota was brought up to speed on image generation/troubleshooting.  ITB progress: New pool configs sprayed out for separated CM and an additional CE for testing pre-release  Internal doc migration started and lives  here  (formerly https://github.com/brianhlin/technology/tree/internal_migration)  xrootd-cmstfc  (contrib) fails to build for EL7, need help with CMake to place libs in  /usr/lib64  rather than  /usr/lib/", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170626/#discussions", 
            "text": "RSV JIRA ticket incoming to fix querying  condor_q  output  Incoming Gratia probe changes for whole node accounting issues by Derek, Carl to review  Hosted CE LCMAPS VOMS transition complete", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170626/#support-update", 
            "text": "FIT (BrianL) - Jobs held, passed on troubleshooting doc    TAMU (BrianL) - SAM tests are better with updates to their scheduler config. Jobs submitted to their backend condor are being held with \"job not found\"  UNL (Derek) - Reprocessed Accounting records for whole node jobs.  Pull request for HTCondor-CE (https://github.com/opensciencegrid/htcondor-ce/pull/151) and Gratia-Probes (https://github.com/opensciencegrid/gratia-probe/pull/18)  Syracuse   UCSD (Derek) - GPU nodes are starting up.  Small amount of support last week, but this week I expect production use to start, so possibly some user support.", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170626/#osg-release-team", 
            "text": "Tim Theisen is handling the  July 11th  release      3.3.26   Both   3.4.1   Total   Status      0   1  1  0  5  1  17  Open    0  1  6  +4  0  1  6  +2  In Progress    1  +1  6  +3  3  +3  10  +7  Ready for Testing    0  +0  0  +0  0  +0  0  +0  Ready for Release    0  +0  0  +0  0  +0  0  +0  Closed    1  1  13  4  3  3  17  8  Total", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170626/#ready-for-testing", 
            "text": "Condor 8.6.4 and 8.7.2 in 3.4 and upcoming, respectively. Including af ix for HTCondor-CE-Bosco without certs  blahp fix for multicore requests for SLURM batch systems  New package,  gridftp-dsi-posix , to replace  xrootd-dsi  Fix for GridFTP startup to use the correct plugin configuration  Fix for CVMFS client failing to mount when very large groups exist  Added ability to include arbitrary ClassAd attributes in Gratia records  Added osg-configure-misc dependency to osg-gridftp  Fix for HDFS NameNode infinite loop", 
            "title": "Ready for Testing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170626/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170626/#osg-investigations-team", 
            "text": "Lots of vacation from Investigations team this week.  Not much to update.", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170626/#last-week", 
            "text": "Reprocess underreported sites and upload new APEL report to WLCG  Debug Nova CVMFS repo, will need to redo repo with chunking.  Packaging of CVMFS-Sync and configurations. https://github.com/bbockelm/cvmfs-sync/pull/1  Investigate backups of GRACC peripheral services", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170626/#this-week", 
            "text": "Will need to redo nova repo with chunking.  Docker'ification of GRACC Agents  Start backups of grafana configurations (dashboards and datasources)", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170626/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170703/", 
            "text": "OSG Technology Area Meeting, 3 July 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n\n\nAttending:\n BrianB, Carl, Derek, Mat, Marian, Suchandra, Vaibhav\n\n\nAnnouncements\n\n\nNone\n\n\nTriage Duty\n\n\n\n\nThis week: Carl\n\n\nNext week: Derek\n\n\n6 (-2) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n149\n\n\n(+0)\n\n\nOpen\n\n\n\n\n\n\n11\n\n\n(-7)\n\n\nIn Progress\n\n\n\n\n\n\n23\n\n\n(+17)\n\n\nReady for Testing\n\n\n\n\n\n\n2\n\n\n(+1)\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n3.4.1 / 3.3.26\n\n\n2017-06-26\n\n\n2017-07-03\n\n\n2017-07-11\n\n\nIndependence Day\n\n\n\n\n\n\n3.4.2 / 3.3.27\n\n\n2017-07-24\n\n\n2017-07-31\n\n\n2017-08-08\n\n\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.\n\n\nOSG Software Team\n\n\n\n\nPackage freeze today\n\n\nAll tickets ready for testing, one ready for release\n\n\n\n\nSupport Update\n\n\n\n\nUtah (Derek) - working with Utah, still debugging. Proxy is trying to be read by the pilot and the proxy doesn't exist in the expected directory. Only happening on one cluster\n\n\nIIT, Clemson (BrianL) - Stacktraces when authenticating some proxies\n\n\n\n\nOSG Release Team\n\n\n\n\n\n\n\n\n3.3.26\n\n\n\n\nBoth\n\n\n\n\n3.4.1\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n1\n\n\n0\n\n\n\n\n0\n\n\n1\n\n\nOpen\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n6\n\n\nIn Progress\n\n\n\n\n\n\n2\n\n\n+1\n\n\n18\n\n\n+12\n\n\n3\n\n\n+0\n\n\n23\n\n\n+13\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n1\n\n\n+0\n\n\n0\n\n\n+0\n\n\n1\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\nClosed\n\n\n\n\n\n\n2\n\n\n+1\n\n\n19\n\n\n+6\n\n\n3\n\n\n+0\n\n\n24\n\n\n+7\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\nShort testing week due to holiday\n\n\n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nOSG-Connect wanted some project renames, done, but waiting on verification.\n\n\n\n\nThis Week\n\n\n\n\nGRACC host cert expired, but that only affects backups so everything is still online\n\n\nWCLG wants some info on missing accounting, but they are slow on finalizing their own accounting.  But it looks like it is now finished.\n\n\nDerek will be at PEARC next week and will be presenting\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)", 
            "title": "July 3, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170703/#osg-technology-area-meeting-3-july-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin  Attending:  BrianB, Carl, Derek, Mat, Marian, Suchandra, Vaibhav", 
            "title": "OSG Technology Area Meeting, 3 July 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170703/#announcements", 
            "text": "None", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170703/#triage-duty", 
            "text": "This week: Carl  Next week: Derek  6 (-2) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170703/#jira", 
            "text": "# of tickets   State      149  (+0)  Open    11  (-7)  In Progress    23  (+17)  Ready for Testing    2  (+1)  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170703/#release-schedule", 
            "text": "Version  Development Freeze  Package Freeze  Release  Notes      3.4.1 / 3.3.26  2017-06-26  2017-07-03  2017-07-11  Independence Day    3.4.2 / 3.3.27  2017-07-24  2017-07-31  2017-08-08      Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170703/#osg-software-team", 
            "text": "Package freeze today  All tickets ready for testing, one ready for release", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170703/#support-update", 
            "text": "Utah (Derek) - working with Utah, still debugging. Proxy is trying to be read by the pilot and the proxy doesn't exist in the expected directory. Only happening on one cluster  IIT, Clemson (BrianL) - Stacktraces when authenticating some proxies", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170703/#osg-release-team", 
            "text": "3.3.26   Both   3.4.1   Total   Status      0  +0  0  1  0   0  1  Open    0  +0  0  +0  0  +0  0  6  In Progress    2  +1  18  +12  3  +0  23  +13  Ready for Testing    0  +0  1  +0  0  +0  1  +0  Ready for Release    0  +0  0  +0  0  +0  0  +0  Closed    2  +1  19  +6  3  +0  24  +7  Total      Short testing week due to holiday", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170703/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170703/#last-week", 
            "text": "OSG-Connect wanted some project renames, done, but waiting on verification.", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170703/#this-week", 
            "text": "GRACC host cert expired, but that only affects backups so everything is still online  WCLG wants some info on missing accounting, but they are slow on finalizing their own accounting.  But it looks like it is now finished.  Derek will be at PEARC next week and will be presenting", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170703/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170710/", 
            "text": "OSG Technology Area Meeting, 10 July 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n  \n\n\nAttending:\n BrianB, BrianL, Carl, Edgar, Marian, Mat, TimT, Vaibhav\n\n\nAnnouncements\n\n\nOSG User School next week  \n\n\nTriage Duty\n\n\n\n\nThis week: Derek\n\n\nNext week: Edgar\n\n\n5 (\n1) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n156\n\n\n+7\n\n\nOpen\n\n\n\n\n\n\n11\n\n\n+0\n\n\nIn Progress\n\n\n\n\n\n\n2\n\n\n21\n\n\nReady for Testing\n\n\n\n\n\n\n25\n\n\n+23\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n3.4.1 / 3.3.26\n\n\n2017-06-26\n\n\n2017-07-03\n\n\n2017-07-11\n\n\nIndependence Day\n\n\n\n\n\n\n3.4.2 / 3.3.27\n\n\n2017-07-24\n\n\n2017-07-31\n\n\n2017-08-08\n\n\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\n\n\nTwo weeks until August development freeze\n\n\nMajor LCMAPS VOMS plugin issues plaguing multiple sites, causing extensive downtime\n\n\n\n\nDiscussions\n\n\n\n\nThe root cause of the LCMAPS VOMS segfaults appear to be due to memory issues in the proxy verification module. BrianB hopes to have a fix by the end of the day.\n\n\n\n\nSupport Update\n\n\n\n\nClemson, IIT, UConn (BrianL) - LCMAPS VOMS plugin issue above\n\n\n\n\nOSG Release Team\n\n\n\n\nTim Theisen is handling the \nJuly 11th\n release\n\n\nRelease tomorrow\n\n\n\n\n\n\n\n\n\n\n3.3.26\n\n\n\n\nBoth\n\n\n\n\n3.4.1\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\nOpen\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\nIn Progress\n\n\n\n\n\n\n0\n\n\n-2\n\n\n18\n\n\n-18\n\n\n0\n\n\n-3\n\n\n0\n\n\n-23\n\n\nReady for Testing\n\n\n\n\n\n\n2\n\n\n+2\n\n\n19\n\n\n+18\n\n\n4\n\n\n+4\n\n\n25\n\n\n+24\n\n\nReady for Release\n\n\n\n\n\n\n2\n\n\n+0\n\n\n19\n\n\n+0\n\n\n4\n\n\n+1\n\n\n25\n\n\n+1\n\n\nTotal\n\n\n\n\n\n\n\n\nDiscussions\n\n\n\n\nOperations will be adopting a more flexible release schedule, at least for changes transparent to the user\n\n\nCert update incoming; Edgar will help test it while Suchandra is at PEARC. In the future, it may make more sense to test cert packages in an automated fashion \\minus; perhaps with \nrpmdiff\n.\n\n\n\n\nOSG Investigations Team\n\n\nThis Week\n\n\n\n\nDerek at PEARC this week and the OSG User School next week\n\n\nContributing to CVMFS and looking at he LIGO use case again\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)", 
            "title": "July 10, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170710/#osg-technology-area-meeting-10-july-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin     Attending:  BrianB, BrianL, Carl, Edgar, Marian, Mat, TimT, Vaibhav", 
            "title": "OSG Technology Area Meeting, 10 July 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170710/#announcements", 
            "text": "OSG User School next week", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170710/#triage-duty", 
            "text": "This week: Derek  Next week: Edgar  5 ( 1) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170710/#jira", 
            "text": "# of tickets   State      156  +7  Open    11  +0  In Progress    2  21  Ready for Testing    25  +23  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170710/#release-schedule", 
            "text": "Version  Development Freeze  Package Freeze  Release  Notes      3.4.1 / 3.3.26  2017-06-26  2017-07-03  2017-07-11  Independence Day    3.4.2 / 3.3.27  2017-07-24  2017-07-31  2017-08-08      Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170710/#osg-software-team", 
            "text": "Two weeks until August development freeze  Major LCMAPS VOMS plugin issues plaguing multiple sites, causing extensive downtime", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170710/#discussions", 
            "text": "The root cause of the LCMAPS VOMS segfaults appear to be due to memory issues in the proxy verification module. BrianB hopes to have a fix by the end of the day.", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170710/#support-update", 
            "text": "Clemson, IIT, UConn (BrianL) - LCMAPS VOMS plugin issue above", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170710/#osg-release-team", 
            "text": "Tim Theisen is handling the  July 11th  release  Release tomorrow      3.3.26   Both   3.4.1   Total   Status      0  +0  0  +0  0  +0  0  +0  Open    0  +0  0  +0  0  +0  0  +0  In Progress    0  -2  18  -18  0  -3  0  -23  Ready for Testing    2  +2  19  +18  4  +4  25  +24  Ready for Release    2  +0  19  +0  4  +1  25  +1  Total", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170710/#discussions_1", 
            "text": "Operations will be adopting a more flexible release schedule, at least for changes transparent to the user  Cert update incoming; Edgar will help test it while Suchandra is at PEARC. In the future, it may make more sense to test cert packages in an automated fashion \\minus; perhaps with  rpmdiff .", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170710/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170710/#this-week", 
            "text": "Derek at PEARC this week and the OSG User School next week  Contributing to CVMFS and looking at he LIGO use case again", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170710/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170717/", 
            "text": "OSG Technology Area Meeting, 17 July 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n\n\nAttending:\n\nCarl, Edgar, Marian, Mat, Suchandra, Tim T\n\n\nAnnouncements\n\n\nOSG User School this week\n\n\nTriage Duty\n\n\n\n\nThis week: Edgar\n\n\nNext week: Mat\n\n\n11 (+6) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n155\n\n\n1\n\n\nOpen\n\n\n\n\n\n\n23\n\n\n+12\n\n\nIn Progress\n\n\n\n\n\n\n4\n\n\n2\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n25\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nName\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nAugust\n\n\n3.4.2, 3.3.27\n\n\n2017-07-24\n\n\n2017-07-31\n\n\n2017-08-08\n\n\n\n\n\n\n\n\nSeptember\n\n\n3.4.3, 3.3.28\n\n\n2017-08-28\n\n\n2017-09-05\n\n\n2017-09-12\n\n\n5 week cycle\n\n\n\n\n\n\nOctober\n\n\n3.4.4, 3.3.29\n\n\n2017-09-25\n\n\n2017-10-02\n\n\n2017-10-10\n\n\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.\n\n\nOSG Software Team\n\n\nDiscussions\n\n\n\n\nNew GFAL tools issue causing problems for XENON1T, on CVMFS. -- need to check if it's a 3.4 issue only\n\n\nNew Condor 8.6.5 release this week to fix IPv6 bugs, but the new release breaks Dave Dykstra's Python modules so we're debugging that\n\n\n\n\nSupport Update\n\n\nnone\n\n\nOSG Release Team\n\n\n\n\nSuchandra Thapa is handling the \nAugust 8th\n release\n\n\nDevelopment Freeze next week\n\n\n\n\n\n\n\n\n\n\n3.3.27\n\n\n\n\nBoth\n\n\n\n\n3.4.2\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n1\n\n\n+0\n\n\n4\n\n\n+0\n\n\n2\n\n\n+0\n\n\n7\n\n\n+0\n\n\nOpen\n\n\n\n\n\n\n0\n\n\n+0\n\n\n10\n\n\n+0\n\n\n4\n\n\n+0\n\n\n14\n\n\n+0\n\n\nIn Progress\n\n\n\n\n\n\n2\n\n\n+0\n\n\n1\n\n\n+0\n\n\n1\n\n\n+0\n\n\n4\n\n\n+0\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n3\n\n\n+0\n\n\n15\n\n\n+0\n\n\n7\n\n\n+0\n\n\n25\n\n\n+0\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\nReady for Testing\n\n\nBoth\n\n\nUpdate gsi-openssh-server\n\n\n\n\n\n\n3.4.2\n\n\nMerge osg-ce packages\n\n\n\n\n\n\nBoth\n\n\nUpdate to HTCondor 8.4.12 in OSG 3.3\n\n\nJGlobus incorrectly refuses proxies with key usage\n\n\n\n\n\n\n\n\nDiscussions\n\n\nnone\n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nDerek at PEARC\n\n\n\n\nThis Week\n\n\n\n\nDerek at OSG school\n\n\nStash origin maintenance at UChicago over the weekend so StashCache inaccessible over the weekend\n\n\nGRACC accounting issue with Florida revealed that Florida wasn't correctly running Gratia probes\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)", 
            "title": "July 17, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170717/#osg-technology-area-meeting-17-july-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin  Attending: \nCarl, Edgar, Marian, Mat, Suchandra, Tim T", 
            "title": "OSG Technology Area Meeting, 17 July 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170717/#announcements", 
            "text": "OSG User School this week", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170717/#triage-duty", 
            "text": "This week: Edgar  Next week: Mat  11 (+6) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170717/#jira", 
            "text": "# of tickets   State      155  1  Open    23  +12  In Progress    4  2  Ready for Testing    0  25  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170717/#release-schedule", 
            "text": "Name  Version  Development Freeze  Package Freeze  Release  Notes      August  3.4.2, 3.3.27  2017-07-24  2017-07-31  2017-08-08     September  3.4.3, 3.3.28  2017-08-28  2017-09-05  2017-09-12  5 week cycle    October  3.4.4, 3.3.29  2017-09-25  2017-10-02  2017-10-10      Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170717/#osg-software-team", 
            "text": "", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170717/#discussions", 
            "text": "New GFAL tools issue causing problems for XENON1T, on CVMFS. -- need to check if it's a 3.4 issue only  New Condor 8.6.5 release this week to fix IPv6 bugs, but the new release breaks Dave Dykstra's Python modules so we're debugging that", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170717/#support-update", 
            "text": "none", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170717/#osg-release-team", 
            "text": "Suchandra Thapa is handling the  August 8th  release  Development Freeze next week      3.3.27   Both   3.4.2   Total   Status      1  +0  4  +0  2  +0  7  +0  Open    0  +0  10  +0  4  +0  14  +0  In Progress    2  +0  1  +0  1  +0  4  +0  Ready for Testing    0  +0  0  +0  0  +0  0  +0  Ready for Release    3  +0  15  +0  7  +0  25  +0  Total      Ready for Testing  Both  Update gsi-openssh-server    3.4.2  Merge osg-ce packages    Both  Update to HTCondor 8.4.12 in OSG 3.3  JGlobus incorrectly refuses proxies with key usage", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170717/#discussions_1", 
            "text": "none", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170717/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170717/#last-week", 
            "text": "Derek at PEARC", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170717/#this-week", 
            "text": "Derek at OSG school  Stash origin maintenance at UChicago over the weekend so StashCache inaccessible over the weekend  GRACC accounting issue with Florida revealed that Florida wasn't correctly running Gratia probes", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170717/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170724/", 
            "text": "OSG Technology Area Meeting, 24 July 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n\n\nAttending:\n   \n\n\nAnnouncements\n\n\n\n\nVaibhav no longer with UCSD\n\n\nOSG Annual Planning Retreat Tue - Wed\n\n\n\n\nTriage Duty\n\n\n\n\nThis week: Mat\n\n\nNext week: Suchandra\n\n\n12 (+1) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n161\n\n\n+6\n\n\nOpen\n\n\n\n\n\n\n15\n\n\n-8\n\n\nIn Progress\n\n\n\n\n\n\n17\n\n\n+13\n\n\nReady for Testing\n\n\n\n\n\n\n1\n\n\n+1\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nName\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nAugust\n\n\n3.4.2, 3.3.27\n\n\n2017-07-24\n\n\n2017-07-31\n\n\n2017-08-08\n\n\n\n\n\n\n\n\nSeptember\n\n\n3.4.3, 3.3.28\n\n\n2017-08-28\n\n\n2017-09-05\n\n\n2017-09-12\n\n\n5 week cycle\n\n\n\n\n\n\nOctober\n\n\n3.4.4, 3.3.29\n\n\n2017-09-25\n\n\n2017-10-02\n\n\n2017-10-10\n\n\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\n\n\nOpen tickets:\n\n\n\n\n\n\n\n\n\n\nDeveloper\n\n\n# non-RFT\n\n\n\n\n\n\n\n\n\n\nBrian L\n\n\n8\n\n\n\n\n\n\nDave D\n\n\n2\n\n\n\n\n\n\nMat S\n\n\n1\n\n\n\n\n\n\n\n\n\n\nNeed volunteers for building HTCondor 8.6.5 and developer test Singularity 2.3\n\n\nVMU tests are unavailable due to Gluster maintenance. Expected to be functional by EOB Tuesday  \n\n\nIn the meantime, use \nTravis-CI\n\n\n\n\n\n\nMysterious gsi-openssh-server failures in the nightlies on EL7, 3.3. testing\n\n\n\n\nDiscussions\n\n\nSupport Update\n\n\n\n\nFIT (BrianL): CE can't contact backend schedd/pool\n\n\nUConn (BrianL): CE auth issues likely an internal OSG issue at this point\n\n\nUWash (BrianL): jobs held due to \"non-existent route or job route limit\"\n\n\nFlorida (Derek): Issues with slurm probe not reporting records.  Caught in WLCG report. (ongoing)\n\n\nWisconsin (Derek): Routes for RHEL6 and RHEL7 nodes (ongoing)\n\n\n\n\nOSG Release Team\n\n\n\n\nSuchandra Thapa is handling the \nAugust 8th\n release\n\n\nDev freeze today\n\n\n\n\n\n\n\n\n\n\n3.3.27\n\n\n\n\nBoth\n\n\n\n\n3.4.2\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n0\n\n\n+0\n\n\n1\n\n\n+1\n\n\n2\n\n\n+2\n\n\n3\n\n\n+3\n\n\nOpen\n\n\n\n\n\n\n1\n\n\n+1\n\n\n5\n\n\n+5\n\n\n2\n\n\n+2\n\n\n8\n\n\n+8\n\n\nIn Progress\n\n\n\n\n\n\n3\n\n\n+3\n\n\n9\n\n\n+9\n\n\n5\n\n\n+5\n\n\n17\n\n\n+17\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n4\n\n\n+4\n\n\n15\n\n\n+15\n\n\n9\n\n\n+9\n\n\n28\n\n\n+28\n\n\nTotal\n\n\n\n\n\n\n\n\nDiscussions\n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nNot much going on with GRACC, which is good for a production service.\n\n\nDerek at OSG User School\n\n\nOSG User School used StashCP with great success!  No issues found with the infrastructure or tools with any of the 60 students using it.\n\n\n\n\nThis Week\n\n\n\n\nGRACC Backup reports\n\n\nCVMFS-Sync RPM across finish line.\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)", 
            "title": "July 24, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170724/#osg-technology-area-meeting-24-july-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin  Attending:", 
            "title": "OSG Technology Area Meeting, 24 July 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170724/#announcements", 
            "text": "Vaibhav no longer with UCSD  OSG Annual Planning Retreat Tue - Wed", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170724/#triage-duty", 
            "text": "This week: Mat  Next week: Suchandra  12 (+1) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170724/#jira", 
            "text": "# of tickets   State      161  +6  Open    15  -8  In Progress    17  +13  Ready for Testing    1  +1  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170724/#release-schedule", 
            "text": "Name  Version  Development Freeze  Package Freeze  Release  Notes      August  3.4.2, 3.3.27  2017-07-24  2017-07-31  2017-08-08     September  3.4.3, 3.3.28  2017-08-28  2017-09-05  2017-09-12  5 week cycle    October  3.4.4, 3.3.29  2017-09-25  2017-10-02  2017-10-10      Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170724/#osg-software-team", 
            "text": "Open tickets:      Developer  # non-RFT      Brian L  8    Dave D  2    Mat S  1      Need volunteers for building HTCondor 8.6.5 and developer test Singularity 2.3  VMU tests are unavailable due to Gluster maintenance. Expected to be functional by EOB Tuesday    In the meantime, use  Travis-CI    Mysterious gsi-openssh-server failures in the nightlies on EL7, 3.3. testing", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170724/#discussions", 
            "text": "", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170724/#support-update", 
            "text": "FIT (BrianL): CE can't contact backend schedd/pool  UConn (BrianL): CE auth issues likely an internal OSG issue at this point  UWash (BrianL): jobs held due to \"non-existent route or job route limit\"  Florida (Derek): Issues with slurm probe not reporting records.  Caught in WLCG report. (ongoing)  Wisconsin (Derek): Routes for RHEL6 and RHEL7 nodes (ongoing)", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170724/#osg-release-team", 
            "text": "Suchandra Thapa is handling the  August 8th  release  Dev freeze today      3.3.27   Both   3.4.2   Total   Status      0  +0  1  +1  2  +2  3  +3  Open    1  +1  5  +5  2  +2  8  +8  In Progress    3  +3  9  +9  5  +5  17  +17  Ready for Testing    0  +0  0  +0  0  +0  0  +0  Ready for Release    4  +4  15  +15  9  +9  28  +28  Total", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170724/#discussions_1", 
            "text": "", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170724/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170724/#last-week", 
            "text": "Not much going on with GRACC, which is good for a production service.  Derek at OSG User School  OSG User School used StashCP with great success!  No issues found with the infrastructure or tools with any of the 60 students using it.", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170724/#this-week", 
            "text": "GRACC Backup reports  CVMFS-Sync RPM across finish line.", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170724/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170731/", 
            "text": "OSG Technology Area Meeting, 31 July 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n\n\nAttending:\n Derek, Edgar, Marian, Mat, Suchandra, TimT\n\n\nAnnouncements\n\n\nTriage Duty\n\n\n\n\nThis week: Suchandra\n\n\nNext week: TimT\n\n\n12 (+0) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n158\n\n\n-3\n\n\nOpen\n\n\n\n\n\n\n14\n\n\n-1\n\n\nIn Progress\n\n\n\n\n\n\n25\n\n\n+8\n\n\nReady for Testing\n\n\n\n\n\n\n1\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nName\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nAugust\n\n\n3.4.2, 3.3.27\n\n\n2017-07-24\n\n\n2017-07-31\n\n\n2017-08-08\n\n\n\n\n\n\n\n\nSeptember\n\n\n3.4.3, 3.3.28\n\n\n2017-08-28\n\n\n2017-09-05\n\n\n2017-09-12\n\n\n5 week cycle\n\n\n\n\n\n\nOctober\n\n\n3.4.4, 3.3.29\n\n\n2017-09-25\n\n\n2017-10-02\n\n\n2017-10-10\n\n\n\n\n\n\n\n\n\n\nOSG Software Team\n\n\n\n\nhtcondor collector python plugin broken in 8.6.4; fixed in 8.6.5 \n(SOFTWARE-2816)\n which will be released tomorrow morning\n\n\n\n\nDiscussions\n\n\n\n\nAlejandro at CERN wants a new fix for JGlobus to add the SSL option DontInsertEmptyFragments to fix a Bestman issue; Mat will take a look\n\n\n\n\nSupport Update\n\n\n\n\nFIT (BrianL): CE can't contact backend schedd/pool\n\n\nUConn (BrianL): CE auth issues likely an internal OSG issue at this point\n\n\nUWash (BrianL): jobs held due to \"non-existent route or job route limit\"\n\n\n\n\nOSG Release Team\n\n\n\n\nSuchandra Thapa is handling the \nAugust 8th\n release\n\n\nPackage freeze today\n\n\nNeed testing help\n\n\n\n\n\n\n\n\n\n\n3.3.27\n\n\n\n\nBoth\n\n\n\n\n3.4.2\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n0\n\n\n+0\n\n\n1\n\n\n+0\n\n\n0\n\n\n-2\n\n\n1\n\n\n-2\n\n\nOpen\n\n\n\n\n\n\n0\n\n\n-1\n\n\n0\n\n\n-5\n\n\n2\n\n\n+0\n\n\n2\n\n\n-6\n\n\nIn Progress\n\n\n\n\n\n\n4\n\n\n+1\n\n\n13\n\n\n+4\n\n\n8\n\n\n+3\n\n\n25\n\n\n+8\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n4\n\n\n+0\n\n\n14\n\n\n-1\n\n\n10\n\n\n+1\n\n\n28\n\n\n+0\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\nBoth\n\n\nFix selinux issues with GSI OpenSSH in EL7 nightly tests\n\n\nUpdate gsi-openssh-server\n\n\nAdd gsi-openssh packages to osg-tested-internal\n\n\nosg-configure does not warn/error in -v\n\n\ncondor-cron: disable gsi authz\n\n\nRelease condor-cron 1.1.3\n\n\ncondor-cron: add a way for users to override condor_ids\n\n\nosg-configure: Configure GUMS before running gums-host-cron\n\n\nosg-configure: Fix logging in ensure_valid_user_vo_file\n\n\nAdd blahp configuration to differentiate PBS flavors\n\n\nCEView VO tab throws 500 error on inital installation\n\n\nCondor-CE: Do not hold running jobs with expired proxy\n\n\nHTCondor-CE: only warn about configuration if osg-configure is present\n\n\nosg-configure: Make exception usage consistent\n\n\n3.4.2\n\n\nFix selinux issues with GSI OpenSSH in EL7 nightly tests\n\n\nUpdate gsi-openssh-server\n\n\nAdd gsi-openssh packages to osg-tested-internal\n\n\nosg-configure does not warn/error in -v\n\n\ncondor-cron: disable gsi authz\n\n\nRelease condor-cron 1.1.3\n\n\ncondor-cron: add a way for users to override condor_ids\n\n\nosg-configure: Configure GUMS before running gums-host-cron\n\n\nosg-configure: Fix logging in ensure_valid_user_vo_file\n\n\nAdd blahp configuration to differentiate PBS flavors\n\n\nCEView VO tab throws 500 error on inital installation\n\n\nCondor-CE: Do not hold running jobs with expired proxy\n\n\nHTCondor-CE: only warn about configuration if osg-configure is present\n\n\nosg-configure: Make exception usage consistent\n\n\n3.2.27\n\n\nJGlobus incorrectly refuses proxies with key usage\n\n\nRelease osg-configure 1.9.1 (OSG 3.3)\n\n\nUpdate to HTCondor 8.4.12 in OSG 3.3\n\n\nRelease htcondor-ce-2.2.2-1+\n\n\n\n\nDiscussions\n\n\n\n\n\n\nWill need software team's help testing\n\n\n\n\n\n\nFrom the planning retreat:\n\n\n\n\nRelease team might go to a \"community testing\" model where we will ask the stakeholders to test changes\n\n\nMaybe might go to a \"rolling release\" model instead of point releases; may be not until 3.5; Tim T to write proposal\n\n\n\n\n\n\n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nGRACC-ITB moving forward\n\n\nSome GRACC support of a \nslurm_meter\n issue, still debugging.\n\n\n\n\nThis Week\n\n\n\n\nGRACC Backup reports\n\n\nCVMFS-Sync RPM across finish line.\n\n\nGRACC backup of Dashboards\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)", 
            "title": "July 31, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170731/#osg-technology-area-meeting-31-july-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin  Attending:  Derek, Edgar, Marian, Mat, Suchandra, TimT", 
            "title": "OSG Technology Area Meeting, 31 July 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170731/#announcements", 
            "text": "", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170731/#triage-duty", 
            "text": "This week: Suchandra  Next week: TimT  12 (+0) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170731/#jira", 
            "text": "# of tickets   State      158  -3  Open    14  -1  In Progress    25  +8  Ready for Testing    1  +0  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170731/#release-schedule", 
            "text": "Name  Version  Development Freeze  Package Freeze  Release  Notes      August  3.4.2, 3.3.27  2017-07-24  2017-07-31  2017-08-08     September  3.4.3, 3.3.28  2017-08-28  2017-09-05  2017-09-12  5 week cycle    October  3.4.4, 3.3.29  2017-09-25  2017-10-02  2017-10-10", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170731/#osg-software-team", 
            "text": "htcondor collector python plugin broken in 8.6.4; fixed in 8.6.5  (SOFTWARE-2816)  which will be released tomorrow morning", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170731/#discussions", 
            "text": "Alejandro at CERN wants a new fix for JGlobus to add the SSL option DontInsertEmptyFragments to fix a Bestman issue; Mat will take a look", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170731/#support-update", 
            "text": "FIT (BrianL): CE can't contact backend schedd/pool  UConn (BrianL): CE auth issues likely an internal OSG issue at this point  UWash (BrianL): jobs held due to \"non-existent route or job route limit\"", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170731/#osg-release-team", 
            "text": "Suchandra Thapa is handling the  August 8th  release  Package freeze today  Need testing help      3.3.27   Both   3.4.2   Total   Status      0  +0  1  +0  0  -2  1  -2  Open    0  -1  0  -5  2  +0  2  -6  In Progress    4  +1  13  +4  8  +3  25  +8  Ready for Testing    0  +0  0  +0  0  +0  0  +0  Ready for Release    4  +0  14  -1  10  +1  28  +0  Total      Both  Fix selinux issues with GSI OpenSSH in EL7 nightly tests  Update gsi-openssh-server  Add gsi-openssh packages to osg-tested-internal  osg-configure does not warn/error in -v  condor-cron: disable gsi authz  Release condor-cron 1.1.3  condor-cron: add a way for users to override condor_ids  osg-configure: Configure GUMS before running gums-host-cron  osg-configure: Fix logging in ensure_valid_user_vo_file  Add blahp configuration to differentiate PBS flavors  CEView VO tab throws 500 error on inital installation  Condor-CE: Do not hold running jobs with expired proxy  HTCondor-CE: only warn about configuration if osg-configure is present  osg-configure: Make exception usage consistent  3.4.2  Fix selinux issues with GSI OpenSSH in EL7 nightly tests  Update gsi-openssh-server  Add gsi-openssh packages to osg-tested-internal  osg-configure does not warn/error in -v  condor-cron: disable gsi authz  Release condor-cron 1.1.3  condor-cron: add a way for users to override condor_ids  osg-configure: Configure GUMS before running gums-host-cron  osg-configure: Fix logging in ensure_valid_user_vo_file  Add blahp configuration to differentiate PBS flavors  CEView VO tab throws 500 error on inital installation  Condor-CE: Do not hold running jobs with expired proxy  HTCondor-CE: only warn about configuration if osg-configure is present  osg-configure: Make exception usage consistent  3.2.27  JGlobus incorrectly refuses proxies with key usage  Release osg-configure 1.9.1 (OSG 3.3)  Update to HTCondor 8.4.12 in OSG 3.3  Release htcondor-ce-2.2.2-1+", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170731/#discussions_1", 
            "text": "Will need software team's help testing    From the planning retreat:   Release team might go to a \"community testing\" model where we will ask the stakeholders to test changes  Maybe might go to a \"rolling release\" model instead of point releases; may be not until 3.5; Tim T to write proposal", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170731/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170731/#last-week", 
            "text": "GRACC-ITB moving forward  Some GRACC support of a  slurm_meter  issue, still debugging.", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170731/#this-week", 
            "text": "GRACC Backup reports  CVMFS-Sync RPM across finish line.  GRACC backup of Dashboards", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170731/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170807/", 
            "text": "OSG Technology Area Meeting,  7 August 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n\n\nAttending:\n\n\nAnnouncements\n\n\nTriage Duty\n\n\n\n\nThis week: TimT\n\n\nNext week: Mat\n\n\n12 (+0) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n164\n\n\n+6\n\n\nOpen\n\n\n\n\n\n\n14\n\n\n+0\n\n\nIn Progress\n\n\n\n\n\n\n2\n\n\n-23\n\n\nReady for Testing\n\n\n\n\n\n\n28\n\n\n+27\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nName\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nAugust\n\n\n3.4.2, 3.3.27\n\n\n2017-07-24\n\n\n2017-07-31\n\n\n2017-08-08\n\n\n\n\n\n\n\n\nSeptember\n\n\n3.4.3, 3.3.28\n\n\n2017-08-28\n\n\n2017-09-05\n\n\n2017-09-12\n\n\n5 week cycle\n\n\n\n\n\n\nOctober\n\n\n3.4.4, 3.3.29\n\n\n2017-09-25\n\n\n2017-10-02\n\n\n2017-10-10\n\n\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.\n\n\nOSG Software Team\n\n\n\n\nGUMS, Gratia, and GSI OpenSSH failures in the 3.3 nightlies\n\n\nNew SELinux policies for HTCondor require rebuilds in 3.3, 3.4, and upcoming\n\n\nList Globus package dependencies to prepare for EOL\n\n\nSoftware team members track effot percentage numbers in shared google spreadsheet\n\n\nDocumentation transition goal: migrate ~2 documents per week. Details incoming.\n\n\n\n\nDiscussions\n\n\n\n\nTWiki doc contents should be completely replaced with header after migration\n\n\nInternal docs should be migrated first\n\n\n\n\nSupport Update\n\n\n\n\nFIT (BrianL): CE can't contact backend schedd/pool\n\n\nosg-connect (BrianL) - Submit wrapper issue with new \n-nobatch\n option\n\n\nVanderbilt (Derek) - HTCondor-CE issues\n\n\n\n\nOSG Release Team\n\n\n\n\nSuchandra Thapa is handling the \nAugust 8th\n release\n\n\nRelease tomorrow\n\n\nData Release this week (IGTF 1.85)\n\n\n\n\n\n\n\n\n\n\n3.3.27\n\n\n\n\nBoth\n\n\n\n\n3.4.2\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n0\n\n\n+0\n\n\n1\n\n\n+0\n\n\n0\n\n\n+0\n\n\n1\n\n\n+0\n\n\nOpen\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n-2\n\n\n0\n\n\n-2\n\n\nIn Progress\n\n\n\n\n\n\n0\n\n\n-4\n\n\n0\n\n\n-13\n\n\n0\n\n\n-8\n\n\n0\n\n\n-25\n\n\nReady for Testing\n\n\n\n\n\n\n4\n\n\n+4\n\n\n13\n\n\n+13\n\n\n10\n\n\n+10\n\n\n27\n\n\n+27\n\n\nReady for Release\n\n\n\n\n\n\n4\n\n\n+0\n\n\n14\n\n\n+0\n\n\n10\n\n\n+0\n\n\n28\n\n\n+0\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\nBoth\n\n\nRed Hat 7.4 update breaks HTCondor\n\n\nCondor-CE: Do not hold running jobs with expired proxy\n\n\ncondor-cron: add a way for users to override condor_ids\n\n\nosg-configure: Fix logging in ensure_valid_user_vo_file\n\n\nosg-configure does not warn/error in -v\n\n\nCEView VO tab throws 500 error on inital installation\n\n\nHTCondor-CE: only warn about configuration if osg-configure is present\n\n\nosg-configure: Configure GUMS before running gums-host-cron\n\n\nAdd blahp configuration to differentiate PBS flavors\n\n\ncondor-cron: disable gsi authz\n\n\nRelease condor-cron 1.1.3\n\n\nAdd gsi-openssh packages to osg-tested-internal\n\n\nFix selinux issues with GSI OpenSSH in EL7 nightly tests\n\n\nosg-configure: Make exception usage consistent\n\n\n3.4.2\n\n\nhtcondor collector python plugin has undefined symbols\n\n\nAdd pilot payload auditing\n\n\nUpdate to HTCondor 8.6.5 in OSG 3.4\n\n\nMerge osg-ce packages\n\n\nosg-configure: Remove unused test configs\n\n\nRelease htcondor-ce-3.0.0-1+\n\n\nAdd osg-gridftp back to osg-tested-internal\n\n\nRelease osg-configure 2.1.1 (OSG 3.4)\n\n\nRelease osg-tested-internal-3.4-3+\n\n\nUpcoming: Patch HTCondor 8.7.2 to work with Python Collector plugins\n\n\n3.3.27\n\n\nJGlobus incorrectly refuses proxies with key usage\n\n\nUpdate to HTCondor 8.4.12 in OSG 3.3\n\n\nRelease htcondor-ce-2.2.2-1+\n\n\nRelease osg-configure 1.9.1 (OSG 3.3)\n\n\n\n\nDiscussions\n\n\n\n\nEdgar was a big help to the release effort, thanks!\n\n\nOU will help with testing the late-breaking condor packaging changes\n\n\n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nGRACC-ITB moving forward\n\n\nSome GRACC support of a `slurm\nmeter\n` issue, still debugging.\n\n\nCVMFS-Sync RPM finished line.\n\n\nIndexing Glidein Logs in GRACC's ES.\n\n\n\n\nThis Week\n\n\n\n\nGRACC Backup reports\n\n\nGRACC backup of Dashboards\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)", 
            "title": "August 7, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170807/#osg-technology-area-meeting-7-august-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin  Attending:", 
            "title": "OSG Technology Area Meeting,  7 August 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170807/#announcements", 
            "text": "", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170807/#triage-duty", 
            "text": "This week: TimT  Next week: Mat  12 (+0) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170807/#jira", 
            "text": "# of tickets   State      164  +6  Open    14  +0  In Progress    2  -23  Ready for Testing    28  +27  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170807/#release-schedule", 
            "text": "Name  Version  Development Freeze  Package Freeze  Release  Notes      August  3.4.2, 3.3.27  2017-07-24  2017-07-31  2017-08-08     September  3.4.3, 3.3.28  2017-08-28  2017-09-05  2017-09-12  5 week cycle    October  3.4.4, 3.3.29  2017-09-25  2017-10-02  2017-10-10      Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170807/#osg-software-team", 
            "text": "GUMS, Gratia, and GSI OpenSSH failures in the 3.3 nightlies  New SELinux policies for HTCondor require rebuilds in 3.3, 3.4, and upcoming  List Globus package dependencies to prepare for EOL  Software team members track effot percentage numbers in shared google spreadsheet  Documentation transition goal: migrate ~2 documents per week. Details incoming.", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170807/#discussions", 
            "text": "TWiki doc contents should be completely replaced with header after migration  Internal docs should be migrated first", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170807/#support-update", 
            "text": "FIT (BrianL): CE can't contact backend schedd/pool  osg-connect (BrianL) - Submit wrapper issue with new  -nobatch  option  Vanderbilt (Derek) - HTCondor-CE issues", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170807/#osg-release-team", 
            "text": "Suchandra Thapa is handling the  August 8th  release  Release tomorrow  Data Release this week (IGTF 1.85)      3.3.27   Both   3.4.2   Total   Status      0  +0  1  +0  0  +0  1  +0  Open    0  +0  0  +0  0  -2  0  -2  In Progress    0  -4  0  -13  0  -8  0  -25  Ready for Testing    4  +4  13  +13  10  +10  27  +27  Ready for Release    4  +0  14  +0  10  +0  28  +0  Total      Both  Red Hat 7.4 update breaks HTCondor  Condor-CE: Do not hold running jobs with expired proxy  condor-cron: add a way for users to override condor_ids  osg-configure: Fix logging in ensure_valid_user_vo_file  osg-configure does not warn/error in -v  CEView VO tab throws 500 error on inital installation  HTCondor-CE: only warn about configuration if osg-configure is present  osg-configure: Configure GUMS before running gums-host-cron  Add blahp configuration to differentiate PBS flavors  condor-cron: disable gsi authz  Release condor-cron 1.1.3  Add gsi-openssh packages to osg-tested-internal  Fix selinux issues with GSI OpenSSH in EL7 nightly tests  osg-configure: Make exception usage consistent  3.4.2  htcondor collector python plugin has undefined symbols  Add pilot payload auditing  Update to HTCondor 8.6.5 in OSG 3.4  Merge osg-ce packages  osg-configure: Remove unused test configs  Release htcondor-ce-3.0.0-1+  Add osg-gridftp back to osg-tested-internal  Release osg-configure 2.1.1 (OSG 3.4)  Release osg-tested-internal-3.4-3+  Upcoming: Patch HTCondor 8.7.2 to work with Python Collector plugins  3.3.27  JGlobus incorrectly refuses proxies with key usage  Update to HTCondor 8.4.12 in OSG 3.3  Release htcondor-ce-2.2.2-1+  Release osg-configure 1.9.1 (OSG 3.3)", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170807/#discussions_1", 
            "text": "Edgar was a big help to the release effort, thanks!  OU will help with testing the late-breaking condor packaging changes", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170807/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170807/#last-week", 
            "text": "GRACC-ITB moving forward  Some GRACC support of a `slurm meter ` issue, still debugging.  CVMFS-Sync RPM finished line.  Indexing Glidein Logs in GRACC's ES.", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170807/#this-week", 
            "text": "GRACC Backup reports  GRACC backup of Dashboards", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170807/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170814/", 
            "text": "OSG Technology Area Meeting, 14 August 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n\n\nAttending:\n BrianL, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT\n\n\nAnnouncements\n\n\nNo meeting next week due to outages  \n\n\nTriage Duty\n\n\n\n\nThis week: Mat\n\n\nNext week: Carl\n\n\n12 (+0) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n160\n\n\n-4\n\n\nOpen\n\n\n\n\n\n\n18\n\n\n+4\n\n\nIn Progress\n\n\n\n\n\n\n2\n\n\n+0\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n-28\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nName\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nSeptember\n\n\n3.4.3, 3.3.28\n\n\n2017-08-28\n\n\n2017-09-05\n\n\n2017-09-12\n\n\n5 week cycle\n\n\n\n\n\n\nOctober\n\n\n3.4.4, 3.3.29\n\n\n2017-09-25\n\n\n2017-10-02\n\n\n2017-10-10\n\n\n\n\n\n\n\n\nNovemeber\n\n\n3.4.5, 3.3.29\n\n\n2017-10-30\n\n\n2017-11-06\n\n\n2017-11-14\n\n\n5 week cycle\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\n\n\n13 documents migrated last week, 3 more awaiting review\n\n\nGUMS and Gratia failures in the RHEL7 3.3 nightlies due to new SELinux policy\n\n\nSoftware team members track effot percentage numbers in shared google spreadsheet\n\n\n\n\nDiscussions\n\n\nNone this week\n\n\nSupport Update\n\n\n\n\nFIT (BrianL): CE can't contact backend schedd/pool\n\n\n\n\nOSG Release Team\n\n\n\n\nSuchandra Thapa is handling the \nSeptember 12th\n release\n\n\nDevelopement Freeze in 2 weeks.\n\n\nData release today (IGTF 1.85)\n\n\n\n\n\n\n\n\n\n\n3.3.28\n\n\n\n\nBoth\n\n\n\n\n3.4.3\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n5\n\n\n+5\n\n\n11\n\n\n+11\n\n\n1\n\n\n+1\n\n\n17\n\n\n+17\n\n\nOpen\n\n\n\n\n\n\n2\n\n\n+2\n\n\n6\n\n\n+6\n\n\n1\n\n\n+1\n\n\n9\n\n\n+9\n\n\nIn Progress\n\n\n\n\n\n\n0\n\n\n+0\n\n\n1\n\n\n+1\n\n\n0\n\n\n+0\n\n\n1\n\n\n+1\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n7\n\n\n+7\n\n\n18\n\n\n+18\n\n\n2\n\n\n+2\n\n\n27\n\n\n+27\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\nBoth\n\n\nUpdate gsi-openssh-server\n\n\n\n\n\n\n3.4.3\n\n\nNothing yet\n\n\n\n\n\n\n3.3.28\n\n\nNothing yet\n\n\n\n\n\n\n\n\nDiscussions\n\n\nNone this week\n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nGRACC-ITB moving forward\n\n\nSome GRACC support of a `slurm\nmeter\n` issue, still debugging\n\n\nIndexing Glidein Logs in GRACC's ES\n\n\nStarted backup reports\n\n\nFirst step of creating ES snapshots for backing up GRACC\n\n\n\n\nThis Week\n\n\n\n\nFinish GRACC Backup reports\n\n\nGRACC backup of Dashboards\n\n\nInitiate backups of ES snapshots\n\n\nStart indexing GOC service status in GRACC ES.\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)\n\n\n\n\nDiscussions\n\n\n\n\nOnce format of Glidein Logs in GRACC ES is finalized, light documentation will be written for the benefit of internal teams\n\n\nDerek offered to give a short presentation on ES queries", 
            "title": "August 14, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170814/#osg-technology-area-meeting-14-august-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin  Attending:  BrianL, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT", 
            "title": "OSG Technology Area Meeting, 14 August 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170814/#announcements", 
            "text": "No meeting next week due to outages", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170814/#triage-duty", 
            "text": "This week: Mat  Next week: Carl  12 (+0) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170814/#jira", 
            "text": "# of tickets   State      160  -4  Open    18  +4  In Progress    2  +0  Ready for Testing    0  -28  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170814/#release-schedule", 
            "text": "Name  Version  Development Freeze  Package Freeze  Release  Notes      September  3.4.3, 3.3.28  2017-08-28  2017-09-05  2017-09-12  5 week cycle    October  3.4.4, 3.3.29  2017-09-25  2017-10-02  2017-10-10     Novemeber  3.4.5, 3.3.29  2017-10-30  2017-11-06  2017-11-14  5 week cycle     Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170814/#osg-software-team", 
            "text": "13 documents migrated last week, 3 more awaiting review  GUMS and Gratia failures in the RHEL7 3.3 nightlies due to new SELinux policy  Software team members track effot percentage numbers in shared google spreadsheet", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170814/#discussions", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170814/#support-update", 
            "text": "FIT (BrianL): CE can't contact backend schedd/pool", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170814/#osg-release-team", 
            "text": "Suchandra Thapa is handling the  September 12th  release  Developement Freeze in 2 weeks.  Data release today (IGTF 1.85)      3.3.28   Both   3.4.3   Total   Status      5  +5  11  +11  1  +1  17  +17  Open    2  +2  6  +6  1  +1  9  +9  In Progress    0  +0  1  +1  0  +0  1  +1  Ready for Testing    0  +0  0  +0  0  +0  0  +0  Ready for Release    7  +7  18  +18  2  +2  27  +27  Total      Both  Update gsi-openssh-server    3.4.3  Nothing yet    3.3.28  Nothing yet", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170814/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170814/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170814/#last-week", 
            "text": "GRACC-ITB moving forward  Some GRACC support of a `slurm meter ` issue, still debugging  Indexing Glidein Logs in GRACC's ES  Started backup reports  First step of creating ES snapshots for backing up GRACC", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170814/#this-week", 
            "text": "Finish GRACC Backup reports  GRACC backup of Dashboards  Initiate backups of ES snapshots  Start indexing GOC service status in GRACC ES.", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170814/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170814/#discussions_2", 
            "text": "Once format of Glidein Logs in GRACC ES is finalized, light documentation will be written for the benefit of internal teams  Derek offered to give a short presentation on ES queries", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170828/", 
            "text": "OSG Technology Area Meeting, 28 August 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n\n\nAttending:\n BrianL, Carl, Derek, Edgar, Suchandra, TimC\n\n\nAnnouncements\n\n\nTriage Duty\n\n\n\n\nThis week: Edgar\n\n\nNext week: Derek\n\n\n9 (-3) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n154\n\n\n-3\n\n\nOpen\n\n\n\n\n\n\n20\n\n\n-1\n\n\nIn Progress\n\n\n\n\n\n\n14\n\n\n+12\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nName\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nSeptember\n\n\n3.4.3, 3.3.28\n\n\n2017-08-28\n\n\n2017-09-05\n\n\n2017-09-12\n\n\n5 week cycle\n\n\n\n\n\n\nOctober\n\n\n3.4.4, 3.3.29\n\n\n2017-09-25\n\n\n2017-10-02\n\n\n2017-10-10\n\n\n\n\n\n\n\n\nNovember\n\n\n3.4.5, 3.3.30\n\n\n2017-10-30\n\n\n2017-11-06\n\n\n2017-11-14\n\n\n5 week cycle\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\n3.4.3/3.3.28\n\n\n\n\nTickets not marked RFT\n\n\n\n\n\n\n\n\n\n\nOwner\n\n\n# tickets\n\n\n\n\n\n\n\n\n\n\nMat\n\n\n10\n\n\n\n\n\n\nBrian\n\n\n7\n\n\n\n\n\n\nCarl\n\n\n5\n\n\n\n\n\n\nMarian\n\n\n1\n\n\n\n\n\n\n\n\nDocumentation\n\n\n\n\n6 documents migrated and meeting-related pages archived last week:  \n\n\nhttps://github.com/opensciencegrid/technology/pulse\n\n\nhttps://github.com/opensciencegrid/docs/pulse\n\n\nIf you're short on time, pick shorter documents to migrate or archive the ones marked as such in the spreadsheet\n\n\n\n\n\n\nDerek has two students that are beginning to familiarize themselves with the migration process\n\n\nWe have enough experience with the process that we can start focusing on high-priority Release3 documents\n\n\n\n\nDiscussions\n\n\nNone this week  \n\n\nSupport Update\n\n\n\n\nClemson (BrianL, Marian): expired WN CRLs and possibly an HTCondor IPv6 bug\n\n\nJINR (BrianL): CE jobs held (\"Error parsing classad or job not found\") due to missing \npbs_pro\n in \n/etc/blah.config\n\n\nIssues with BLAHP at some of the BOSCO-CE's.  Mats and Derek are continueing to investigate.  Some issue with a sub-directory of the Bosco sandbox not being populated with the X509 certificate.\n\n\n\n\nOSG Release Team\n\n\n\n\nSuchandra Thapa is handling the \nSeptember 12th\n release\n\n\nDevelopment Freeze today!\n\n\nTimT may call upon software team members for testing assistance\n\n\n\n\n\n\n\n\n\n\n3.3.28\n\n\n\n\nBoth\n\n\n\n\n3.4.3\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n2\n\n\n-3\n\n\n9\n\n\n-2\n\n\n0\n\n\n-1\n\n\n11\n\n\n-6\n\n\nOpen\n\n\n\n\n\n\n3\n\n\n+1\n\n\n8\n\n\n+2\n\n\n1\n\n\n+0\n\n\n12\n\n\n+3\n\n\nIn Progress\n\n\n\n\n\n\n5\n\n\n+5\n\n\n6\n\n\n+5\n\n\n3\n\n\n+3\n\n\n14\n\n\n+13\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n10\n\n\n+3\n\n\n22\n\n\n+4\n\n\n4\n\n\n+2\n\n\n37\n\n\n+10\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\nBoth  \n\n\nStashCache 0.8 (\nSOFTWARE-2873\n)\n\n\nosg-ca-scripts 1.1.7 (\nSOFTWARE-2834\n)\n\n\nxrootd-lcmaps 1.3.4 (\nSOFTWARE-2847\n)\n\n\n\n\n\n\n3.4.3  \n\n\nSingularity 2.3 (\nSOFTWARE-2755\n)\n\n\nCVMFS 2.4.1 (\nSOFTWARE-2858\n)\n\n\nosg-configure 2.2.0 (\nSOFTWARE-2864\n)\n\n\n\n\n\n\n3.3.28  \n\n\nosg-configure 1.10.0 (\nSOFTWARE-2865\n)\n\n\nxrootd-hdfs 1.9.2 (\nSOFTWARE-2853\n)\n\n\n\n\n\n\n\n\nDiscussions\n\n\nNone this week  \n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nGRACC-ITB moving forward\n\n\nSome GRACC support of a \nslurm_meter\n issue, still debugging.\n\n\nIndexing Glidein Logs in GRACC's ES - Ongoing\n\n\nBackup Reports are now running every Monday (or, Sunday night, because GRACC node is in UTC time)\n\n\nFirst step of creating ES snapshots for backup up GRACC - Ongoing.\n\n\n\n\nThis Week\n\n\n\n\nGRACC backup of Dashboards\n\n\nInitiate backups of ES snapshots\n\n\nStart indexing GOC server status in GRACC ES\n\n\nWork no Glidein logs in ES\n\n\nFix naming issues in GRACC related to explosion of \"Fake\" sites in records.\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)\n\n\n\n\nDiscussions\n\n\nXRootD developers have moved to a release model where they seek explicit sign-off from stakeholders", 
            "title": "August 28, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170828/#osg-technology-area-meeting-28-august-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin  Attending:  BrianL, Carl, Derek, Edgar, Suchandra, TimC", 
            "title": "OSG Technology Area Meeting, 28 August 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170828/#announcements", 
            "text": "", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170828/#triage-duty", 
            "text": "This week: Edgar  Next week: Derek  9 (-3) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170828/#jira", 
            "text": "# of tickets   State      154  -3  Open    20  -1  In Progress    14  +12  Ready for Testing    0  +0  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170828/#release-schedule", 
            "text": "Name  Version  Development Freeze  Package Freeze  Release  Notes      September  3.4.3, 3.3.28  2017-08-28  2017-09-05  2017-09-12  5 week cycle    October  3.4.4, 3.3.29  2017-09-25  2017-10-02  2017-10-10     November  3.4.5, 3.3.30  2017-10-30  2017-11-06  2017-11-14  5 week cycle     Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170828/#osg-software-team", 
            "text": "", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170828/#3433328", 
            "text": "Tickets not marked RFT      Owner  # tickets      Mat  10    Brian  7    Carl  5    Marian  1", 
            "title": "3.4.3/3.3.28"
        }, 
        {
            "location": "/meetings/2017/TechArea20170828/#documentation", 
            "text": "6 documents migrated and meeting-related pages archived last week:    https://github.com/opensciencegrid/technology/pulse  https://github.com/opensciencegrid/docs/pulse  If you're short on time, pick shorter documents to migrate or archive the ones marked as such in the spreadsheet    Derek has two students that are beginning to familiarize themselves with the migration process  We have enough experience with the process that we can start focusing on high-priority Release3 documents", 
            "title": "Documentation"
        }, 
        {
            "location": "/meetings/2017/TechArea20170828/#discussions", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170828/#support-update", 
            "text": "Clemson (BrianL, Marian): expired WN CRLs and possibly an HTCondor IPv6 bug  JINR (BrianL): CE jobs held (\"Error parsing classad or job not found\") due to missing  pbs_pro  in  /etc/blah.config  Issues with BLAHP at some of the BOSCO-CE's.  Mats and Derek are continueing to investigate.  Some issue with a sub-directory of the Bosco sandbox not being populated with the X509 certificate.", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170828/#osg-release-team", 
            "text": "Suchandra Thapa is handling the  September 12th  release  Development Freeze today!  TimT may call upon software team members for testing assistance      3.3.28   Both   3.4.3   Total   Status      2  -3  9  -2  0  -1  11  -6  Open    3  +1  8  +2  1  +0  12  +3  In Progress    5  +5  6  +5  3  +3  14  +13  Ready for Testing    0  +0  0  +0  0  +0  0  +0  Ready for Release    10  +3  22  +4  4  +2  37  +10  Total      Both    StashCache 0.8 ( SOFTWARE-2873 )  osg-ca-scripts 1.1.7 ( SOFTWARE-2834 )  xrootd-lcmaps 1.3.4 ( SOFTWARE-2847 )    3.4.3    Singularity 2.3 ( SOFTWARE-2755 )  CVMFS 2.4.1 ( SOFTWARE-2858 )  osg-configure 2.2.0 ( SOFTWARE-2864 )    3.3.28    osg-configure 1.10.0 ( SOFTWARE-2865 )  xrootd-hdfs 1.9.2 ( SOFTWARE-2853 )", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170828/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170828/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170828/#last-week", 
            "text": "GRACC-ITB moving forward  Some GRACC support of a  slurm_meter  issue, still debugging.  Indexing Glidein Logs in GRACC's ES - Ongoing  Backup Reports are now running every Monday (or, Sunday night, because GRACC node is in UTC time)  First step of creating ES snapshots for backup up GRACC - Ongoing.", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170828/#this-week", 
            "text": "GRACC backup of Dashboards  Initiate backups of ES snapshots  Start indexing GOC server status in GRACC ES  Work no Glidein logs in ES  Fix naming issues in GRACC related to explosion of \"Fake\" sites in records.", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170828/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170828/#discussions_2", 
            "text": "XRootD developers have moved to a release model where they seek explicit sign-off from stakeholders", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170905/", 
            "text": "OSG Technology Area Meeting, 5 September 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n\n\nAttending:\n Edgar, Derek, Tim C, Tim T, Carl, Suchandra\n\n\nAnnouncements\n\n\nTriage Duty\n\n\n\n\nThis week: Derek\n\n\nNext week: Brian Lin\n\n\n9 (+0) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n152\n\n\n-4\n\n\nOpen\n\n\n\n\n\n\n19\n\n\n-1\n\n\nIn Progress\n\n\n\n\n\n\n21\n\n\n+7\n\n\nReady for Testing\n\n\n\n\n\n\n1\n\n\n+1\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nName\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nSeptember\n\n\n3.4.3, 3.3.28\n\n\n2017-08-28\n\n\n2017-09-05\n\n\n2017-09-12\n\n\n5 week cycle\n\n\n\n\n\n\nOctober\n\n\n3.4.4, 3.3.29\n\n\n2017-09-25\n\n\n2017-10-02\n\n\n2017-10-10\n\n\n\n\n\n\n\n\nNovember\n\n\n3.4.5, 3.3.30\n\n\n2017-10-30\n\n\n2017-11-06\n\n\n2017-11-14\n\n\n5 week cycle\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\n3.4.3/3.3.28\n\n\n\n\nTickets not marked RFT\n\n\n\n\n\n\n\n\n\n\nOwner\n\n\n# tickets\n\n\n\n\n\n\n\n\n\n\nMat\n\n\n7\n\n\n\n\n\n\nBrian\n\n\n1\n\n\n\n\n\n\nCarl\n\n\n3\n\n\n\n\n\n\nTim\n\n\n1\n\n\n\n\n\n\n\n\nDocumentation\n\n\nDiscussions\n\n\nNew 8.7 Condor build might have to be punted to October.  We might still release a new 8.6 this month -- have to look at OSG's demands to see if it's worth releasing.\n\n\nSupport Update\n\n\nSome issues with Florida -- Derek to investigate.\n\n\nOSG Release Team\n\n\n\n\nSuchandra Thapa is handling the \nSeptember 12th\n release\n\n\nPackage Freeze today!\n\n\nTimT may call upon software team members for testing assistance\n\n\n\n\n\n\n\n\n\n\n3.3.28\n\n\n\n\nBoth\n\n\n\n\n3.4.3\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n1\n\n\n-1\n\n\n2\n\n\n-7\n\n\n0\n\n\n+0\n\n\n3\n\n\n-8\n\n\nOpen\n\n\n\n\n\n\n1\n\n\n-2\n\n\n6\n\n\n-2\n\n\n2\n\n\n+1\n\n\n9\n\n\n-3\n\n\nIn Progress\n\n\n\n\n\n\n7\n\n\n+2\n\n\n10\n\n\n+4\n\n\n4\n\n\n+1\n\n\n21\n\n\n+7\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n1\n\n\n+1\n\n\n0\n\n\n+0\n\n\n1\n\n\n+1\n\n\nReady for Release\n\n\n\n\n\n\n9\n\n\n-1\n\n\n19\n\n\n-4\n\n\n6\n\n\n+2\n\n\n34\n\n\n-3\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\nBoth  \n\n\nStashCache 0.8 (\nSOFTWARE-2873\n)\n\n\nosg-ca-scripts 1.1.7 (\nSOFTWARE-2834\n)\n\n\nxrootd-lcmaps 1.3.4 (\nSOFTWARE-2847\n)\n\n\n\n\n\n\n3.4.3  \n\n\nSingularity 2.3 (\nSOFTWARE-2755\n)\n\n\nCVMFS 2.4.1 (\nSOFTWARE-2858\n)\n\n\nosg-configure 2.2.0 (\nSOFTWARE-2864\n)\n\n\n\n\n\n\n3.3.28  \n\n\nosg-configure 1.10.0 (\nSOFTWARE-2865\n)\n\n\nxrootd-hdfs 1.9.2 (\nSOFTWARE-2853\n)\n\n\n\n\n\n\n\n\nDiscussions\n\n\nWill need extra help for testing.  Nebraska will help test the Globus update for 3.4, but need a 3.3 site to test the 3.3 updates -- should contact osg-sites.\n\n\nSuchandra may test gridftp-hdfs but Matyas still needs to debug \n promote.\n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nGRACC backup of Dashboards\n\n\nInitiate backups of ES snapshots\n\n\nStart indexing GOC server status in GRACC ES\n\n\nWork no Glidein logs in ES\n\n\nFix naming issues in GRACC related to explosion of \"Fake\" sites in records.\n\n\n\n\nThis Week\n\n\n\n\nSome corruption in GRACC\n\n\nTesting GRACC update\n\n\nStashcache XRootD update going smoothly\n\n\nSome minor issues at Syracuse\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)\n\n\n\n\nDiscussions", 
            "title": "September 5, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170905/#osg-technology-area-meeting-5-september-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin  Attending:  Edgar, Derek, Tim C, Tim T, Carl, Suchandra", 
            "title": "OSG Technology Area Meeting, 5 September 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170905/#announcements", 
            "text": "", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170905/#triage-duty", 
            "text": "This week: Derek  Next week: Brian Lin  9 (+0) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170905/#jira", 
            "text": "# of tickets   State      152  -4  Open    19  -1  In Progress    21  +7  Ready for Testing    1  +1  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170905/#release-schedule", 
            "text": "Name  Version  Development Freeze  Package Freeze  Release  Notes      September  3.4.3, 3.3.28  2017-08-28  2017-09-05  2017-09-12  5 week cycle    October  3.4.4, 3.3.29  2017-09-25  2017-10-02  2017-10-10     November  3.4.5, 3.3.30  2017-10-30  2017-11-06  2017-11-14  5 week cycle     Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170905/#osg-software-team", 
            "text": "", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170905/#3433328", 
            "text": "Tickets not marked RFT      Owner  # tickets      Mat  7    Brian  1    Carl  3    Tim  1", 
            "title": "3.4.3/3.3.28"
        }, 
        {
            "location": "/meetings/2017/TechArea20170905/#documentation", 
            "text": "", 
            "title": "Documentation"
        }, 
        {
            "location": "/meetings/2017/TechArea20170905/#discussions", 
            "text": "New 8.7 Condor build might have to be punted to October.  We might still release a new 8.6 this month -- have to look at OSG's demands to see if it's worth releasing.", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170905/#support-update", 
            "text": "Some issues with Florida -- Derek to investigate.", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170905/#osg-release-team", 
            "text": "Suchandra Thapa is handling the  September 12th  release  Package Freeze today!  TimT may call upon software team members for testing assistance      3.3.28   Both   3.4.3   Total   Status      1  -1  2  -7  0  +0  3  -8  Open    1  -2  6  -2  2  +1  9  -3  In Progress    7  +2  10  +4  4  +1  21  +7  Ready for Testing    0  +0  1  +1  0  +0  1  +1  Ready for Release    9  -1  19  -4  6  +2  34  -3  Total      Both    StashCache 0.8 ( SOFTWARE-2873 )  osg-ca-scripts 1.1.7 ( SOFTWARE-2834 )  xrootd-lcmaps 1.3.4 ( SOFTWARE-2847 )    3.4.3    Singularity 2.3 ( SOFTWARE-2755 )  CVMFS 2.4.1 ( SOFTWARE-2858 )  osg-configure 2.2.0 ( SOFTWARE-2864 )    3.3.28    osg-configure 1.10.0 ( SOFTWARE-2865 )  xrootd-hdfs 1.9.2 ( SOFTWARE-2853 )", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170905/#discussions_1", 
            "text": "Will need extra help for testing.  Nebraska will help test the Globus update for 3.4, but need a 3.3 site to test the 3.3 updates -- should contact osg-sites.  Suchandra may test gridftp-hdfs but Matyas still needs to debug   promote.", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170905/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170905/#last-week", 
            "text": "GRACC backup of Dashboards  Initiate backups of ES snapshots  Start indexing GOC server status in GRACC ES  Work no Glidein logs in ES  Fix naming issues in GRACC related to explosion of \"Fake\" sites in records.", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170905/#this-week", 
            "text": "Some corruption in GRACC  Testing GRACC update  Stashcache XRootD update going smoothly  Some minor issues at Syracuse", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170905/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170905/#discussions_2", 
            "text": "", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170911/", 
            "text": "OSG Technology Area Meeting, 11 September 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n\n\nAttending:\n BrianL, Carl, Derek, Edgar, Marian, Mat, TimC, TimT  \n\n\nAnnouncements\n\n\nWiFi issues at UW-Madison today so availability may be spotty  \n\n\nTriage Duty\n\n\n\n\nThis week: BrianL\n\n\nNext week: Suchandra\n\n\n10 (+1) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n153\n\n\n-1\n\n\nOpen\n\n\n\n\n\n\n17\n\n\n-2\n\n\nIn Progress\n\n\n\n\n\n\n6\n\n\n-15\n\n\nReady for Testing\n\n\n\n\n\n\n20\n\n\n+19\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nName\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nSeptember\n\n\n3.4.3, 3.3.28\n\n\n2017-08-28\n\n\n2017-09-05\n\n\n2017-09-12\n\n\n5 week cycle\n\n\n\n\n\n\nOctober\n\n\n3.4.4, 3.3.29\n\n\n2017-09-25\n\n\n2017-10-02\n\n\n2017-10-10\n\n\n\n\n\n\n\n\nNovember\n\n\n3.4.5, 3.3.30\n\n\n2017-10-30\n\n\n2017-11-06\n\n\n2017-11-14\n\n\n5 week cycle\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\n3.4.3/3.3.28\n\n\n\n\nTickets not marked RFT\n\n\n\n\n\n\n\n\n\n\nOwner\n\n\n# tickets\n\n\n\n\n\n\n\n\n\n\nMat\n\n\n10\n\n\n\n\n\n\nBrian\n\n\n7\n\n\n\n\n\n\nCarl\n\n\n5\n\n\n\n\n\n\nMarian\n\n\n1\n\n\n\n\n\n\n\n\nDocumentation\n\n\n\n\nCarl has written a doc migration/archival wrapper for Software  \n\n\nThere were concerns about pandoc hanging so it's not yet foolproof as an area mass migration tool\n\n\nCarl will work on robustness so that other areas can benefit\n\n\n\n\n\n\nDerek to speak to his two students this week about their progress with migrating docs\n\n\n\n\nITB\n\n\n\n\nHTCondor 8.6.6 pre-release is installed on an ITB CE and some worker nodes\n\n\nBrianL will contact factory ops to add Madison ITB entries to the ITB factory\n\n\n\n\nDiscussions\n\n\nNone this week\n\n\nSupport Update\n\n\n\n\nBaylor (BrianL): Fixed issues with edg-mkgridmap -\n LCMAPS VOMS transition due to an old version of HTCondor-CE. Docs to be updated.\n\n\nClemson (BrianL): Blahp using too much CPU and segfaulting - need to talk to Jaime\n\n\n\n\nOSG Release Team\n\n\n\n\nTim Theisen is handling the \nSeptember 12th\n release\n\n\nRelease tomorrow???\n\n\n\n\n\n\n\n\n\n\n3.3.28\n\n\n\n\nBoth\n\n\n\n\n3.4.3\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n1\n\n\n+0\n\n\n0\n\n\n-2\n\n\n0\n\n\n+0\n\n\n1\n\n\n-2\n\n\nOpen\n\n\n\n\n\n\n0\n\n\n-1\n\n\n0\n\n\n-6\n\n\n0\n\n\n-2\n\n\n0\n\n\n-9\n\n\nIn Progress\n\n\n\n\n\n\n3\n\n\n-4\n\n\n3\n\n\n-7\n\n\n0\n\n\n-2\n\n\n6\n\n\n-15\n\n\nReady for Testing\n\n\n\n\n\n\n5\n\n\n+5\n\n\n11\n\n\n+10\n\n\n4\n\n\n+4\n\n\n20\n\n\n+19\n\n\nReady for Release\n\n\n\n\n\n\n9\n\n\n+0\n\n\n14\n\n\n-5\n\n\n4\n\n\n-2\n\n\n27\n\n\n-7\n\n\nTotal\n\n\n\n\n\n\n\n\nTickets needing attention\n\n\n\n\nBoth\n\n\nRelease StashCache metapackage 0.8+ (\nSOFTWARE-2873\n)\n\n\nMigrate transfer limit code from HDFS to generic plugin (\nSOFTWARE-2512\n)\n\n\nPackage and release xrootd-lcmaps 1.3.4 (\nSOFTWARE-2847\n)\n\n\n\n\n\n\n3.4.3\n\n\n3.3.28\n\n\nLCMAPS VOMS plugin and xrootd-lcmaps  (\nSOFTWARE-2848\n)\n\n\nUpdate to xrootd-hdfs 1.9.2 (\nSOFTWARE-2853\n)\n\n\nMigrate GridFTP-HDFS from Globus-Toolkit back to OSG (\nSOFTWARE-2856\n)\n\n\n\n\n\n\n\n\nDiscussions\n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nSome GRACC support of a \nslurm_meter\n issue, still debugging.\n\n\nIndexing Glidein Logs in GRACC's ES - Ongoing.  We now have worker node hostnames!\n\n\nFirst step of creating ES snapshots for backup up GRACC - Ongoing.\n\n\nFixed explosion of \"Fake\" sites in records.  Also fixed incorrect user VOs\n\n\n\n\nThis Week\n\n\n\n\nInitiate backups of ES snapshots\n\n\nGRACC-ITB work\n\n\nStart indexing GOC server status in GRACC ES\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)\n\n\n\n\nDiscussions\n\n\nNone this week", 
            "title": "September 11, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170911/#osg-technology-area-meeting-11-september-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin  Attending:  BrianL, Carl, Derek, Edgar, Marian, Mat, TimC, TimT", 
            "title": "OSG Technology Area Meeting, 11 September 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170911/#announcements", 
            "text": "WiFi issues at UW-Madison today so availability may be spotty", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170911/#triage-duty", 
            "text": "This week: BrianL  Next week: Suchandra  10 (+1) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170911/#jira", 
            "text": "# of tickets   State      153  -1  Open    17  -2  In Progress    6  -15  Ready for Testing    20  +19  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170911/#release-schedule", 
            "text": "Name  Version  Development Freeze  Package Freeze  Release  Notes      September  3.4.3, 3.3.28  2017-08-28  2017-09-05  2017-09-12  5 week cycle    October  3.4.4, 3.3.29  2017-09-25  2017-10-02  2017-10-10     November  3.4.5, 3.3.30  2017-10-30  2017-11-06  2017-11-14  5 week cycle     Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170911/#osg-software-team", 
            "text": "", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170911/#3433328", 
            "text": "Tickets not marked RFT      Owner  # tickets      Mat  10    Brian  7    Carl  5    Marian  1", 
            "title": "3.4.3/3.3.28"
        }, 
        {
            "location": "/meetings/2017/TechArea20170911/#documentation", 
            "text": "Carl has written a doc migration/archival wrapper for Software    There were concerns about pandoc hanging so it's not yet foolproof as an area mass migration tool  Carl will work on robustness so that other areas can benefit    Derek to speak to his two students this week about their progress with migrating docs", 
            "title": "Documentation"
        }, 
        {
            "location": "/meetings/2017/TechArea20170911/#itb", 
            "text": "HTCondor 8.6.6 pre-release is installed on an ITB CE and some worker nodes  BrianL will contact factory ops to add Madison ITB entries to the ITB factory", 
            "title": "ITB"
        }, 
        {
            "location": "/meetings/2017/TechArea20170911/#discussions", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170911/#support-update", 
            "text": "Baylor (BrianL): Fixed issues with edg-mkgridmap -  LCMAPS VOMS transition due to an old version of HTCondor-CE. Docs to be updated.  Clemson (BrianL): Blahp using too much CPU and segfaulting - need to talk to Jaime", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170911/#osg-release-team", 
            "text": "Tim Theisen is handling the  September 12th  release  Release tomorrow???      3.3.28   Both   3.4.3   Total   Status      1  +0  0  -2  0  +0  1  -2  Open    0  -1  0  -6  0  -2  0  -9  In Progress    3  -4  3  -7  0  -2  6  -15  Ready for Testing    5  +5  11  +10  4  +4  20  +19  Ready for Release    9  +0  14  -5  4  -2  27  -7  Total     Tickets needing attention   Both  Release StashCache metapackage 0.8+ ( SOFTWARE-2873 )  Migrate transfer limit code from HDFS to generic plugin ( SOFTWARE-2512 )  Package and release xrootd-lcmaps 1.3.4 ( SOFTWARE-2847 )    3.4.3  3.3.28  LCMAPS VOMS plugin and xrootd-lcmaps  ( SOFTWARE-2848 )  Update to xrootd-hdfs 1.9.2 ( SOFTWARE-2853 )  Migrate GridFTP-HDFS from Globus-Toolkit back to OSG ( SOFTWARE-2856 )", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170911/#discussions_1", 
            "text": "", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170911/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170911/#last-week", 
            "text": "Some GRACC support of a  slurm_meter  issue, still debugging.  Indexing Glidein Logs in GRACC's ES - Ongoing.  We now have worker node hostnames!  First step of creating ES snapshots for backup up GRACC - Ongoing.  Fixed explosion of \"Fake\" sites in records.  Also fixed incorrect user VOs", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170911/#this-week", 
            "text": "Initiate backups of ES snapshots  GRACC-ITB work  Start indexing GOC server status in GRACC ES", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170911/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170911/#discussions_2", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170918/", 
            "text": "OSG Technology Area Meeting, 18 September 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n\n\nAttending:\n   \n\n\nAnnouncements\n\n\nTriage Duty\n\n\n\n\nThis week: Suchandra\n\n\nNext week: TimT\n\n\n10 (+0) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n162\n\n\n+9\n\n\nOpen\n\n\n\n\n\n\n19\n\n\n+2\n\n\nIn Progress\n\n\n\n\n\n\n1\n\n\n-5\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n-20\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nName\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nOctober\n\n\n3.4.4, 3.3.29\n\n\n2017-09-25\n\n\n2017-10-02\n\n\n2017-10-10\n\n\n\n\n\n\n\n\nNovember\n\n\n3.4.5, 3.3.30\n\n\n2017-10-30\n\n\n2017-11-06\n\n\n2017-11-14\n\n\n5 week cycle\n\n\n\n\n\n\nDecember\n\n\n3.4.6, 3.3.31\n\n\n2017-11-27\n\n\n2017-12-04\n\n\n2017-12-12\n\n\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\nosghost downtime starts tomorrow at 2pm Central  \n\n\nDocumentation\n\n\n\n\nhttps://github.com/opensciencegrid/docs/pulse#new-issues\n  \n\n\n7 docs fully migrated (many more awaiting review!), \n100 docs archived\n\n\nRelease3: 17 high-priority and ~30 low-priority docs remaining\n\n\nSoftwareTeam: ~56 docs remaining\n\n\n\n\n\n\nCarl's mass migration and archival wrapper tools are robust enough to be used by other areas\n\n\n\n\nDiscussions\n\n\nNone this week  \n\n\nSupport Update\n\n\n\n\nClemson (BrianL): Blahp segfaults appeared to be due to overloaded NFS server\n\n\nCTSC (BrianL): Attended meeting to discuss HTCondor issues encountered\n\n\nLBNL (BrianL): Strange Gridmanager errors showing up intermittently\n\n\n\n\nOSG Release Team\n\n\n\n\n\n\n\n\n3.3.29\n\n\n\n\nBoth\n\n\n\n\n3.4.4\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n3\n\n\n+3\n\n\n12\n\n\n+12\n\n\n3\n\n\n+3\n\n\n18\n\n\n+18\n\n\nOpen\n\n\n\n\n\n\n0\n\n\n+0\n\n\n8\n\n\n+8\n\n\n2\n\n\n+2\n\n\n10\n\n\n+10\n\n\nIn Progress\n\n\n\n\n\n\n0\n\n\n+0\n\n\n1\n\n\n+1\n\n\n0\n\n\n+0\n\n\n1\n\n\n+1\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\n0\n\n\n+0\n\n\nReady for Release\n\n\n\n\n\n\n3\n\n\n+3\n\n\n21\n\n\n+21\n\n\n5\n\n\n+5\n\n\n29\n\n\n+29\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\nBoth\n\n\nUpdate globus-gridftp-server-control to 5.2\n\n\n\n\n\n\n3.4.4\n\n\n3.3.29\n\n\n\n\nDiscussions\n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nFirst step of creating ES snapshots for backup up GRACC - Ongoing.\n\n\nFixed explosion of \"Fake\" sites in records.  Also fixed incorrect user VOs.  Changing indexes to relefect the changes.\n\n\n\n\nThis Week\n\n\n\n\nInitiate backups of ES snapshots\n\n\nGRACC-ITB work\n\n\nStart indexing GOC server status in GRACC ES\n\n\nStash Writeback hack-a-thon on Tuesday \n Wednesday (Derek, Brian, Lincoln, Marian)\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)\n\n\n\n\nDiscussions\n\n\nStarting to setup time for Kibana walkthrough for Glidein Logs.", 
            "title": "September 18, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170918/#osg-technology-area-meeting-18-september-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin  Attending:", 
            "title": "OSG Technology Area Meeting, 18 September 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170918/#announcements", 
            "text": "", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170918/#triage-duty", 
            "text": "This week: Suchandra  Next week: TimT  10 (+0) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170918/#jira", 
            "text": "# of tickets   State      162  +9  Open    19  +2  In Progress    1  -5  Ready for Testing    0  -20  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170918/#release-schedule", 
            "text": "Name  Version  Development Freeze  Package Freeze  Release  Notes      October  3.4.4, 3.3.29  2017-09-25  2017-10-02  2017-10-10     November  3.4.5, 3.3.30  2017-10-30  2017-11-06  2017-11-14  5 week cycle    December  3.4.6, 3.3.31  2017-11-27  2017-12-04  2017-12-12      Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170918/#osg-software-team", 
            "text": "osghost downtime starts tomorrow at 2pm Central", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170918/#documentation", 
            "text": "https://github.com/opensciencegrid/docs/pulse#new-issues     7 docs fully migrated (many more awaiting review!),  100 docs archived  Release3: 17 high-priority and ~30 low-priority docs remaining  SoftwareTeam: ~56 docs remaining    Carl's mass migration and archival wrapper tools are robust enough to be used by other areas", 
            "title": "Documentation"
        }, 
        {
            "location": "/meetings/2017/TechArea20170918/#discussions", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170918/#support-update", 
            "text": "Clemson (BrianL): Blahp segfaults appeared to be due to overloaded NFS server  CTSC (BrianL): Attended meeting to discuss HTCondor issues encountered  LBNL (BrianL): Strange Gridmanager errors showing up intermittently", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170918/#osg-release-team", 
            "text": "3.3.29   Both   3.4.4   Total   Status      3  +3  12  +12  3  +3  18  +18  Open    0  +0  8  +8  2  +2  10  +10  In Progress    0  +0  1  +1  0  +0  1  +1  Ready for Testing    0  +0  0  +0  0  +0  0  +0  Ready for Release    3  +3  21  +21  5  +5  29  +29  Total      Both  Update globus-gridftp-server-control to 5.2    3.4.4  3.3.29", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170918/#discussions_1", 
            "text": "", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170918/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170918/#last-week", 
            "text": "First step of creating ES snapshots for backup up GRACC - Ongoing.  Fixed explosion of \"Fake\" sites in records.  Also fixed incorrect user VOs.  Changing indexes to relefect the changes.", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170918/#this-week", 
            "text": "Initiate backups of ES snapshots  GRACC-ITB work  Start indexing GOC server status in GRACC ES  Stash Writeback hack-a-thon on Tuesday   Wednesday (Derek, Brian, Lincoln, Marian)", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170918/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170918/#discussions_2", 
            "text": "Starting to setup time for Kibana walkthrough for Glidein Logs.", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170925/", 
            "text": "OSG Technology Area Meeting, 25 September 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n\n\nAttending:\n Tim C, Brian L, Suchandra, Marian, Edgar, Mat, Derek, Carl\n\n\nAnnouncements\n\n\nOSG All Hands 2018 at University of Utah Mar 19 - 22, 2018\n\n\nTriage Duty\n\n\n\n\nThis week: TimT\n\n\nNext week: BrianL\n\n\n11 (+1) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n155\n\n\n-7\n\n\nOpen\n\n\n\n\n\n\n20\n\n\n+1\n\n\nIn Progress\n\n\n\n\n\n\n8\n\n\n+7\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n0\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nName\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nOctober\n\n\n3.4.4, 3.3.29\n\n\n2017-09-25\n\n\n2017-10-02\n\n\n2017-10-10\n\n\n\n\n\n\n\n\nNovember\n\n\n3.4.5, 3.3.30\n\n\n2017-10-30\n\n\n2017-11-06\n\n\n2017-11-14\n\n\n5 week cycle\n\n\n\n\n\n\nDecember\n\n\n3.4.6, 3.3.31\n\n\n2017-11-27\n\n\n2017-12-04\n\n\n2017-12-12\n\n\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.\n\n\nOSG Software Team\n\n\n\n\n\n\nOpen JIRA tickets\n\n\n\n\n\n\n\n\nOwner\n\n\n# tickets not RFT\n\n\n\n\n\n\n\n\n\n\nMat\n\n\n12\n\n\n\n\n\n\nCarl\n\n\n4\n\n\n\n\n\n\nEdgar\n\n\n2\n\n\n\n\n\n\nBrianL\n\n\n1\n\n\n\n\n\n\n\n\n\n\n\n\nSoftware and release teams merged. Effort #s updated \nhere\n\n\n\n\nIt sounds like HDFS 3 isn't expected until at least April 2018\n\n\nShip the newest version of HDFS 2 in OSG 3.4\n\n\nAim for November, but don't know how much work it will take yet\n\n\n\n\n\n\n\n\nDocumentation\n\n\nhttps://github.com/opensciencegrid/docs/pulse#new-issues\n\n\nhttps://github.com/opensciencegrid/technology/pulse#new-issues\n\n\n\n\nOnly 3 docs fully migrated, ~7 docs awaiting review\n\n\nRelease3: ~45 docs remaining\n\n\nSoftwareTeam: ~50 docs remaining\n\n\n\n\nDiscussions\n\n\n\n\nSoftware/Release merger finalized -- see front page of tech area docs\n\n\nSoftware folks should expect to do more testing; release folks might have to help out with software updates\n\n\n\n\nSupport Update\n\n\n\n\nLBNL (BrianL): Strange Gridmanager errors showing up intermittently; asked them to update to condor 8.6\n\n\n\n\nOSG Release Team\n\n\n\n\n\n\n\n\n3.3.29\n\n\n\n\nBoth\n\n\n\n\n3.4.4\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n0\n\n\n-3\n\n\n2\n\n\n-10\n\n\n0\n\n\n-3\n\n\n2\n\n\n-16\n\n\nOpen\n\n\n\n\n\n\n1\n\n\n+1\n\n\n6\n\n\n-2\n\n\n0\n\n\n-2\n\n\n7\n\n\n-3\n\n\nIn Progress\n\n\n\n\n\n\n2\n\n\n+2\n\n\n5\n\n\n+4\n\n\n4\n\n\n+4\n\n\n11\n\n\n+10\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n1\n\n\n+1\n\n\n1\n\n\n+1\n\n\n2\n\n\n+2\n\n\nReady for Release\n\n\n\n\n\n\n3\n\n\n+0\n\n\n14\n\n\n-7\n\n\n5\n\n\n+0\n\n\n22\n\n\n-7\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\nBoth\n\n\nUpdate globus-gridftp-server-control to 5.2\n\n\nDon't use mirrors for goc repos\n\n\nosg-ca-scripts: require wget\n\n\nosg-configure: Detect when fetch-crl missing\n\n\nosg-configure: don't use condor_config_val -expand\n\n\n\n\n\n\n3.4.4\n\n\nUpdate to HTCondor 8.6.6 in OSG 3.4\n\n\nUpdate to HTCondor 8.7.3 in Upcoming\n\n\nUpdate to singularity-2.3.2+\n\n\nAdd singularity to osg-tested-internal\n\n\nosg-configure: Release 2.2.1\n\n\n\n\n\n\n3.3.29\n\n\nRelease voms-admin-server-2.7.0-1.23+\n\n\nosg-configure: Release 1.10.1\n\n\n\n\n\n\n\n\nDiscussions\n\n\nNone this week\n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nEnabling writeback of Stash\n\n\nExtra backups made of GRACC records\n\n\nAPEL report issues related to firewall; issues were fixed and a check_mk report was added\n\n\n\n\nThis Week\n\n\n\n\nStill working on cleaning up GRACC data\n\n\nFinish writeback of Stash\n\n\nBetter monitoring of APEL uploads\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)\n\n\n\n\nDiscussions\n\n\nStarting to setup time for Kibana walkthrough for Glidein Logs", 
            "title": "September 25, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170925/#osg-technology-area-meeting-25-september-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin  Attending:  Tim C, Brian L, Suchandra, Marian, Edgar, Mat, Derek, Carl", 
            "title": "OSG Technology Area Meeting, 25 September 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20170925/#announcements", 
            "text": "OSG All Hands 2018 at University of Utah Mar 19 - 22, 2018", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20170925/#triage-duty", 
            "text": "This week: TimT  Next week: BrianL  11 (+1) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20170925/#jira", 
            "text": "# of tickets   State      155  -7  Open    20  +1  In Progress    8  +7  Ready for Testing    0  0  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20170925/#release-schedule", 
            "text": "Name  Version  Development Freeze  Package Freeze  Release  Notes      October  3.4.4, 3.3.29  2017-09-25  2017-10-02  2017-10-10     November  3.4.5, 3.3.30  2017-10-30  2017-11-06  2017-11-14  5 week cycle    December  3.4.6, 3.3.31  2017-11-27  2017-12-04  2017-12-12      Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20170925/#osg-software-team", 
            "text": "Open JIRA tickets     Owner  # tickets not RFT      Mat  12    Carl  4    Edgar  2    BrianL  1       Software and release teams merged. Effort #s updated  here   It sounds like HDFS 3 isn't expected until at least April 2018  Ship the newest version of HDFS 2 in OSG 3.4  Aim for November, but don't know how much work it will take yet", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170925/#documentation", 
            "text": "https://github.com/opensciencegrid/docs/pulse#new-issues  https://github.com/opensciencegrid/technology/pulse#new-issues   Only 3 docs fully migrated, ~7 docs awaiting review  Release3: ~45 docs remaining  SoftwareTeam: ~50 docs remaining", 
            "title": "Documentation"
        }, 
        {
            "location": "/meetings/2017/TechArea20170925/#discussions", 
            "text": "Software/Release merger finalized -- see front page of tech area docs  Software folks should expect to do more testing; release folks might have to help out with software updates", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170925/#support-update", 
            "text": "LBNL (BrianL): Strange Gridmanager errors showing up intermittently; asked them to update to condor 8.6", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20170925/#osg-release-team", 
            "text": "3.3.29   Both   3.4.4   Total   Status      0  -3  2  -10  0  -3  2  -16  Open    1  +1  6  -2  0  -2  7  -3  In Progress    2  +2  5  +4  4  +4  11  +10  Ready for Testing    0  +0  1  +1  1  +1  2  +2  Ready for Release    3  +0  14  -7  5  +0  22  -7  Total      Both  Update globus-gridftp-server-control to 5.2  Don't use mirrors for goc repos  osg-ca-scripts: require wget  osg-configure: Detect when fetch-crl missing  osg-configure: don't use condor_config_val -expand    3.4.4  Update to HTCondor 8.6.6 in OSG 3.4  Update to HTCondor 8.7.3 in Upcoming  Update to singularity-2.3.2+  Add singularity to osg-tested-internal  osg-configure: Release 2.2.1    3.3.29  Release voms-admin-server-2.7.0-1.23+  osg-configure: Release 1.10.1", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170925/#discussions_1", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20170925/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20170925/#last-week", 
            "text": "Enabling writeback of Stash  Extra backups made of GRACC records  APEL report issues related to firewall; issues were fixed and a check_mk report was added", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170925/#this-week", 
            "text": "Still working on cleaning up GRACC data  Finish writeback of Stash  Better monitoring of APEL uploads", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20170925/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20170925/#discussions_2", 
            "text": "Starting to setup time for Kibana walkthrough for Glidein Logs", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171002/", 
            "text": "OSG Technology Area Meeting, 2 October 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n\n\nAttending:\n Mat, Carl, Edgar, Suchandra, Derek, Marian, BrianB\n\n\nAnnouncements\n\n\nTriage Duty\n\n\n\n\nThis week: Carl\n\n\nNext week: BrianL\n\n\n10 (-1) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n159\n\n\n+4\n\n\nOpen\n\n\n\n\n\n\n23\n\n\n+3\n\n\nIn Progress\n\n\n\n\n\n\n12\n\n\n+4\n\n\nReady for Testing\n\n\n\n\n\n\n2\n\n\n+2\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nName\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nOctober\n\n\n3.4.4, 3.3.29\n\n\n2017-09-25\n\n\n2017-10-02\n\n\n2017-10-10\n\n\n\n\n\n\n\n\nNovember\n\n\n3.4.5, 3.3.30\n\n\n2017-10-30\n\n\n2017-11-06\n\n\n2017-11-14\n\n\n5 week cycle\n\n\n\n\n\n\nDecember\n\n\n3.4.6, 3.3.31\n\n\n2017-11-27\n\n\n2017-12-04\n\n\n2017-12-12\n\n\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.\n\n\nOSG Software Team\n\n\n\n\n\n\nOpen JIRA tickets\n\n\n\n\n\n\n\n\nOwner\n\n\n# tickets not RFT\n\n\n\n\n\n\n\n\n\n\nBrianL\n\n\n5\n\n\n\n\n\n\nMat\n\n\n4\n\n\n\n\n\n\nEdgar\n\n\n2\n\n\n\n\n\n\nCarl\n\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\nSoftware and release teams merged. Effort #s updated \nhere\n\n\n\n\n\n\nDocumentation\n\n\nhttps://github.com/opensciencegrid/docs/pulse#new-issues\n\n\nhttps://github.com/opensciencegrid/technology/pulse#new-issues\n\n\n\n\nHold off on these for now\n\n\n\n\nDiscussions\n\n\nSupport Update\n\n\n\n\nStill debugging XRootD issues at Florida; possibly found leaks?\n\n\n\n\nOSG Release Team\n\n\n\n\n\n\n\n\n3.3.29\n\n\n\n\nBoth\n\n\n\n\n3.4.4\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n0\n\n\n-3\n\n\n2\n\n\n-10\n\n\n0\n\n\n-3\n\n\n2\n\n\n-16\n\n\nOpen\n\n\n\n\n\n\n1\n\n\n+1\n\n\n6\n\n\n-2\n\n\n0\n\n\n-2\n\n\n7\n\n\n-3\n\n\nIn Progress\n\n\n\n\n\n\n2\n\n\n+2\n\n\n5\n\n\n+4\n\n\n4\n\n\n+4\n\n\n11\n\n\n+10\n\n\nReady for Testing\n\n\n\n\n\n\n0\n\n\n+0\n\n\n1\n\n\n+1\n\n\n1\n\n\n+1\n\n\n2\n\n\n+2\n\n\nReady for Release\n\n\n\n\n\n\n3\n\n\n+0\n\n\n14\n\n\n-7\n\n\n5\n\n\n+0\n\n\n22\n\n\n-7\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\nBoth\n\n\nUpdate globus-gridftp-server-control to 5.2\n\n\nDon't use mirrors for goc repos\n\n\nosg-ca-scripts: require wget\n\n\nosg-configure: Detect when fetch-crl missing\n\n\nosg-configure: don't use condor_config_val -expand\n\n\n\n\n\n\n3.4.4\n\n\nUpdate to HTCondor 8.6.6 in OSG 3.4\n\n\nUpdate to HTCondor 8.7.3 in Upcoming\n\n\nUpdate to singularity-2.3.2+\n\n\nAdd singularity to osg-tested-internal\n\n\nosg-configure: Release 2.2.1\n\n\n\n\n\n\n3.3.29\n\n\nRelease voms-admin-server-2.7.0-1.23+\n\n\nosg-configure: Release 1.10.1\n\n\n\n\n\n\n\n\nDiscussions\n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nWorked on cleaning up GRACC data\n\n\n\n\nThis Week\n\n\n\n\nFinalizing GRACC changes\n\n\nHope to have writeable StashCache avail. for early testers; waiting for Condor bugfix\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project (New URL!)\n\n\n\n\nDiscussions", 
            "title": "October 2, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20171002/#osg-technology-area-meeting-2-october-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin  Attending:  Mat, Carl, Edgar, Suchandra, Derek, Marian, BrianB", 
            "title": "OSG Technology Area Meeting, 2 October 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20171002/#announcements", 
            "text": "", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20171002/#triage-duty", 
            "text": "This week: Carl  Next week: BrianL  10 (-1) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20171002/#jira", 
            "text": "# of tickets   State      159  +4  Open    23  +3  In Progress    12  +4  Ready for Testing    2  +2  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20171002/#release-schedule", 
            "text": "Name  Version  Development Freeze  Package Freeze  Release  Notes      October  3.4.4, 3.3.29  2017-09-25  2017-10-02  2017-10-10     November  3.4.5, 3.3.30  2017-10-30  2017-11-06  2017-11-14  5 week cycle    December  3.4.6, 3.3.31  2017-11-27  2017-12-04  2017-12-12      Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20171002/#osg-software-team", 
            "text": "Open JIRA tickets     Owner  # tickets not RFT      BrianL  5    Mat  4    Edgar  2    Carl  0       Software and release teams merged. Effort #s updated  here", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171002/#documentation", 
            "text": "https://github.com/opensciencegrid/docs/pulse#new-issues  https://github.com/opensciencegrid/technology/pulse#new-issues   Hold off on these for now", 
            "title": "Documentation"
        }, 
        {
            "location": "/meetings/2017/TechArea20171002/#discussions", 
            "text": "", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171002/#support-update", 
            "text": "Still debugging XRootD issues at Florida; possibly found leaks?", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20171002/#osg-release-team", 
            "text": "3.3.29   Both   3.4.4   Total   Status      0  -3  2  -10  0  -3  2  -16  Open    1  +1  6  -2  0  -2  7  -3  In Progress    2  +2  5  +4  4  +4  11  +10  Ready for Testing    0  +0  1  +1  1  +1  2  +2  Ready for Release    3  +0  14  -7  5  +0  22  -7  Total      Both  Update globus-gridftp-server-control to 5.2  Don't use mirrors for goc repos  osg-ca-scripts: require wget  osg-configure: Detect when fetch-crl missing  osg-configure: don't use condor_config_val -expand    3.4.4  Update to HTCondor 8.6.6 in OSG 3.4  Update to HTCondor 8.7.3 in Upcoming  Update to singularity-2.3.2+  Add singularity to osg-tested-internal  osg-configure: Release 2.2.1    3.3.29  Release voms-admin-server-2.7.0-1.23+  osg-configure: Release 1.10.1", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171002/#discussions_1", 
            "text": "", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171002/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171002/#last-week", 
            "text": "Worked on cleaning up GRACC data", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20171002/#this-week", 
            "text": "Finalizing GRACC changes  Hope to have writeable StashCache avail. for early testers; waiting for Condor bugfix", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20171002/#ongoing", 
            "text": "GRACC Project  StashCache Project (New URL!)", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20171002/#discussions_2", 
            "text": "", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171009/", 
            "text": "OSG Technology Area Meeting,  9 October 2017\n\n\nCoordinates:\n Conference: 719-284-5267, PIN: 57363; \nhttps://www.uberconference.com/osgblin\n\n\nAttending:\n BrianL, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT   \n\n\nAnnouncements\n\n\nTriage Duty\n\n\n\n\nThis week: BrianL\n\n\nNext week: Derek\n\n\n12 (+1) open tickets\n\n\n\n\nJIRA\n\n\n\n\n\n\n\n\n# of tickets\n\n\n\n\nState\n\n\n\n\n\n\n\n\n\n\n166\n\n\n+7\n\n\nOpen\n\n\n\n\n\n\n25\n\n\n+2\n\n\nIn Progress\n\n\n\n\n\n\n1\n\n\n-11\n\n\nReady for Testing\n\n\n\n\n\n\n17\n\n\n+15\n\n\nReady for Release\n\n\n\n\n\n\n\n\nRelease Schedule\n\n\n\n\n\n\n\n\nName\n\n\nVersion\n\n\nDevelopment Freeze\n\n\nPackage Freeze\n\n\nRelease\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nOctober\n\n\n3.4.4, 3.3.29\n\n\n2017-09-25\n\n\n2017-10-02\n\n\n2017-10-10\n\n\n\n\n\n\n\n\nNovember\n\n\n3.4.5, 3.3.30\n\n\n2017-10-30\n\n\n2017-11-06\n\n\n2017-11-14\n\n\n5 week cycle\n\n\n\n\n\n\nDecember\n\n\n3.4.6, 3.3.31\n\n\n2017-11-27\n\n\n2017-12-04\n\n\n2017-12-12\n\n\n\n\n\n\n\n\n\n\nNotes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.  \n\n\nOSG Software Team\n\n\nDocumentation transition on temporary hold as we improve the migration process  \n\n\nDiscussions\n\n\nNone this week  \n\n\nSupport Update\n\n\nNone last week  \n\n\nOSG Release Team\n\n\n\n\nTim Theisen is handling the October Release\n\n\nVO Package v75 needs testing by CMS\n\n\nIGTF Released today\n\n\n\n\n\n\n\n\n\n\n3.3.29\n\n\n\n\nBoth\n\n\n\n\n3.4.4\n\n\n\n\nTotal\n\n\n\n\nStatus\n\n\n\n\n\n\n\n\n\n\n0\n\n\n+0\n\n\n0\n\n\n-2\n\n\n0\n\n\n+0\n\n\n0\n\n\n-2\n\n\nOpen\n\n\n\n\n\n\n0\n\n\n-1\n\n\n0\n\n\n-6\n\n\n0\n\n\n+0\n\n\n0\n\n\n-7\n\n\nIn Progress\n\n\n\n\n\n\n0\n\n\n-2\n\n\n0\n\n\n+5\n\n\n0\n\n\n-4\n\n\n0\n\n\n-11\n\n\nReady for Testing\n\n\n\n\n\n\n2\n\n\n+2\n\n\n10\n\n\n+9\n\n\n5\n\n\n+4\n\n\n17\n\n\n+15\n\n\nReady for Release\n\n\n\n\n\n\n2\n\n\n-1\n\n\n10\n\n\n-4\n\n\n5\n\n\n+0\n\n\n17\n\n\n-5\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\nBoth\n\n\nUpdate globus-gridftp-server-control to 5.2\n\n\nDon't use mirrors for goc repos\n\n\nosg-ca-scripts: require wget\n\n\nosg-configure: Detect when fetch-crl missing\n\n\nosg-configure: don't use condor_config_val -expand\n\n\ngsi-openssh-server update for LIGO\n\n\n\n\n\n\n3.4.4\n\n\nUpdate to HTCondor 8.6.6 in OSG 3.4\n\n\nUpdate to HTCondor 8.7.3 in Upcoming\n\n\nUpdate to singularity-2.3.2+\n\n\nAdd singularity to osg-tested-internal\n\n\nosg-configure: Release 2.2.1\n\n\n\n\n\n\n3.3.29\n\n\nRelease voms-admin-server-2.7.0-1.23+\n\n\nosg-configure: Release 1.10.1\n\n\n\n\n\n\n\n\nDiscussions\n\n\n\n\nDerek and Marian will get a tester from CMS for the vo-client package\n\n\n\n\nOSG Investigations Team\n\n\nLast Week\n\n\n\n\nWorked on cleaning up GRACC data\n\n\nComparing cleaned GRACC Data\n\n\nMore work on writable stashcache, still waiting on bugfix\n\n\n\n\nThis Week\n\n\n\n\nFinalizing GRACC changes\n\n\nHope to have writeable StashCache avail. for early testers; waiting for Condor bugfix\n\n\nOSG - CVMFS focus day to improve the \n*.osgstorage\n and \nsingularity.opensciencegrid.org\n repo stability.\n\n\n\n\nOngoing\n\n\n\n\nGRACC Project\n\n\nStashCache Project\n\n\n\n\nDiscussions\n\n\nNone this week", 
            "title": "October 9, 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20171009/#osg-technology-area-meeting-9-october-2017", 
            "text": "Coordinates:  Conference: 719-284-5267, PIN: 57363;  https://www.uberconference.com/osgblin  Attending:  BrianL, Derek, Edgar, Marian, Mat, Suchandra, TimC, TimT", 
            "title": "OSG Technology Area Meeting,  9 October 2017"
        }, 
        {
            "location": "/meetings/2017/TechArea20171009/#announcements", 
            "text": "", 
            "title": "Announcements"
        }, 
        {
            "location": "/meetings/2017/TechArea20171009/#triage-duty", 
            "text": "This week: BrianL  Next week: Derek  12 (+1) open tickets", 
            "title": "Triage Duty"
        }, 
        {
            "location": "/meetings/2017/TechArea20171009/#jira", 
            "text": "# of tickets   State      166  +7  Open    25  +2  In Progress    1  -11  Ready for Testing    17  +15  Ready for Release", 
            "title": "JIRA"
        }, 
        {
            "location": "/meetings/2017/TechArea20171009/#release-schedule", 
            "text": "Name  Version  Development Freeze  Package Freeze  Release  Notes      October  3.4.4, 3.3.29  2017-09-25  2017-10-02  2017-10-10     November  3.4.5, 3.3.30  2017-10-30  2017-11-06  2017-11-14  5 week cycle    December  3.4.6, 3.3.31  2017-11-27  2017-12-04  2017-12-12      Notes: Additional \u201curgent\u201d releases may be scheduled for the 4th Tuesday of each month. The Testing date is when acceptance testing will be scheduled for releasable packages; if a package is added after this date, it may not be possible to schedule adequate testing time, thereby forcing it into the next release.", 
            "title": "Release Schedule"
        }, 
        {
            "location": "/meetings/2017/TechArea20171009/#osg-software-team", 
            "text": "Documentation transition on temporary hold as we improve the migration process", 
            "title": "OSG Software Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171009/#discussions", 
            "text": "None this week", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171009/#support-update", 
            "text": "None last week", 
            "title": "Support Update"
        }, 
        {
            "location": "/meetings/2017/TechArea20171009/#osg-release-team", 
            "text": "Tim Theisen is handling the October Release  VO Package v75 needs testing by CMS  IGTF Released today      3.3.29   Both   3.4.4   Total   Status      0  +0  0  -2  0  +0  0  -2  Open    0  -1  0  -6  0  +0  0  -7  In Progress    0  -2  0  +5  0  -4  0  -11  Ready for Testing    2  +2  10  +9  5  +4  17  +15  Ready for Release    2  -1  10  -4  5  +0  17  -5  Total      Both  Update globus-gridftp-server-control to 5.2  Don't use mirrors for goc repos  osg-ca-scripts: require wget  osg-configure: Detect when fetch-crl missing  osg-configure: don't use condor_config_val -expand  gsi-openssh-server update for LIGO    3.4.4  Update to HTCondor 8.6.6 in OSG 3.4  Update to HTCondor 8.7.3 in Upcoming  Update to singularity-2.3.2+  Add singularity to osg-tested-internal  osg-configure: Release 2.2.1    3.3.29  Release voms-admin-server-2.7.0-1.23+  osg-configure: Release 1.10.1", 
            "title": "OSG Release Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171009/#discussions_1", 
            "text": "Derek and Marian will get a tester from CMS for the vo-client package", 
            "title": "Discussions"
        }, 
        {
            "location": "/meetings/2017/TechArea20171009/#osg-investigations-team", 
            "text": "", 
            "title": "OSG Investigations Team"
        }, 
        {
            "location": "/meetings/2017/TechArea20171009/#last-week", 
            "text": "Worked on cleaning up GRACC data  Comparing cleaned GRACC Data  More work on writable stashcache, still waiting on bugfix", 
            "title": "Last Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20171009/#this-week", 
            "text": "Finalizing GRACC changes  Hope to have writeable StashCache avail. for early testers; waiting for Condor bugfix  OSG - CVMFS focus day to improve the  *.osgstorage  and  singularity.opensciencegrid.org  repo stability.", 
            "title": "This Week"
        }, 
        {
            "location": "/meetings/2017/TechArea20171009/#ongoing", 
            "text": "GRACC Project  StashCache Project", 
            "title": "Ongoing"
        }, 
        {
            "location": "/meetings/2017/TechArea20171009/#discussions_2", 
            "text": "None this week", 
            "title": "Discussions"
        }
    ]
}